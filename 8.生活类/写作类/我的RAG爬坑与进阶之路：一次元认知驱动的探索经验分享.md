## 🎯 文章目标

本文主要面向对 RAG 技术感兴趣的实践者、学习者，以及希望从浅层应用走向深入理解的技术爱好者，希望帮助大家：

- 系统梳理一次从 RAG 初识、框架应用、遭遇瓶颈到深入原理、探索优化的个人学习与实践历程。
    
- 深刻理解 RAG 的核心机制，特别是检索管道与知识库构建中的关键挑战与应对策略。
    
- 感受元认知在技术学习与问题解决中的驱动作用，启发大家进行更深层次的技术探索。
    

> **💡 小提示**深刻理解 RAG 各环节的底层原理与相互关联，是跳出“调包侠”困境、有效诊断问题并针对性地优化系统性能的关键前提。

## 📄 本次主题

本次主要和大家共同探讨：我个人在学习和实践 RAG 技术过程中的**关键认知转折点、技术难点探索、以及元认知驱动下的反思与对未来发展的畅想**。

---

**📋 目录**

- 🎯 文章目标
    
- 📄 本次主题
    
- 🚁 前言
    
- 💡 第一阶段：初识RAG——从“听过”到“用过”的鸿沟
    

- 最初的遇见与误解
    
- 拥抱框架的“便捷”：LangChain/LlamaIndex初体验
    

- 🤔 第二阶段：反思与觉醒——元认知驱动下的认知重塑
    

- 对框架的深度反思
    
- 明确核心：为何我坚信RAG的核心是检索？
    

- ⚙️ 第三阶段：深入引擎——解构与优化检索管道
    

- RAG系统宏观架构回顾
    
- 从用户查询出发：理解与预处理
    
- 核心检索与排序：从词频到语义，再到精准重排
    

- 🧱 第四阶段：夯实地基——知识库构建：挑战与工程实践
    

- 知识库：RAG系统的基石与信息源
    
- 构建关键步骤与考量
    
- 核心难点聚焦：文件处理的挑战与务实应对
    

- 🌟 第五阶段：元认知升华——总结、反思与前行
    

- 未来展望：Agentic RAG 与深度智能化的畅想
    

- 最后的最后
    
- 📒 总结与展望
    
- 📚 往期精选
    

  

## 🚁 前言

这篇文章，便是我在这条RAG爬坑与进阶之路上，一次由表及里、充满反思（元认知驱动）的学习、实践与思考之旅的粗糙纪录。希望能通过分享我的困惑、顿悟、踩过的坑以及对关键技术的理解，为同样走在这条路上的朋友们提供一些参考和启发。

---

✨ 你好，我是筱可，欢迎来到「筱可 AI 研习社」！🚀 标签关键词| AI 实战派开发者 | 技术成长陪伴者 | RAG 前沿探索者 | 文档处理先锋 |

## 💡 第一阶段：初识RAG——从“听过”到“用过”的鸿沟

### 最初的遇见与误解

回想最初接触RAG概念时，坦白说，我对其理解是相当模糊甚至错误的。我当时的认知停留在一种朦胧的想象中，以为RAG是通过某种方式，让经过Embedding处理后的知识向量直接参与到大模型的内部计算过程中，从而影响模型的输出内容。这种理解现在看来显然是偏离了核心机制。

直到后来深入了解，我才恍然大悟：原来RAG的核心机制并非如此。它的本质流程更为清晰和工程化：

当用户提出查询时，系统首先利用Embedding等技术在外部知识库中进行**检索**（Retrieval），找到与查询最相关的若干文本片段；

然后，将这些检索到的信息（上下文）与用户的原始查询一起，**增强**（Augmented）成一个新的、信息量更丰富的提示（Prompt）；

最后，将这个增强后的提示交给大语言模型进行**生成**（Generation），从而得到更准确、更贴合知识库内容的回答。

这个“检索-增强-生成”的过程，在当时大模型浪潮刚刚兴起的阶段，着实让我眼前一亮。它似乎为构建一个智能问答系统提供了一条捷径，让人觉得“做一个智能问答系统似乎很简单”。这种看似触手可及的“智能”让我对RAG充满了期待。

### 拥抱框架的“便捷”：LangChain/LlamaIndex初体验

带着这份期待，我自然而然地接触到了当时流行的RAG开发框架，如LangChain和LlamaIndex。初次使用时，这些框架提供的丰富组件和便捷的API确实让我感觉“如获至宝”。我一度认为，要想“学会”RAG，就必须熟练掌握这些库，能够灵活调用它们的接口来实现各种功能。

然而，随着实践的深入，问题逐渐暴露出来。我发现自己常常只是停留在“知道这个API可以实现这个功能”的层面，对于其背后的具体实现逻辑、算法原理知之甚少。这种“知其然，不知其所以然”的状态，导致我在遇到实际问题时束手无策：RAG系统的效果时好时坏，但我完全不知道问题出在哪里，更不用说如何进行针对性的优化了。

为了打破这种局面，我尝试过去深入阅读框架的源码，希望能理解其内部运作机制。但以当时的LangChain为例，其庞大的代码量、复杂的抽象层次以及各种设计模式，对于软件工程能力尚未达到一定高度的我来说，带来了巨大的认知负担。理解其内部逻辑成了一项极其困难的任务，这也成为了我开始对其产生疑虑的起点。

---

## 🤔 第二阶段：反思与觉醒——元认知驱动下的认知重塑

### 对框架的深度反思

深入源码的受挫经历，以及在实际应用中遇到的种种问题，促使我开始进行更深层次的反思，特别是关于框架本身及其在不同场景下的适用性。

![框架依赖的挑战](https://mmbiz.qpic.cn/mmbiz_jpg/OpQ5uW8ao3ic6JYXiccGlNZECmYXJYYMicdBico2pA66FLTm8XC8kqvebN8c5lXdnnramoWibCxY2cg4rOiaTsWiabSGA/640?wx_fmt=jpeg&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1)

框架依赖的挑战

首先，我意识到，**“封装好”并不等同于“逻辑优”**。虽然框架提供了便捷的接口，但其内部实现的算法、策略是否足够优秀和适用，是需要打个问号的。尤其是在AI技术日新月异的背景下，框架需要快速迭代以跟上最新的研究进展，这使得我们对框架的认知也需要不断更新。

其次，这种快速迭代的特性，恰恰与**工业级应用对稳定性的核心需求**产生了矛盾。在企业开发中，我们往往需要的是稳定、可靠、行为可预测的系统。像LangChain这样快速变化的框架，虽然适合快速构建原型、验证想法，但在追求长期稳定性和可维护性的生产环境中，其适用性值得商榷。在我看来，这类框架更适合被定位为**快速原型开发工具**。

更重要的是，过度依赖框架，会让我们失去对底层技术的掌控力。框架的抽象和封装，虽然降低了入门门槛，但也可能成为理解和优化的障碍。当我们需要根据特定业务场景进行深度定制、或者需要对性能进行极致优化时，框架的“黑箱”特性往往会成为障碍。

当然，这并非全盘否定框架的价值。其中不乏设计精妙、值得借鉴的模块和思路。但经过反思，我逐渐认识到，对于需要长期维护和持续优化的RAG系统而言，**拥有一套自己能够理解、掌控和快速修改的代码基础至关重要**。这种底层因素决定了，完全依赖一个庞大且快速变化的外部框架，大概率不会是我的最终选择。这也是我后来坚持要深入理解底层原理的重要原因。

### 明确核心：为何我坚信RAG的核心是检索？

在逐步摆脱对框架的盲目依赖，开始回归第一性原理思考的过程中，一个关键问题浮出水面：

RAG的核心到底是什么？我的答案是：**检索（Retrieval）**。

![RAG核心机制](https://mmbiz.qpic.cn/mmbiz_jpg/OpQ5uW8ao3ic6JYXiccGlNZECmYXJYYMicdn9QmUuOcpib4ibSY6qr32sMxN9HibXrGb6h3ibzQYaxo2Siaww4Hd1EOYow/640?wx_fmt=jpeg&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1)

RAG核心机制

首先，从其全称“Retrieval-Augmented Generation”（检索增强生成）来看，**“检索”被明确地放在了核心位置**，名字本身就昭示了他的重要性。

其次，从RAG系统的最小构成单元来看，它包含检索器（Retriever）和生成器（Generator）。

生成器通常是已经训练好的大型语言模型（LLM），虽然其能力是基础，但在RAG的背景下，我们通常不直接修改或重新训练它（否则就变成了微调Fine-tuning）。

RAG的核心价值恰恰在于，通过**外部知识检索**来**增强**这个预训练模型的表现，以较低的技术成本（相比于微调）来降低模型的幻觉、提升回答的相关性和准确性。

那么，这个“增强”效果的好坏，很大程度上就取决于“检索”这一步能否找到真正有用、准确、全面的信息。因此，**检索环节是RAG区别于单纯LLM应用、实现其核心价值的关键所在**。

我们可以将“检索”进一步拓展理解为构建一个“检索管道”（Retrieval Pipeline）或者说一个针对本地知识库的“搜索引擎”。这个管道的复杂程度可以根据具体需求来定制。

最简单的例子就是联网查询：

用户的查询被发送给外部的搜索引擎（如Google Search, Bing Search等），搜索引擎扮演了检索器的角色，从互联网这个巨大的“知识库”中查找信息，然后返回给生成器。

在这个过程中，我们无需管理知识库的构建和索引，只需调用API即可。

这个例子清晰地展示了，即使在最简化的RAG形式中，检索也是不可或缺的核心环节。

那么，在更常见的场景下，当我们希望RAG系统能够基于我们自己的私有文档、数据来回答问题时，就需要构建一个属于我们自己的、针对本地知识库的“搜索引擎”。

如何设计和优化这个内部的“检索管道”，自然就成了构建高质量RAG系统的重中之重。

---

## ⚙️ 第三阶段：深入引擎——解构与优化检索管道

既然认识到检索是RAG的核心，那么深入理解和优化检索管道就成了我的下一个主攻方向。在深入探讨具体的检索算法之前，我们有必要先回顾一下典型的RAG系统架构。

### RAG系统宏观架构回顾

一个典型的RAG系统通常由三大核心组件构成：

1. **知识库 (Knowledge Base)**: 这是系统的信息源泉，可以是文档集合、数据库记录、网页内容等。原始数据需要经过预处理（清洗、切分）和索引（通常是倒排索引，还有hnsw，ivf等等），才能被检索器高效利用，当然暴力搜索也是可以的，只是速度会下降几个量级。
    
2. **检索器 (Retriever)**: 它的核心任务是根据用户的查询（Query），从知识库中快速、准确地找出最相关的若干信息片段（Context）。检索器的实现技术多样，包括传统的关键词匹配（如BM25）、向量相似度搜索（基于Embeddings），或是更复杂的混合策略。
    
3. **生成器 (Generator)**: 通常是一个大型语言模型（LLM）。它接收用户的原始查询以及检索器找回的相关上下文，然后基于这些输入，“理解”并“综合”信息，生成最终的答案。
    

**典型数据流**如下： 用户输入查询 -> 检索器在知识库中搜索相关上下文 -> 将上下文与原始查询构造成增强提示 -> 生成器（LLM）基于增强提示生成答案 -> 输出答案给用户。

![典型RAG系统](https://mmbiz.qpic.cn/mmbiz_jpg/OpQ5uW8ao3ic6JYXiccGlNZECmYXJYYMicdoymzHRFPPWT7vx5wpD5ibLpicHb935lZkvGF0SlSsbgHs7TpthMakXhg/640?wx_fmt=jpeg&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1)

典型RAG系统

这个“检索”+“生成”的机制，使得RAG能够在不重新训练模型的情况下，有效利用外部知识，提升回答质量。

接下来，我们将聚焦于这个流程的核心——检索管道的构建与优化。

### 从用户查询出发：理解与预处理

用户的查询是检索流程的起点。一个好的RAG系统，必须能够有效地理解用户的意图，并对其进行必要的处理，才能为后续的检索环节打下坚实的基础。我们可以将这一系列处理步骤视为检索管道的“前置处理层”。

**为何要如此重视用户查询的理解与处理呢？**

在我看来，**理解并区分用户问题的类型和复杂度至关重要**。这直接影响到我们如何设计检索策略、选择数据处理方式、乃至优化整个系统的性能。并非所有问题都生而平等：有些是简单的“事实查找”，而另一些则可能涉及跨文档的“复杂推理”或需要处理“模糊不清”的表述。这些差异对RAG流程中的检索器和生成器都提出了截然不同的挑战。

如果我们不能有效地区分和应对这些不同类型的问题，系统就很容易在检索时“大海捞针”却找不到关键信息，或者生成时“胡言乱语”、产生幻觉。例如，一个需要**多步推理（Multi-hop）**的问题，如果用处理简单事实查找的方法去检索，很可能只找到部分碎片信息，导致答案不完整。一个**措辞模糊或充满歧义**的查询，则可能让检索器完全“跑偏”，取回一堆不相关的文档，给后续的生成器带来巨大的“噪音”和干扰。

因此，深入理解用户查询的性质，并进行针对性的预处理，是避免在系统设计和性能调优上走弯路的关键一步。这不仅仅是简单的文本规范化，更涉及到对问题背后**意图、复杂度和所需信息类型**的深层把握。

**问题的复杂性来自哪里？**

除了传统问答系统也面临的挑战（如事实查找、列表、定义、因果、假设等不同问题类型），RAG因为引入了**从外部知识库检索信息**这一环节，带来了额外的复杂度维度。我总结下来，RAG中问题的复杂性主要源于以下几个方面：

1. **查询本身的清晰度与明确性**：用户的提问往往是口语化、模糊不清、甚至带有歧义的（比如“给我讲讲AI的最佳实践” vs “银行欺诈检测中的AI实践哪个更好？”）。这直接考验系统对用户真实意图的捕捉能力。
    
2. **所需信息的明确性与分布**：答案是直接存在于某段文本中的**明确事实**？还是需要结合多个信息片段、甚至进行一定常识或逻辑推断才能得到的**隐含信息**？或者，答案根本就不在知识库里（**上下文不足**）？
    
3. **推理的深度与广度**：回答问题是否需要连接来自多个文档的信息，进行**多跳（Multi-hop）推理**？或者需要进行比较、聚合等更复杂的操作？
    
4. **主题的特异性与专业性**：查询是否涉及高度具体、小众或包含大量**专业术语**的主题？这些术语可能在通用嵌入模型中代表性不足，导致语义匹配困难。
    
5. **知识库的数据质量**：即便查询本身清晰，如果检索到的知识片段充满**噪声、相互矛盾或信息过时**，也会大大增加处理的复杂度，甚至误导生成器。
    

**复杂性对RAG组件的影响**

这些不同层面的复杂性，对RAG的两大核心组件——检索器和生成器——都提出了严峻的考验：

- **对检索器**（Retriever）：
    

- **查找相关性**：面对模糊、多跳或包含专业术语的查询，更难精确地找到所有必要且相关的文档片段。
    
- **处理歧义**：需要更强的能力来解读意图，避免因歧义而检索到无关内容。
    
- **协调多步**：对于需要多步推理的问题，简单的单次检索往往不够，需要更复杂的检索策略（如分解问题、顺序检索）。
    
- **应对知识限制**：如果知识库本身不完整或过时，再好的检索器也无能为力。
    

- **对生成器**（Generator / LLM）：
    

- **抗噪能力**：如果检索器返回了大量噪声或矛盾信息，LLM需要具备更强的能力来筛选、识别并基于可靠信息生成答案，否则容易“迷失”或产生幻觉。
    
- **上下文维持**：处理多跳问题或长对话时，需要有效维持和利用跨轮次或跨文档的上下文信息。
    
- **推理与综合**：对于需要推理或从隐含信息生成答案的查询，要求LLM具备超越简单文本提取的复杂推理和综合能力。
    
- **避免幻觉**：即使有上下文，在信息不足、冲突或需要深度推断时，LLM仍可能产生幻觉。
    

这里存在一个典型的**权衡**（Trade-off）：为了应对复杂问题（尤其是需要多方面信息的问题），我们可能倾向于让检索器召回更多候选文档（追求高召回率），但这又会增加噪声，加大生成器的处理难度；反之，如果过于追求检索精度，又可能漏掉关键信息，导致生成器“无米下锅”或给出片面答案。这正是一些高级RAG技术（如Self-RAG、CRAG、Adaptive RAG等）试图通过更智能的检索、评估和生成协同来解决的问题。

**那么，具体如何进行查询预处理呢？**![用户查询预处理](https://mmbiz.qpic.cn/mmbiz_jpg/OpQ5uW8ao3ic6JYXiccGlNZECmYXJYYMicdc45zfrjj4a4WPMicjMIgILfCkzwPQcwDOzQGQjpFXbG1dRicUE5ZGbDw/640?wx_fmt=jpeg&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1)

认识到用户查询的多样性和复杂性后，我们就可以更有针对性地应用一系列预处理技术，将原始的、可能混乱的用户查询，转化为更规范、更聚焦、意图更清晰的输入，为后续的检索匹配打下良好基础。主要可以从两个层面入手：

**层面一：查询的规范化处理** (Normalization)

这主要解决查询在“表面形式”上的问题，确保后续处理（尤其是基于关键词的检索，如BM25）能顺利进行。常用手段包括：

- **拼写纠错**：自动修正输入错误，避免因错别字导致检索失败（例如，将 "langchain" 的错误拼写 "langhchain" 改正）。
    
- **大小写转换**：通常统一转为小写，以进行不区分大小写的匹配。但最好保留原始大小写副本，供最终答案生成时参考。
    
- **词干提取或词形还原**：将词语的不同形式（如 "running", "ran"）归一为其基本形态（"run"），有助于扩大召回，匹配文档中同一词根的不同变体。词形还原更准确但计算成本稍高。
    
- **去除停用词**：移除 "的", "是", "和" 等常见但信息量低的词。需谨慎使用，有时停用词对维持句子结构和语义（尤其对于嵌入模型）很重要。
    

**层面二：查询意图的深度理解与增强** (Intent Understanding & Enhancement)

这旨在更深层次地把握用户需求，应对查询的歧义性、复杂性等挑战。

- **关键词提取**：识别并提取最能代表查询核心意图的词语或短语，去除口语化修饰（如“我想问一下...”）。
    
- **命名实体识别** (NER)：识别查询中的人名、地名、组织名、技术术语等专有名词，这对于需要精确匹配知识库中特定实体信息的场景（尤其在有知识图谱支持时）非常有价值。
    
- **意图分类**(Intent Classification)：判断用户的查询类型，是寻求定义、方法、比较，还是其他？不同的意图可能触发不同的检索策略或后续处理逻辑。例如，识别出用户在“闲聊”，可以直接由LLM响应，无需启动RAG流程。
    
- **查询重写/扩展**(Query Rewriting/Expansion)：对于过于模糊、简洁或复杂的查询，可以利用LLM或其他技术进行改写，使其更清晰、信息更丰富，或者分解为多个子查询。例如，可以加入同义词扩展召回范围，或者将一个复杂的多跳问题分解成几个更简单的子问题逐步查询。
    

通过这样从表层规范化到深层意图理解和增强的处理，我们可以最大限度地提升查询质量，为后续的核心检索与排序环节提供最好的起点。

### 核心检索与排序：从词频到语义，再到精准重排

![优化检索管道](https://mmbiz.qpic.cn/mmbiz_jpg/OpQ5uW8ao3ic6JYXiccGlNZECmYXJYYMicdBSuRtG8fnmp8HeX7fJwwicsPibOOvgTWxg9CuhhkxXaTibwhTsxfiaP8dw/640?wx_fmt=jpeg&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1)

优化检索管道

经过预处理的查询，接下来就进入了核心的检索与排序环节。我们如何从知识库中找到与查询最相关的文档片段，并对其进行有效排序呢？

让我们从最简单的想法开始。最直接的方法是**文本直接匹配**，只要文档片段中包含查询的关键词，就将其返回。但这种方法的缺点显而易见：我们无法对结果进行排序，相关性强的和弱的混杂在一起。

那么，我们可以**匹配多个关键词**，并按照匹配上的关键词数量进行排序吗？命中的关键词越多，排名越靠前。这确实在一定程度上解决了排序问题，算是一个简单的排序方案。

但继续思考，如果一篇很长的文章，仅仅因为他的长度优势，包含了大量重复的关键词（即使这些关键词只占文章很小一部分），导致其命中数远超其他更相关但较短的文章，怎么办？

看来我们需要引入**文本长度**作为影响因子。可以考虑对过长的文档进行一定的“惩罚”，避免长度导致分数差距过大。但单纯扣分似乎也不太合理，万一这篇长文确实通篇都在深入讨论这些关键词呢？所以，我们不仅要看长度，还要看**关键词在文档中的占比或分布**。

仅仅关注关键词数量、文章长度以及它们之间的关系，似乎还不够精确和鲁棒。这里，就引出了信息检索领域一个非常经典的算法——**BM25（Best Match 25）**。

![BM25 算法核心概念](https://mmbiz.qpic.cn/mmbiz_jpg/OpQ5uW8ao3ic6JYXiccGlNZECmYXJYYMicdLrfsvDGiby20Fy6iaUxmjVc0kFoxOIrjdYyl2SnmbVh0JOELBOicFuVEQ/640?wx_fmt=jpeg&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1)

BM25 算法核心概念

BM25是一种基于概率检索模型的排名函数，它能够更精妙地平衡词频（Term Frequency, TF）、逆文档频率（Inverse Document Frequency, IDF）和文档长度，来计算文档与查询之间的相关性得分。其核心公式如下：

其中：

-  是文档  相对于查询  的得分。
    
-  是查询，包含多个查询词 。
    
-  是查询词  的逆文档频率，衡量词语在整个文档集中的稀有程度（越稀有，越重要，IDF值越高）。
    
-  是查询词  在文档  中出现的频率（TF）。
    
-  是文档  的长度， 是文档集平均长度。
    
-  和  是可调参数。 控制词频饱和度（词频很高时，得分增长趋缓）， 控制文档长度归一化的程度（ 越大，长文档的“惩罚”越重）。
    

简单来说，BM25通过IDF衡量词的重要性，同时考虑了词在具体文档中的频率以及文档相对平均长度，奖励那些包含多个重要（稀有）查询词、且这些词出现频率适中、同时文档长度不过分长的文档。它是一种基于**词频统计学**的算法，能够在很大程度上提供稳定且效果不错的排序结果。

然而，即使是BM25这样经典的算法，也并非万能。因为它本质上还是基于**词汇匹配**，它无法理解词语背后的**语义**。这就带来一个典型的问题：如果用户输入的查询词和文档中的相关词语是同义词或近义词，但字面上完全不同，BM25就无能为力了。比如，一个不熟悉“乌龟”学名的小孩子可能会搜索“王八”，但如果我们的知识库文档里只写了“乌龟”，那么基于BM25的搜索很可能什么也找不到。

面对这种语义鸿沟，我们该怎么办？难道要用正则表达式写大量的同义词替换规则吗？这显然不现实。或者用大模型先对用户问题进行改写，让其表达更规范？这或许可行，但存在不稳定性（模型可能改写错）和成本问题（调用大模型）。

这里，我们需要引入一种能够理解文本深层含义、跨越字面障碍的技术——**嵌入（Embeddings）技术**。

嵌入技术的核心思想是将文本（词语、句子、段落等）转换为**高维数字向量**，这些向量能够捕捉文本的**语义信息**。关键在于，**语义上相似的文本，在转换成向量后，它们在向量空间中的距离会比较近**。现在有许多强大的预训练嵌入模型（如Sentence-BERT、OpenAI Embeddings、BGE系列等），它们在大规模语料上学习到了如何将文本映射到能够反映其语义的向量空间。

利用嵌入技术解决语义匹配问题的流程大致如下：首先，将知识库中的所有文档（或切分后的块）通过选定的嵌入模型转换成向量，并存储在专门的**向量数据库**中。当用户输入查询时，使用**相同的嵌入模型**将查询也转换为一个向量。然后，计算查询向量与知识库中所有文档向量之间的**相似度**（常用的是余弦相似度Cosine Similarity）。最后，根据相似度得分对文档进行排序，得分最高的即为语义上最相关的文档。

这样一来，即使用户查询“王八”，其生成的向量也能与文档中“乌龟”生成的向量在空间中距离很近（因为它们在语义上高度相关），从而实现有效的匹配。嵌入技术的优势在于它能**超越关键词匹配**，理解词语和句子的深层含义，较好地处理同义词、近义词甚至一定程度的多义词问题，并且结合向量数据库能够高效地在大规模文档集上进行语义搜索。

![利用嵌入技术进行语义匹配的基本流程](https://mmbiz.qpic.cn/mmbiz_jpg/OpQ5uW8ao3ic6JYXiccGlNZECmYXJYYMicdKYGRjYrjpktpJBrVO3uicq6ecwv6d7PlJialMze6Stia1j4XzaH1VtTlw/640?wx_fmt=jpeg&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1)

利用嵌入技术进行语义匹配的基本流程

那么，引入了强大的嵌入技术后，我们是否就可以完全抛弃BM25这类传统算法了呢？答案是否定的。

原因有几方面：

首先，**嵌入模型的效果高度依赖于其训练数据**，如果训练数据与你的应用场景偏差较大，其效果不一定优于基于明确规则的BM25。

其次，**嵌入向量的生成（推理）需要消耗大量的计算资源**。

根据我的个人实验（使用Ultra 7 155H处理器，大致相当于i7 13代），使用像BGE-M3这样的模型，在不进行批处理的情况下，生成单个向量大约需要0.2秒，即使进行批处理，每个向量也需要约0.02秒。这在需要快速响应的场景下是一个不可忽视的成本。

更重要的是，嵌入模型本身也存在其复杂性。例如，不同的任务可能需要不同的嵌入模型才能达到最佳效果。我们常常需要区分**问答匹配（QA）**、**查询-文档检索（QD）**、**查询-查询相似度（QQ）**以及**重排序（Rerank）**等不同任务，并选择针对性优化的模型。

即便是专门调整过的模型，其表现也并非总是完美，模型的训练过程很大程度上像一个“黑箱”，我们通过调整数据“炼丹”，但难以完全保证其在所有情况下的稳定性和准确性。

例如，对于短文本和长文本的相似度计算，或者对于细微语义差别的捕捉，嵌入模型有时会给出不符合直觉的结果。

相比之下，BM25作为基于规则的算法，其行为是**稳定且可预测**的，虽然它无法跨越语义鸿沟，但在处理字面匹配和基于词频统计的相关性判断上，往往非常可靠。

因此，在实践中，**最佳策略往往是将BM25和嵌入向量搜索结合起来使用**。我们可以同时使用两种方法进行多路召回（分别找出各自认为最相关的文档），然后将结果进行融合（例如，通过某种加权方式或排序算法合并两组结果）。这样可以兼顾词法匹配的稳定性和语义匹配的灵活性。

此外，为了进一步提升最终排序的精度，我们还可以在多路召回之后引入一个**重排序（Rerank）**环节。这个环节通常使用更复杂、更强大的模型（例如基于Transformer的**交叉编码器Cross-Encoders**），对初步召回的候选文档列表（比如Top 50或Top 100）进行二次精排。

为什么需要重排序呢？初步召回阶段使用的模型（无论是BM25还是用于生成嵌入的Bi-Encoder模型）为了追求效率，其计算相对简单。

而重排序模型（如Cross-Encoder）可以同时接收查询和单个候选文档作为输入，进行更深层次的交互和语义关系判断，从而给出更准确的相关性得分。因为它只处理少量候选文档，所以即使模型更复杂，总体开销也是可控的。

可以将Rerank模型理解为一个**加强版的相关性打分器**，它不需要区分QA/QD等任务，而是能直接对输入对进行打分，你可以理解他可以自己判断当前任务是什么。虽然它也依赖训练数据，但由于模型结构的优势（交叉注意力机制），排序结果通常优于单独的BM25或基于向量相似度的排序。

总结来说，一个健壮的检索管道通常包含：

**查询预处理 -> 多路召回（如BM25 + 向量检索）-> 结果融合 -> Rerank 精排**。![一个健壮的检索管道](https://mmbiz.qpic.cn/mmbiz_jpg/OpQ5uW8ao3ic6JYXiccGlNZECmYXJYYMicdze0a0lzXNpzvBheZCzib85ibP9hDpXko6fqZUE5tGdiclfS6v58bywe3w/640?wx_fmt=jpeg&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1)

通过这样层层递进、结合多种技术的策略，我们可以在效率、成本和效果之间取得较好的平衡，最大限度地提升检索结果的质量，为后续的生成环节提供最优质的上下文信息。

当然，还有像ColBERT这样的多向量检索技术、MMR（最大边际相关性搜索）用于提升结果多样性等更高级的策略，感兴趣的读者可以进一步探索。

---

## 🧱 第四阶段：夯实地基——知识库构建：挑战与工程实践

理解了检索管道的复杂性后，我们必须认识到，再强大的检索能力也需要有高质量的数据才能发挥作用。这个就是RAG系统的另一个基石——**知识库（Knowledge Base）**。如果说检索是“如何找到信息”，那么知识库构建就是“确保有好信息可供查找”。一个结构清晰、内容准确、易于检索的知识库，是保证RAG系统输出质量的根本前提。

### 知识库：RAG系统的基石与信息源

**知识库构建**指的是将原始数据（如文档、网页、数据库记录等）处理、转换并组织成一种结构化的、易于检索的格式，以便RAG系统能够有效地从中提取信息的过程。这绝非简单地将文件堆砌在一起，而是涉及一系列为了优化检索效果而进行的精心设计与工程实践。

### 构建关键步骤与考量

构建一个高质量的知识库，通常需要经历下面的一些关键步骤，每一步都伴随着挑战与决策，或许很多环节做好了都能发不少论文：

首先是**数据收集与加载（Data Collection and Ingestion）**。我们需要明确知识的来源，它可以是各种格式的文档（PDF, DOCX, TXT, Markdown等）、网页内容、数据库记录、API接口返回的数据，甚至是结构化的表格。针对不同的数据源，需要选择合适的工具或编写脚本（Data Loaders）来读取和加载数据。许多RAG框架（如LlamaIndex, LangChain）虽然提供了丰富的数据加载器，但在实际应用中，我发现它们的通用性往往有限，特别是面对格式复杂或包含特殊元素的数据时，但是效果可能不尽如人意。

接下来是**数据清洗与预处理（Data Cleaning and Preprocessing）**。原始数据往往充满噪音，需要进行细致的处理。这包括将不同格式的数据尽量**统一**为纯文本或某种标准格式；**去除**不相关的元素，如HTML标签、广告、页眉页脚、水印等；对于来自扫描文档的数据，可能需要**修正OCR识别错误**；还需要**处理特殊字符**，规范化或移除可能影响后续处理的字符。数据质量是根本，“Garbage in, garbage out”，这一步的质量直接影响后续所有环节。

然后是至关重要的文本切分（Chunking）。

**为何分块如此重要？** 主要原因有三：

一是克服大语言模型（LLM）的**上下文窗口限制**，原始长文档必须切分成模型能处理的小块；

二是**提高检索精度与效率**，小而美的、语义集中的块更容易被精确匹配和快速检索；

三是尽可能**维护上下文完整性**，好的分块应避免在不恰当的地方断开，保持信息单元的相对完整。

分块的核心挑战在于**在“检索精度”（小块有利）和“上下文完整性”（大块有利）之间找到最佳平衡点**。

实践中存在多种**基础分块策略**。

**固定大小分块**(Fixed-size Chunking)是最简单的方法，按固定字符或Token数切割，通常带重叠（Overlap）以缓解边界问题，但极易破坏语义，忽略文档结构。 我在后续的实践中倾向于避免使用Overlap，因为它可能引入冗余信息，降低向量相似度的区分度。

**基于句子的分块**(Sentence Splitting)试图尊重语言边界，语义保持较好，但块大小不均，且对简单标点分割的准确性或NLP库有依赖（我在示例代码中为简化未引入外部NLP库）。

**递归字符分块**(Recursive Character Text Splitting)是LangChain等框架常用的策略，它按预设的分隔符列表（如`\n\n`, `\n`, 空格等）递归尝试分割，试图在保持结构（如段落）的同时满足大小限制，通用性较好。

**基于文档结构的分块**(Document Structure-aware Chunking)利用文档固有结构（如HTML标签、Markdown标题）进行分割，最能保持原文逻辑，且方便附加结构化元数据，但依赖清晰的文档结构，且块大小可能极不均匀。

为了结合不同策略的优点，**混合分块** (Hybrid Chunking)应运而生。一种常见的思路是**分层处理**：先利用文档结构（如Markdown标题）进行高级别、粗粒度的分割，得到逻辑上相关的“大块”并保留标题等元数据；然后，如果这些“大块”仍然超出目标大小限制（`chunk_size`），再在内部使用递归字符分块等更细粒度的策略进行切分。我个人实践并实现过类似基于Markdown的混合分块策略，它能在保留标题提供的上下文信息的同时，通过递归分割确保块大小可控，并特别注意保持代码块的完整性，不对其进行切分。这种方式通常能产生质量更高、更适合RAG的文本块。![分块策略](https://mmbiz.qpic.cn/mmbiz_jpg/OpQ5uW8ao3ic6JYXiccGlNZECmYXJYYMicdj3gz8aWdfVfJcKdQvubQ1gNYw0gCsCPhIOGZBZbXVgdicrrQFscWYZA/640?wx_fmt=jpeg&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1)

当基础策略无法满足需求时，还可以探索**高级分块策略**。**语义分块**(Semantic Chunking)利用Embedding向量计算相邻文本片段的语义相似度，在“语义断裂点”进行切割，理论上最符合人类理解，但计算成本高，依赖模型和阈值调整。**分层分块**(Hierarchical Chunking)系统地创建多层级块（如章、段、句），增加检索灵活性，但索引更复杂。**Small-to-Big / 父文档检索器**(Parent Document Retriever)是一种检索策略，它依赖于分层或父子块结构：先检索精确的小块，然后返回其对应的、包含更丰富上下文的父块给LLM。**命题分块** (Proposition Chunking)尝试用LLM/NLP将文本分解为原子事实，粒度极细，适合事实问答，但成本高，可能丢失信息。**Agentic / LLM-based Chunking**则更进一步，让Agent或LLM根据内容理解来决策如何分块，潜力巨大，但复杂、昂贵且处于探索阶段。

此外，还有一种**块优化策略**：**上下文富化**(Context Enrichment)。它在分块之后，为每个块补充额外信息，如相邻句子、摘要或父级标题元数据，以增强上下文，但需注意过度使用摘要可能降低块间区分度。

选择哪种分块策略，需要根据数据特性、应用场景、对质量的要求以及成本预算进行综合权衡，如果追求非常快的速度以及相对较好的分割的效果我推荐使用混合分块的形式，对应的讲解和代码在我的另外一篇文章里面。

分块完成后，需要进行**嵌入生成**（Embedding Generation）。选择合适的嵌入模型（考虑语言、领域、精度、成本），对所有文本块进行批量处理，生成对应的向量表示，值得注意的是需要考虑对应的模型性能问题，或者对于你的领域是否适应，因为每个模型训练的数据不一定包含你的领域数据，我把这个效应叫做“水土不服”。

紧接着是**索引与存储（Indexing and Storage）**。将文本块及其对应的嵌入向量存入专门的**向量数据库**（如FAISS, Milvus, Pinecone, Qdrant, ChromaDB等），这些数据库为高效的向量相似度搜索进行了优化。同时，**存储丰富的元数据（Metadata）至关重要**，例如源文件名、文档标题、章节信息、创建日期、关键词、块ID等。这些元数据不仅有助于后续的过滤和排序，还能为用户提供答案溯源的能力。

在构建大规模知识库时，**数据去重（Data Deduplication）**不容忽视。重复内容浪费资源，还可能导致检索偏见。可以采用**基于哈希的去重算法**，如`SimHash`（检测近似重复）或`MinHash`（快速估计Jaccard相似度）。也可以**借鉴成熟的论文查重算法**，如基于N-gram匹配或Winnowing算法。以知网为例，他们做的就是分段检测、设定阈值（如段落重复率低于5%忽略）、基于连续字符匹配（如连续13个字符重复）的模糊匹配机制，都为知识库去重提供了有益的参考。我建议在数据入库前进行查重检测，并结合人工审核来处理重复或近似重复的内容，更新旧数据。

当然，也可以考虑基于向量的语义查重，但是效率会降低，成本也会提升，具体有多大的提升还有待验证。

最后，知识库还需要**更新与维护（Updating and Maintenance）**。当原始数据变化时，需要能够高效地进行**增量更新**，而非全量重建。对知识库进行**版本控制**便于追踪和回滚。还需要**定期评估**知识库的质量和检索效果，并根据评估结果调整构建策略。

### 核心难点聚焦：文件处理的挑战与务实应对

在知识库构建的诸多环节中，**文件内容的准确提取无疑是最为关键也最容易遇到陷阱的一步**。无论后续的切分、嵌入、索引做得多么完美，如果最初从源文件中提取出来的信息就是错误或不完整的，那么整个RAG系统的效果都将大打折扣，不打地基，如建空中楼阁，不做数据清洗，garbage in 真的就是garbage out。

不同格式的文件（PDF、DOCX、网页、图片等）内部结构千差万别，这给通用化的内容提取带来了巨大挑战，有的公司直接能依靠这项业务能挣的盆满钵满。特别是PDF文件，复杂性尤为突出：

**扫描版PDF**需要OCR，准确率受多因素影响；

**复杂的排版**（多栏、图文混排、页眉页脚、水印）干扰文本流提取；

**表格和公式**的结构化信息提取和正确识别是公认的难题；

还可能遇到**特殊字体和编码**导致乱码或字符丢失的问题。

**尤其需要强调的是对包含表格的文件，特别是那些含有超长、复杂表格的PDF文件**。

根据我的经验和观察，目前市面上绝大多数基于模型的自动提取方案，在处理这类表格时效果往往不尽人意，很容易出现错行、漏列、数据解析错误等问题。与其投入巨大精力去调试和优化模型提取效果，有时不如考虑更务实的方法：

**人工介入或采用更针对性的策略**。

对于这类包含复杂表格的文档，我强烈建议：

对于内容较短、结构相对简单的表格，可以考虑将他**转换为Markdown或HTML格式的表格**，直接嵌入文本块中。

而对于**超长或结构复杂的表格**，更可靠的方案是：

1. **首先，通过人工或使用专门的工具，将PDF（或其他格式文件）中的这些复杂表格准确地转换为结构化的数据格式，如Excel表格或CSV文件。** 这个过程虽然可能耗费人力，但能最大限度地保证数据的准确性和完整性，如果是不干净的数据为什么还要去污染数据库的数据源呢，对于知识库，我们必须保持洁癖，我认为既然是自己要建立的知识库，那么就应该是一个第二大脑，我们这种喜欢终生学习的人对于第二大脑应该不会陌生吧，那么你能允许你的第二大脑被污染吗，答案是显而易见的。
    
2. **然后，将这些结构化的表格数据导入到关系型数据库或类似的结构化存储中。**
    
3. **最后，在RAG系统中，当用户的查询涉及到这些存储在数据库中的表格数据时，可以考虑采用Text-to-SQL的思路。** 即，将用户的自然语言查询转换为SQL查询语句，直接在结构化的表格数据中进行精确查找，然后将查询结果（可以是数据本身或对数据的描述）融入到大模型的上下文中生成答案。
    

这样做的好处显而易见：

- **数据准确性高**（人工保证）
    
- **检索精准**（SQL对结构化数据查询能力强）
    
- **有效避免模型幻觉**（直接从准确数据源获取信息）。
    

这也再次印证了我在反思框架时得出的结论：尽管市面上存在便捷的RAG框架（如LlamaIndex, LangChain），它们在原型验证和快速搭建上很有价值。但在实际生产环境中，面对复杂数据处理（如难搞的PDF表格）、追求极致性能和特定业务需求时，这些框架的通用性往往难以满足要求。很多时候，我们需要**基于对底层原理的深刻理解，自行搭建和维护更具针对性的服务**。这意味着我们可能需要自己调用像`MinerU`、`PaddleOCR`通用管道或`DocLing`这样的专业文档解析工具来处理复杂文档，甚至在某些对速度要求极高的场景下，回归使用`pymupdf` (Fitz)这类传统库，以牺牲部分版面分析精度换取更高效率。这种灵活选择和深度定制，是通用框架难以全面覆盖的，也是走向RAG实践深水区的必经之路。

我觉得RAG工程上的工作越做越深，最终大部分时间都会用来解决文档解析的问题。![知识库构建周期](https://mmbiz.qpic.cn/mmbiz_jpg/OpQ5uW8ao3ic6JYXiccGlNZECmYXJYYMicdUl3vHpVU1s9UqH6eia4G58xy7uRajdic5jPWje3eLlx2sOxTibc8dtvLQ/640?wx_fmt=jpeg&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1)

---

🙋♂️ 入群交流公众号菜单点击「社群」，扫码直接入群回复关键词「入群」，添加作者微信人工邀请，注意备注：入群

## 🌟 第五阶段：元认知升华——总结、反思与前行

回顾整个RAG的学习与实践过程，我深刻体会到**元认知（Metacognition）**——即对自身思考过程的认知和反思——在其中扮演的关键驱动作用。

从最初对概念的误解，到对框架的依赖与反思，再到深入原理、攻坚克难，每一步认知的提升，都离不开对“我当前知道什么？”、“我不知道什么？”、“我为什么会卡住？”、“如何才能更好地理解？”这些问题的持续追问和审视。

正是这种元认知驱动，让我没有停留在API调用的表面，而是主动去探究框架源码（虽然一度受挫）、去质疑现有方案的局限性、去思考更底层的原理、去寻找更适合特定场景的解决方案（比如对复杂表格的处理）。

它帮助我识别知识的盲区，调整学习的策略，最终从一个被动的“工具使用者”转变为一个更主动的“问题解决者”，以及“理论的探索者”。

### 未来展望：Agentic RAG 与深度智能化的畅想

![未来RAG系统的模块化agentic 工作流](https://mmbiz.qpic.cn/mmbiz_jpg/OpQ5uW8ao3ic6JYXiccGlNZECmYXJYYMicdzh01rdCoOcIwaIpuialWIjvUicvhyYGEhdGurfDwUF8lYw3kmjJZst8A/640?wx_fmt=jpeg&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1)

未来RAG系统的模块化agentic 工作流

基于当前的实践和对技术发展趋势的观察，我对未来RAG的发展充满期待，特别是Agentic（智能体化）和深度智能化方向的潜力：

#### **模块化 Agentic 工作流**

我构想未来的RAG系统，其各个核心模块——如用户交互、查询理解、多路检索策略选择与执行、知识库的动态更新与维护、复杂文档的智能解析、乃至文本分块本身——都可能演变成由专门的**Agent**驱动的、具备**局部回环（Local Loop）自我优化能力**的工作流。每个Agent负责其特定领域，并通过协同工作完成复杂的RAG任务。

#### **Agentic 用户交互与查询理解**

交互不再是被动的“一问一答”。Agent可以**主动与用户进行多轮对话**，收集更丰富的背景信息、澄清模糊的查询意图，从而更精准地把握用户需求，再启动后续的检索流程。

#### **Agentic 智能检索**

检索过程将更加智能和动态。Agent可以根据初步分析，**自动进行多轮、自适应的问题改写**，或者生成**假设性文档（HyDE）**来辅助检索，实现更强大的**多路召回**。不仅如此，Agent还能对初步检索到的结果进行**深度推理**，比如构建临时的**关系网络图谱**，分析信息间的关联，**识别知识缺口**，并自主决定是否需要进行补充检索或提出进一步的问题

#### **Agentic 自维护知识库**

知识库将变得更加“鲜活”。当本地知识无法满足查询时，Agent能**自动触发互联网搜索**（当然，这依赖于未来网络搜索质量和可靠性的提升），**智能筛选、验证**网络信息，并将确认有效的知识**无缝整合**到本地知识库中。同样，在数据入库时，Agent能**自动执行更精密的查重**，处理新旧数据的冲突与更新，持续**自我维护**知识库的高质量与时效性。

#### **模型驱动的无损文档解析**

对于文档处理这一痛点，未来的**多模态大模型**或**专门优化的小模型管道**有望实现对各类复杂文档（尤其是PDF）的**近乎无损的信息提取**。这不仅包括精准的文本OCR，还可能涵盖表格的完美结构化提取、图片内容的理解与描述，甚至是对整个页面**版面布局信息的把握**（我期待能出现直接输出带坐标的图文结构的单个模型，而非仅仅是OCR文本，也非仅仅是坐标，而是一体化的模型，因为一体化的模型能够减少流程中犯错之后难以修改，举例来说，表格区域如果判断错误，那么表格内容提取的模型出错也是必定的，流程越多，错误发生的概率越高，这是必然的）。更进一步，可以利用大模型对提取结果进行**智能后处理**，如**修复OCR识别错误**、**去除无关内容**（如页脚、意外提取的脚注）、甚至**根据语义和视觉信息优化排版**。

#### **Agentic 智能文本分块**

文本分块也将摆脱固定规则的束缚。Agent能够**根据内容的语义、上下文关联以及下游任务的需求，动态、智能地决定最佳的分割点和分块策略**。在分块过程中，Agent还能主动进行**去重、消除歧义表达、补充必要的背景元数据**，从而极大提升每个文本块的“信息纯度”和“自解释性”。又或者我们不需要分块了，到时候embedding模型会不会已经达到超级长的上下文空间了，又或者是对话的大语言模型直接就能够达到近乎无限的上下文空间呢。![未来RAG系统：agentic 工作流与智能化](https://mmbiz.qpic.cn/mmbiz_jpg/OpQ5uW8ao3ic6JYXiccGlNZECmYXJYYMicdxTVIiaeucbd4UF4hQeILRzFdjlPzibk6fCm8ZqBcCqE1vMiaRaCmKpdrA/640?wx_fmt=jpeg&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1)

## 最后的最后

行文至此，本文的分享也临近尾声。

然而，RAG 的探索之路远未结束，仍有诸多引人深思的议题值得我们进一步探究：

**1. 何时以及如何对 Embedding 模型进行微调以适应特定领域？**

**2. 多模态 RAG 中的检索与数据处理应如何设计？**

**3. Agentic RAG 的引入，其成本效益与传统的模型微调相比又将如何权衡？**

**4. 关于搜索，你还有更多的答案吗？**

这些开放性问题，既是挑战也是机遇，留待我们在未来的实践中共同求索与验证。我热切期待与各位同仁一道，继续在这条充满魅力的 RAG 之路上并肩探索。希望愿在不远的将来，当大家提及“筱可”，便能联想到我是个RAG的探索实践者。

---

## 📒 总结与展望

🚀 **核心要点回顾**

- **RAG核心机制**：RAG的本质是“检索（Retrieval）-增强（Augmented）-生成（Generation）”，其核心在于通过高质量的外部知识检索来增强大语言模型的表现。
    
- **检索为王**：在RAG系统中，检索管道的质量直接决定了最终效果。优化检索是提升RAG性能的关键，远比单纯调整生成模型更重要。
    
- **检索管道关键技术**：一个健壮的检索管道通常包括：查询预处理（规范化、意图理解）、多路召回（如BM25的词法匹配 + 向量的语义匹配）、结果融合与Rerank精排（使用Cross-Encoder等模型提升精度）。
    
- **知识库构建是基石**：高质量的知识库是RAG系统的根基。关键环节包括：数据清洗、精细化的文本切分（推荐结合文档结构的混合分块策略）、丰富的元数据、有效去重，以及针对复杂文档（特别是含表格的PDF）的务实处理策略（如人工介入或Text-to-SQL）。
    
- **框架与自建的权衡**：现有RAG框架适合快速原型验证，但在生产环境中，面对复杂需求和性能极致优化时，基于底层原理理解的自建或深度定制往往是更优选择。
    
- **元认知驱动学习**：在技术探索中，持续反思自身的认知过程、识别知识盲区、主动探究原理，是实现从“调包侠”到“问题解决者”转变的关键。
    
- **Agentic RAG展望**：未来的RAG系统将更加智能化、自动化，通过Agent驱动的模块化工作流，实现更智能的用户交互、动态检索策略、自维护知识库以及无损文档解析与智能分块。
    

---

♻️ **互动思考**

1. 在RAG实践中，遇到的最大挑战是哪个环节（查询理解、检索、知识库构建、生成效果）？您是如何尝试解决的？
    
2. 对于处理特定类型的复杂文档（如包含大量表格的财报PDF、图文混排的设计文档等），你认为除了Text-to-SQL，还有哪些值得探索的有效策略？
    
3. 你认为在当前技术条件下，最有可能率先实现突破的Agentic RAG应用方向会是哪个？
    

欢迎在留言区分享你的想法，每条留言我都会认真阅读！你的反馈是我创作的最大动力 ❤️💗 立即行动1. 点赞、收藏、关注，分享给朋友。2. 为账号加星标，获取更多内容。3. 以本文为灵感，整理一份属于自己的笔记。🚗 行动召唤"与其等着 AI 改变世界，不如自己参与变革！在这里，让 AI 成为你弯道超车的秘密武器。"