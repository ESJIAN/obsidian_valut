
# Celsius架构

# Kelvin架构

# Rankine架构


# Curie架构

# Tesla 架构

# Fermi架构
## 1. GPU计算简史

GPU自1999年NVIDIA发明以来，是最普及的并行处理器。被人们对栩栩如生的实时图形渲染的强烈渴望所推动，GPU已经演变成了一个具有前所未有的浮点处理和可编程能力的处理器。如今的GPU在算数吞吐和内存带宽方面已经大大超越CPU，使之成为加速数据并行应用的理想处理器。

自2003年以来，人们一直在努力利用GPU进行非图形应用。通过使用高级着色语言，如DirectX、OpenGL和Cg，各种数据并行算法被移植到了GPU上。诸如蛋白质折叠、股票期权定价、SQL查询和MRI重建等问题在GPU上实现了显著的性能加速。这些早期使用图形API进行通用计算的努力被称为GPGPU程序。

虽然GPGPU模型展示了巨大的加速能力，但它面临着一些缺点。

- 首先，它要求程序员对**图形API和GPU架构有深入的了解**。
- 其次，问题必须以顶点坐标、纹理和着色器程序的**形式表达**，大大增加了程序的复杂性。
- 第三，基本的编程特性，如对**内存的随机读写操作，并不支持**，这大大限制了编程模型。
- 最后，在最近之前**缺乏双精度**支持意味着一些科学应用无法在GPU上运行。

为解决这些问题，NVIDIA引入了两项关键技术

- **G80**统一图形计算架构：（首次出现在GeForce 8800®、Quadro FX 5600®和Tesla C870® GPU中）
- **[CUDA](https://zhida.zhihu.com/search?content_id=228735531&content_type=Article&match_order=1&q=CUDA&zhida_source=entity)**：这是一种软件和硬件架构，使GPU能够使用各种高级编程语言进行编程。

这两项技术共同代表了一种使用GPU的新方式。程序员不再使用专用的图形API对图形单元进行编程，而是可以使用具有CUDA扩展的C程序，针对通用的、高度并行的处理器进行编程。我们把这种新的GPU编程方式称为“GPU计算”——它代表了更广泛的应用支持、更广泛的编程语言支持，与早期的“GPGPU”编程模型有着明显区分。

### 1.1. [G80架构](https://zhida.zhihu.com/search?content_id=228735531&content_type=Article&match_order=1&q=G80%E6%9E%B6%E6%9E%84&zhida_source=entity)

NVIDIA的GeForce 8800令GPU计算模型焕然新生。该产品于**2006年11**月推出，基于G80的GeForce 8800为GPU计算带来了几个关键创新点：

- G80是第一个**支持C语言**的GPU，使程序员能够在不学习新编程语言的情况下利用GPU的计算能力。
- G80是第一个用**单一统一处理器**替代独立的顶点和像素管道的GPU，该处理器执行顶点、几何、像素和计算程序。
- G80是第一个使用**标量线程（Scalar processors）**处理器的GPU，消除了程序员需要手动管理向量寄存器的麻烦。
- G80引入了**单指令多线程（SIMT）**执行模型，多个独立线程并发执行单个指令。
- G80引入了**共享内存**和**屏障同步**，以实现线程间通信。

### 1.2. [GT200架构](https://zhida.zhihu.com/search?content_id=228735531&content_type=Article&match_order=1&q=GT200%E6%9E%B6%E6%9E%84&zhida_source=entity)

**2008年6月**，NVIDIA对G80架构进行了重大升级调整，带来了第二代统一架构GT200（首次出现在GeForce GTX 280、Quadro FX 5800和Tesla T10 GPU中）

- 将流处理器核心（之后称为CUDA核心）的数量从**128增加到240**。
- 每个处理器**寄存器文件的大小加倍**，允许更多的线程在芯片上同时执行。
- **硬件内存访问合并**被添加以提高内存访问效率。
- 还添加了**双精度浮点**支持以满足科学和高性能计算（HPC）应用的需求。

在设计每一代新的GPU时，NVIDIA的理念一直是同时提高现有应用程序的性能和GPU的可编程性。虽然更快的应用程序性能带来即时的好处，但持续不断的GPU可编程性进步才使它成为我们时代最通用的并行处理器。基于这种思想，我们开始开发GT200架构的下一代新架构。

### 1.3. 新一代架构Fermi

费米架构是自G80以来GPU架构的最重大飞跃。G80是我们对统一的图形和计算并行处理器的初始设想。GT200扩展了G80的性能和功能。通过费米架构，我们将从前两个处理器以及为它们编写的所有应用程序中学到的一切，采用全新的设计方法创建了世界上第一款计算GPU。费米的设计基础采纳了自G80和GT200推出以来的大量用户反馈，重点关注以下关键领域的改进：

- **改进双精度性能**：尽管单精度浮点性能比桌面CPU高出十倍，但一些GPU计算应用程序也需要更多的双精度性能
- **ECC支持**：ECC允许GPU计算用户在数据中心安装大量GPU，并确保像医学成像和金融期权定价这样的数据敏感应用程序受到内存错误的保护（内存容错能力）
- **真正的缓存层次结构**：一些并行算法无法使用GPU的共享内存，并且用户需要一个真正的缓存架构来帮助他们
- **更多的共享内存**：许多CUDA程序员要求超过16 KB的SM共享内存来加速他们的应用程序
- **更快的上下文切换**：用户要求更快的应用程序之间的上下文切换和更快的图形和计算互操作
- **更快的原子操作**：用户要求更快的读取-修改-写入原子操作来执行其并行算法

出于这些需求的考虑，Fermi团队设计了新一代处理器，它大大增加了计算能力，并通过架构创新，显著提高了可编程性和计算效率。Fermi的关键架构亮点如下：

- **第三代流式多处理器（SM）**

- 每个SM有**32**个CUDA核心，比GT200多**4倍**
- **双精度浮点**峰值性能比GT200提高**8倍**
- **两个warp调度器**，可以同时调度和分发指令给两个独立的warp
- **64KB RAM**，可供共享内存和L1缓存配置化划分使用

- **第二代并行线程执行ISA（Instruction Set Architecture指令集架构）**

- 统一地址空间，完全支持**C++**
- 针对OpenCL和DirectCompute进行了优化
- 完全支持**[IEEE 754-2008](https://zhida.zhihu.com/search?content_id=228735531&content_type=Article&match_order=1&q=IEEE+754-2008&zhida_source=entity)** 32位和64位精度
- 具有64位扩展的完整32位整数路径
- 内存访问指令支持向64位寻址的过渡
- 通过**预测**提高性能

- **改进的内存子系统**

- 具有**可配置L1和统一L2高速缓存**的NVIDIA Parallel DataCacheTM层次结构
- 支持**ECC**内存的第一款GPU
- 大大提高原子内存操作性能

- **NVIDIA GigaThreadTM引擎**

- 应用程序上下文**切换速度提高了10倍**
- 并发内核执行
- 无序线程块执行
- **双向可重叠**的内存传输引擎

### 1.4. CUDA简要回顾

CUDA是硬件和软件兼具的架构，它使NVIDIA GPU可以执行由C，C++，Fortran，OpenCL，DirectCompute和其他语言编写的程序。一个CUDA程序调用许多并行执行的内核。内核在一组并行线程中并行执行。程序员或编译器将这些线程组织成线程块（thread blocks）和网格（grids），GPU在线程块并行的网格上实例化内核程序。每个线程在线程块内执行内核实例，并具有块内的线程ID、程序计数器、寄存器、私有内存、输入和输出结果。

一个线程块是一组并行执行的线程，它们可以通过屏障同步和共享内存相互合作。一个线程块在其网格中有一个块ID。网格是一组执行相同内核的线程块，从全局内存读取输入，将结果写入全局内存，对于有依赖的内核之间会有同步操作。在CUDA并行编程模型中，每个线程具有一个线程专用的私有内存空间，用于寄存器溢出、函数调用和类C的自动数组变量。每个线程块都具有一个块共享内存空间，用于并行算法中的线程间通信、数据共享和结果共享。线程块的网格在内核范围的全局同步后，共享全局内存空间中的结果。

![](https://picx.zhimg.com/v2-971d49610313acf2198ca30623accbc3_1440w.jpg)

### 1.5. 硬件执行

CUDA的线程层次结构映射到GPU上的处理器层次结构；GPU执行一个或多个内核网格；流多处理器（SM）执行一个或多个线程块；SM中的CUDA核心和其他执行单元执行线程。 SM以32个线程为一组执行线程，称为warp。虽然程序员通常可以忽略warp执行以实现功能正确性，只考虑编程一个线程，但是他们可以通过让warp中的线程执行相同的代码路径并访问相邻地址的内存来大大提高性能。

## 2. Fermi

第一款基于Fermi架构的GPU具有**30亿个晶体管**，最多可配备**512个CUDA核心**。每个CUDA核心每个时钟周期可以执行一个**浮点数或整数**指令。512个CUDA核心分为16个SM，每个SM包含32个核心。拥有六个64位内存分区，共384位的内存接口，支持最多**6GB**的GDDR5 DRAM内存。主机接口通过**PCI-Express**连接GPU和CPU。**GigaThread**全局调度器将线程块分配给SM线程调度器。

内存接口位数是显卡或其他设备与内存之间传输数据时的数据位数。它的作用主要体现在两个方面：

- **内存带宽**：内存接口位数越高，设备每秒钟可以传输的数据量就越大，带宽就越大。带宽的大小直接影响设备的性能。
- **内存容量**：内存接口位数和内存容量之间有一定的关系。例如，如果显卡的内存接口位数为256位，那么它对应的内存容量通常为2GB或4GB。因为一般来说，内存容量越大，数据位数也就越大，需要更高的内存接口位数才能满足数据传输的需求。

因此，内存接口位数对于显卡或其他设备的性能和内存容量都有很大的影响，是一项非常重要的技术指标。

![](https://pica.zhimg.com/v2-f4f3d7edbdeef3f0f55f0a04c5e7b25c_1440w.jpg)

## 2.1. 第三代流式多核处理器

第三代SM引入了几项架构创新，使之不仅成为迄今为止最强大的SM，而且可编程性也出类拔萃，非常高效。

![](https://pic3.zhimg.com/v2-aef3125b3acd713a9b3498724f65a08c_1440w.jpg)

### 2.1.1. 512个高性能CUDA cores

每个SM都具有32个CUDA处理器，是之前SM的四倍。每个CUDA处理器都有完全**流水线化**的整数算术逻辑单元（ALU）和浮点单位（FPU）。先前的GPU使用IEEE 754-1985浮点算术。 Fermi架构实现了新的IEEE 754-2008浮点标准，为单精度和双精度算术提供了**融合乘加（FMA）**指令。FMA将乘法和加法操作 **合并为一步**，减少了指令的数量和指令执行的时间。在执行MAD指令时，需要先进行乘法运算，然后再进行加法运算，这会导致额外的舍入误差和复杂的指令流程。而FMA将这两个操作合并为一个操作，同时保持了相同的精度，减少了舍入误差，提高了计算的准确性和效率。因此，FMA更加高效，能够在相同时间内执行更多的计算。 GT200实现了双精度FMA。

在GT200中，整数ALU在乘法操作上仅支持24位精度，因此需要使用多个指令来模拟整数算术。在Fermi中，全新设计的整数ALU**支持所有指令的32位精度**，符合标准编程语言要求。整数ALU还经过优化，能够高效地支持64位和扩展精度操作。支持各种指令，包括布尔运算、移位、移动、比较、转换、位域提取（从一个字中提取出特定位数的二进制位，并将它们作为一个字或一个字节的值返回）、位反转插入（将一个字或一个字节的二进制位按照相反的顺序插入到另一个字中的指定位置）和位统计（用于计算二进制数中1的个数，也被称为"位计数"）。

### 2.1.2. Load/Store 单元

每个SM有16个Load/Store单元，允许16个线程每个时钟周期计算源和目的地址，支持将每个地址的数据读取和存储到缓存或DRAM中。

### 2.1.3. 4个特殊函数单元

特殊函数单元（SFU）执行三角函数指令，如正弦、余弦、倒数和平方根。每个SFU在每个时钟周期内为每个线程执行一个指令；一个warp需要执行8个时钟周期。SFU流水线与调度单元解耦，使得调度单元可以向其他执行单元发出指令，同时SFU正忙。

### 2.1.4. 双精度设计

双精度算术是HPC应用程序（如线性代数、数值模拟和量子化学）的核心。Fermi架构专门设计以在双精度计算方面提供前所未有的性能；每个SM每个时钟周期可以执行高达16个双精度融合乘加操作，这是对GT200架构的显著改进。

![](https://pic1.zhimg.com/v2-6af20a41bd44b71b36a53f0e6fa58aac_1440w.jpg)

### 2.1.5. 双warp调度器

SM将32个并行的线程归为一组称为线程束（即warp），以此为单位进行调度。每个SM具有两个warp调度器和两个指令分派单元，允许同时有两个warp被分派指令和执行。Fermi的双warp调度器选择两个warp，每个warp的指令会发送到16个核心、16个加载/存储单元或4个SFU中。由于warp独立执行，Fermi的调度器不需要从指令流中检查依赖关系。使用这种优雅的双分发模型，Fermi实现了接近硬件峰值的性能。

![](https://picx.zhimg.com/v2-e1c1f02ad0c3ee26839fa2dfe8882199_1440w.jpg)

大多数指令都可以双发出；两个整数指令、两个浮点指令或混合发出整数、浮点、加载、存储和 SFU 指令。双精度指令不支持与任何其他操作的双重调度。

### 2.1.6. 64KB可配置的共享内存和L1缓存

其中一个核心架构创新是片上共享内存，它极大地提高了GPU应用程序的可编程性和性能。共享内存使得同一线程块内的线程可以合作，便于芯片内数据的复用，大大减少了芯片外的访问。共享内存是许多高性能CUDA应用程序的实现基石。

在G80和GT200架构中，每个SM有16KB的共享内存。在Fermi架构中，每个SM有64KB的芯片内存，可以配置为48KB的共享内存和16KB的L1缓存，或者配置为16 KB的共享内存和48KB的L1缓存。

对于现有的大量使用共享内存的应用程序，共享内存增加三倍可以显著提高性能，特别是对于那些带宽受限的情况。对于把共享内存当缓存使用（代码控制）的现有应用程序，可以优化代码以利用硬件缓存系统，同时仍然可以访问至少16KB的共享内存以进行显式线程协作。更重要的是，不使用共享内存的应用程序将自动受益于L1缓存，因此可以使用最少的时间和精力构建高性能的CUDA程序。

### 2.1.7. G80/GT200/Fermi对比

![](https://pic3.zhimg.com/v2-141b53d50561b126822d38bac9d98522_1440w.jpg)

## 2.2. 第二代并行线程执行ISA

ISA是指指令集架构（Instruction Set Architecture），是一种用于定义计算机硬件和软件接口的标准规范，即规定了计算机的指令集以及如何执行这些指令的规则。ISA是计算机体系结构的基础，对于不同的处理器架构，其ISA也会有所不同。

Fermi是第一个支持新的Parallel Thread eXecution**（PTX）2.0**指令集的架构。PTX是一个低级虚拟机和ISA，旨在支持并行线程处理器的操作。在程序安装时，PTX指令会被GPU驱动程序翻译成机器指令。

**PTX的主要目标是:**

- 提供跨多个GPU代系的稳定ISA
- 让完成编译的应用程序充分发挥GPU的性能
- 为C、C++、Fortran和其他编译器目标提供机器无关的ISA
- 为应用程序和中间件开发人员提供代码分发ISA
- 为优化代码生成器和翻译器提供一个通用的ISA，将PTX映射到特定目标机器上
- 促进库和性能内核的手工编码
- 提供可扩展的编程模型，实现少量核心到大量核心的跨越

[PTX 2.0](https://zhida.zhihu.com/search?content_id=228735531&content_type=Article&match_order=1&q=PTX+2.0&zhida_source=entity)引入了几个新功能，大大改善了GPU的可编程性、精度和性能。其中包括：完整的IEEE 32位浮点精度、所有变量和指针的统一地址空间、64位寻址以及针对OpenCL和DirectCompute的新指令。最重要的是，PTX 2.0专门设计为完全支持C++编程语言。

### 2.2.1. 统一地址空间以支持C++

Fermi和PTX 2.0 ISA实现了一个统一的地址空间，将加载和存储操作的三个独立地址空间（线程私有的本地地址空间，块共享地址空间和全局地址空间）统一起来。在PTX 1.0中，加载/存储指令是针对三个地址空间中的一个具体指定的；程序在编译时可以确定需要加载/存储的目标地址空间。由于指针的目标地址空间可能在编译时不可知，有些情况下只能在运行时动态确定，因此实现C和C++指针是非常困难的。

在PTX 2.0中，统一地址空间将所有三个地址空间统一到一个连续的地址空间中。一个统一的加载/存储指令集操作这个地址空间，增强了本地，共享和全局内存的三个独立加载/存储指令集。40位的统一地址空间支持1TB的可寻址内存，并且加载/存储ISA支持64位寻址以支持未来的增长。

  

![](https://pic1.zhimg.com/v2-90b3c2ed987d8dea4977c9c80deb708a_1440w.jpg)

实现统一地址空间的设计使得Fermi能够支持真正的C++程序。在C++中，所有变量和函数都驻留在对象中，这些对象通过指针传递。PTX 2.0可以使用统一指针在任何内存空间中传递对象，然后Fermi的硬件地址转换单元会自动将指针引用映射到正确的内存空间。Fermi和PTX 2.0 ISA还增加了对**C++虚函数**、函数指针以及**new**和**delete**运算符的支持，以进行动态对象的分配和释放。同时，也支持C++异常处理操作**try**和**catch**。

### 2.2.2. OpenCL和DirectCompute优化

OpenCL和DirectCompute与CUDA编程模型密切相关，共享抽象设计：线程、线程块、线程块网格、屏障同步、块内共享内存、全局内存和原子操作等。Fermi是第三代CUDA架构，天生就针对这些API进行了优化。此外，Fermi还提供了OpenCL和DirectCompute表面指令的硬件支持，包括格式转换，使得图形和计算程序能够轻松地在同一数据上进行操作。PTX 2.0 ISA还增加了对DirectCompute指令位统计、附加和位反转的支持。

### 2.2.3. IEEE 32位浮点精度

subnormal number(非规格数)即**将指数位全部填充为0**的数, 也叫作denormal number, 可以理解为"低于正常数的数"，硬件中现在默认支持非规格数的单精度浮点指令，以及四种IEEE 754-2008舍入模式（最近、零、正无穷和负无穷）。

非规格数是指位于零和给定浮点数系统中最小归一化数之间的小数。前一代GPU会将非规格操作数和结果刷新为零，从而损失精度。CPU通常在异常处理软件中执行非规格计算，需要数千个周期。Fermi的浮点单元在硬件中处理非规格数，允许值逐渐下溢到零，而不会降低性能。

在计算机图形、线性代数和科学应用中经常使用的一系列操作是将两个数相乘，将乘积加到第三个数中，例如，D = A × B + C。前一代GPU使用乘加（MAD）指令加速了这个函数，该指令允许在单个时钟中执行两个操作。MAD指令执行带截断的乘法，然后进行四舍五入的加法。Fermi为32位单精度和64位双精度浮点数都实现了新的融合乘加（FMA）指令（GT200仅支持双精度），它通过在中间阶段保留完整的精度来改进乘加。精度的提高有利于许多算法，例如呈现细微交错的几何图形、在迭代数学计算中提高精度以及快速、精确舍入的除法和平方根操作。

![](https://pica.zhimg.com/v2-ca176c80d235ccc0df31f84fc6a106ba_1440w.jpg)

### 2.2.4. 通过分支预测提高执行效率

在Fermi ISA中，原生的硬件支持分支预测并用于处理线程发散（divergent），现在在指令级别也可用了。预测使得短条件代码段能够高效地执行且没有分支指令开销。

  

## 2.3. 内存子系统创新

### 2.3.1. NVIDIA并行数据缓存，具有可配置的L1和统一的L2缓存

通过研究来自各行各业数百个GPU计算应用程序的工作，我们了解到，虽然共享内存对许多问题都有好处，但并不适用于所有问题。一些算法自然地适用于共享内存，另一些需要缓存，而另一些则需要两者的组合。最佳的内存层次结构应该同时提供共享内存和缓存的优点，并允许程序员选择其分区。Fermi内存层次结构适应于两种类型的程序行为。

![](https://pica.zhimg.com/v2-431ea05abdb7cd5dcf1b27a49c80863e_1440w.jpg)

为加载/存储操作添加真正的缓存层次结构带来了重大的挑战。传统的GPU架构支持只读的纹理操作“加载”路径和只写的像素数据输出“导出”路径。然而，这种方法不适用于执行期望读写有序的通用C或C++线程程序。例如，将寄存器操作数溢出到内存，然后将其读回会造成读后写冲突。如果读写路径是分开的，则可能需要在发出读取之前显式刷新整个写入“导出”路径，而读取路径上的任何缓存都不会与写入数据一致。

RAW冲突指的是数据相关（Data Dependency）的一种，也称为读后写冲突（Read After Write Hazard）。当一个指令对某个数据进行写操作后，另一个指令在此之后读取同一数据，就会发生RAW冲突，也就是读取操作可能得到一个过期的、错误的值，从而导致程序出错。

例如，以下是一个简单的代码片段：

```text
a = 1
b = a + 2
c = b + 3
```

在这个代码片段中，第二行的指令 (b = a + 2) 依赖于第一行的指令 (a = 1)，因为它需要读取 a 的值。如果第一行和第二行的指令没有正确的控制和顺序，可能会导致第二行的指令读取到 a 的旧值，从而得出错误的结果。

因此，为了避免RAW冲突，需要对指令进行正确的调度和排序，以确保每个指令在执行前，其所需要的数据都已经准备好。在现代计算机体系结构中，通常使用流水线、乱序执行、指令缓存等技术来解决RAW冲突和其他相关的时序问题。

Fermi架构通过实现单一的统一内存请求路径来解决这个挑战，用于加载和存储操作，每个SM多处理器都有一个L1缓存，统一的L2缓存用于服务所有操作（加载、存储和纹理）。每个SM的L1缓存可配置为支持共享内存和本地和全局内存操作的缓存。这64 KB的内存可以配置为48 KB的共享内存和16 KB的L1缓存，或者16 KB的共享内存和48 KB的L1缓存。当配置为48 KB的共享内存时，对共享内存使用频繁的程序（例如电动力学模拟）可以提高高达三倍的性能。对于不事先了解其内存访问情况的程序，48 KB的L1缓存配置比直接访问DRAM提供了大大提高的性能。

![](https://pica.zhimg.com/v2-f71e19d794e7b981ba7cf51385da9b3a_1440w.jpg)

在任一配置中，当复杂程序的临时寄存器发生溢出时，因为L1高速缓存的存在，性能也会得以提升。上一代GPU直接将溢出的寄存器写入DRAM中，增加了访问延迟。而使用L1缓存，随着临时寄存器的使用量增加，性能会逐渐提高，这种性能提升具有优雅的扩展性。

Fermi架构拥有一个768KB的统一L2缓存，服务于所有的加载、存储和纹理请求。L2提供了高效的、高速的数据共享，可在GPU内部实现数据共享。对于那些数据地址不事先已知的算法，如物理求解器、光线追踪和稀疏矩阵乘法等，特别受益于缓存层次结构。需要多个SM读取同一数据的滤波器和卷积内核也可以受益于L2缓存。

  

![](https://pic4.zhimg.com/v2-0f03a37f56277450a7c9923e385eee9f_1440w.jpg)

### 2.3.2. 第一款支持[ECC内存](https://zhida.zhihu.com/search?content_id=228735531&content_type=Article&match_order=1&q=ECC%E5%86%85%E5%AD%98&zhida_source=entity)的GPU

Fermi是第一款支持基于纠错编码（ECC）的内存数据保护的GPU。GPU计算用户要求增强高性能计算环境中的数据完整性，因此支持ECC技术。ECC是医学成像和大规模集群计算等领域的一个非常重要的特性。

> ECC是Error Correction Code（错误校正码）的缩写，是一种能够检测和纠正计算机内存中出现的错误的技术。ECC技术通过在内存中添加额外的校验码来实现，这些校验码能够检测到并纠正位于内存中的特定位的错误，从而保证数据的完整性和准确性。ECC技术通常用于对数据安全性要求较高的系统中，例如服务器、高性能计算集群和科学研究中心等。

自然辐射可能会导致存储在内存中的数据位被改变，从而产生软错误。ECC技术可以在软错误影响系统之前检测和纠正单比特软错误。由于这种辐射诱发错误的概率随着安装系统数量的线性增加，因此在大型集群安装中，ECC是一个必不可少的要求。

Fermi支持单错误纠和正双错误检测（SECDED）ECC代码，在数据访问时硬件可以纠正任何单比特错误。此外，SECDED ECC确保所有双比特错误和许多多比特错误也可以被检测和报告，以便程序可以重新运行而不是继续使用错误的数据执行。

Fermi的寄存器文件、共享内存、L1缓存、L2缓存和DRAM内存都经过了ECC保护，使其不仅是最强大的用于HPC应用的GPU，而且是最可靠的。此外，Fermi还支持行业标准，用于检查从芯片到芯片传输期间的数据。所有NVIDIA GPU都支持PCI Express标准，用于在数据链路层进行CRC检查和重试。Fermi还支持类似的GDDR5标准，在内存总线上传输数据期间进行CRC检查和重试（也称为“EDC”）。

### 2.3.3. 快速原子内存操

原子内存操作在并行编程中非常重要，允许并发线程正确地对共享数据结构进行读取-修改-写入操作。像加、减、最大值、最小值和比较与交换等原子操作在读取、修改和写入的时候没有其他线程干扰的情况下是原子的。原子内存操作广泛用于并行排序、规约操作和并行构建数据结构，而不需要使用会使线程执行串行化的锁。

由于硬件中原子单元的数量增加以及L2缓存的添加，相比GT200架构，Fermi架构的原子操作性能提高了高达**20**倍。

## 2.4. Giga线程调度

Fermi架构中最重要的技术之一是其两级分布式线程调度器。在芯片级别上，全局工作分配引擎将线程块调度到各个SM，而在每个SM级别上，每个warp调度器分配具有32个线程的warp到其执行单元。首代G80中引入的GigaThread引擎可以实时管理多达12,288个线程。Fermi架构在此基础上进行了改进，不仅提供了更大的线程吞吐量，而且实现了大幅度更快的上下文切换、并发内核执行和改进的线程块调度。

### 2.4.1. 10倍的应用上下文切换

与CPU一样，GPU通过上下文切换支持多任务处理，每个程序都会获得处理器资源的时间片。Fermi流水线经过优化，将应用程序上下文切换的成本降低到**25微秒**以下，这比上一代GPU有了显著的改进。除了提高性能外，这还允许开发人员创建更多利用内核之间频繁通信的应用程序，例如在图形和PhysX应用程序之间进行精细的互操作。

### 2.4.2. 并行kernel执行

Fermi支持并发内核执行，同一应用程序上下文的不同内核可以同时在GPU上执行。并发内核执行允许执行多个小内核的程序利用整个GPU。例如，一个PhysX程序可以调用一个流体求解器和刚体求解器，如果顺序执行，只会使用可用线程处理器的一半。在Fermi架构上，同一CUDA上下文的不同内核可以并发执行，充分利用GPU资源。来自不同应用程序上下文的内核仍然可以以极高的效率顺序运行，这要归功于改进的上下文切换性能。

![](https://pic1.zhimg.com/v2-3c6cdd27f490a8c93728df8feffa307a_1440w.jpg)

## 2.5. 总结

在过去的16年中，NVIDIA一直致力于构建全球最快的图形处理器。虽然G80是GPU计算领域的先驱性架构，GT200是一个重大的改进，但它们的设计仍然深深扎根于图形领域。Fermi架构代表着NVIDIA的一种新方向。Fermi不仅仅是GT200的后继者，而是对GPU的角色、目的和能力进行了彻底的重新思考而产生的结果。

Fermi团队并没有简单地增加执行单元，而是解决了GPU计算中一些最棘手的问题。Fermi通过其两级缓存层次结构和组合的加载/存储内存路径，认识到数据本地性的重要性。双精度性能提高到超级计算水平，而原子操作的执行速度提高了多达二十倍。最后，Fermi全面的ECC支持强烈证明了我们对高性能计算市场的承诺。

在软件方面，Fermi架构推出了对C++的支持，这是世界上最普遍的面向对象编程语言，以及Nexus，世界上第一个专为大规模并行GPU计算应用程序设计的集成开发环境。

Fermi架构以其突破性的性能、功能和可编程性相结合，代表了GPU计算领域的下一次革命。



## 2.6 参考链接

[Nvidia Fermi架构白皮书​www.nvidia.com/content/pdf/fermi_white_papers/nvi](https://link.zhihu.com/?target=https%3A//www.nvidia.com/content/pdf/fermi_white_papers/nvidia_fermi_compute_architecture_whitepaper.pdf)


# Kepler架构
## 1. Kepler GK110/210 GPU架构

随着科学、医学、工程和金融各领域对高性能并行计算需求的增加，英伟达™ (NVIDIA®) 以无比强大的 GPU 计算架构来不断创新和满足这种需求。英伟达现有的 [Fermi GPU](https://zhida.zhihu.com/search?content_id=228979375&content_type=Article&match_order=1&q=Fermi+GPU&zhida_source=entity)已经重新定义和加速了以下领域的高性能计算（HPC）的功能，如地震处理、生化模拟、天气和气候建模、信号处理、计算金融、计算机辅助工程、计算流体力学和数据分析。英伟达的新 Kepler GK110 GPU 大大提高了并行计算标准，并将会帮助解决世界上面临的最困难的计算问题。

通过提供比上一代 GPU 更强大的处理功能以及优化和提高 GPU 上并行执行工作负载的新方法，Kepler GK110 简化了并行程序的创建，将对会对高性能计算引起进一步改革。

![](https://pic1.zhimg.com/v2-14631024aff59e876fc6b68256323baa_1440w.jpg)

## 2. Kepler高性能计算

Kepler GK110 由 71 亿个晶体管组成，不仅速度最快，而且还是有史以来架构最复杂的微处理器。GK110 新加了许多注重计算性能创新功能，目的是要成为英伟达™ Tesla® 和 HPC 市场上 的并行处理动力站。

Kepler GK110 会提供超过每秒 **1 万亿次双精度浮点**计算的吞吐量，DGEMM 效率大于 80%， 而之前的 Fermi 架构的效率是 60‐65%。

除了大大提高的性能之外，Kepler 架构在电源效率方面有 3 次巨大的飞跃，使 Fermi 的性能/功率比提高了 3 倍

![](https://pica.zhimg.com/v2-7b2bfb20e2811cb1fe5f0aef97e07ccc_1440w.jpg)

Kepler GK110 模具照片

Kepler GK110 的以下新功能提高 GPU 的利用率，简化了并行程序设计，并有助于 GPU 在各种计算环境中部署，无论是从个人工作站还是到超级计算机： 

- **Dynamic Parallelism**： 能够让 GPU 在无需 CPU 介入的情况下，通过专用加速硬件路径为自己创造新的工作，对结果同步，并控制这项工作的调度。这种灵活性是为了适应程序执行过程中并行的数量和形式，编程人员可以处理更多的各种并行工作，更有效的将 GPU 用为计算用途。此功能允许结构较简单，一但较复杂的任务方便有效地运行，能使较大部分的应用程序在整个 GPU 上运行。此外，程序能够更容易的创建，CPU 能为其他任务释放。
- **[Hyper-Q](https://zhida.zhihu.com/search?content_id=228979375&content_type=Article&match_order=1&q=Hyper-Q&zhida_source=entity)**：Hyper-Q 允许多个 CPU 核同时在单一 GPU 上启动工作，从而大大提高了 GPU 的利用率并削减了 CPU 空闲时间。Hyper‐Q 增加了主机和 Kepler GK110 GPU 之间的连接总数（工作队列），允许 32 个并发、硬件管理的连接（与 Fermi 相比，Fermi 只 允许单个连接）。Hyper-Q 是一种灵活的解决方案，允许来自多个 CUDA 流、多个消息传递接口（MPI）进程，甚至是进程内多个线程的单独连接。以前遇到跨任务虚假串行化的应用程序，限制了 GPU 的利用率，而现在无需改变任何现有代码性能就能大幅度提升。
- **Grid Management Unit**：使 Dynamic Parallelism 能够使用先进、灵活的 GRID 管理和调度控制系统。新 GK110 Grid Management Unit (GMU) 管理并按优先顺序在 GPU 上执行的 Grid。GMU 可以暂停新 GRID 和等待队列的调度，并能中止 GRID，直到其能够执行时为止，这为 Dynamic Parallelism 这样的强大运行提供了灵活性。GMU 确保 CPU和 GPU产生的工作负载得到妥善的管理和调度。 
- **英伟达™ GPUDirect™**：英伟达™ GPUDirect™ 能够使单个计算机内的 GPU 或位于网络内不同服务器内的 GPU 直接交换数据，无需进入 CPU 系统内存。GPUDirect 中的 RDMA 功能允许第三方设备，例如 SSD、NIC、和 IB 适配器，直接访问相同系统内多个 GPU 上的内存，大大降低 MPI 从 GPU 内存发送/接收信息的延迟。还降低了系统内存带宽的要求并释放其他 CUDA 任务使用的 GPU DMA 引擎。Kepler GK110 还支持其他的 GPUDirect 功能，包括 Peer‐to‐Peer 和 GPUDirect for Video。

## 3. Kepler GK110/210 架构概述

Kepler GK110 专为英伟达™ Tesla® 打造，其目标是成为世界上并行计算性能最高的微处理器。GK110 不仅大大超过由 Fermi 提供的原始计算能力，而且非常节能，显著减少电力消耗，同时产生的热量更少。

GK110和GK210旨在提供快速的双精度计算性能，加速专业的高性能计算工作负载；这是与NVIDIA Maxwell GPU架构的主要区别，后者主要设计用于快速图形性能和单精度消费者计算任务。虽然Maxwell架构的双精度计算速度为单精度计算速度的1/32，但基于Kepler的GK110和GK210 GPU能够以高达单精度计算性能的1/3的速度执行双精度计算。

完整 Kepler GK110/210 实施包括 15 [SMX 单元](https://zhida.zhihu.com/search?content_id=228979375&content_type=Article&match_order=1&q=SMX+%E5%8D%95%E5%85%83&zhida_source=entity)和六个 64 位内存控制器。不同的产品将使用 GK110 不同的配置。例如，某些产品可能部署 13 或 14 个 SMX。 在下面进一步讨论的该架构的主要功能，包括：

- 新 SMX 处理器架构
- 增强的内存子系统，在每个层次提供额外的缓存能力，更多的带宽，且完全进行了重新设计，DRAM I/O 实施的速度大大加快。
- 贯穿整个设计的硬件支持使其具有新的编程模型功能
- GK210在GK110的芯片资源基础上进行了扩展，每个SMX的可用寄存器文件和共享内存容量增加了一倍。

![](https://pic1.zhimg.com/v2-c683f4ceca15760099fdc86a4ce87aa4_1440w.jpg)

Kepler GK110 完整芯片框图

Kepler GK110 支持新 CUDA Compute Capability 3.5。下表对比了 Fermi 和 Kepler GPU 架构的不同计算能力的参数：

![](https://pic1.zhimg.com/v2-927d5bef85b1fced165b100ccd79d2a8_1440w.jpg)

Fermi 和 Kepler GPU 的计算能力

### 3.1. 性能/功率比

Kepler 架构的一个主要设计目标是提高电源效率。设计 Kepler 时，英伟达工程师应用从 Fermi 中积累的经验，以更好地优化 Kepler 、实现高效运行。台积电的 **28nm** 制造工艺在降低功耗方面起着重要的作用，但许多 GPU 架构需要修改，以进一步降低功耗，同时保持出色的性能。

Kepler 每一个硬件设备都经过设计和擦洗，以提供卓越的性能/功率比。出色性能/功率比的最佳案例是 Kepler GK110 新流式多处理器 (SMX) 中的设计，与最近 Kepler GK104 引入的 SMX 单元的许多方面类似，但计算算法包括更多双精度单位。

### 3.2. 流式多处理器（SMX）架构

Kepler GK110/GK210 的新 SMX 引入几个架构创新，使其不仅成为有史以来最强大的多处理器，而且更具编程性，更节能。

![](https://picx.zhimg.com/v2-e69a79f7f175b3619ff6cfe5f7f0fc9b_1440w.jpg)

SMX: 192 个单精度 CUDA 核、64 个双精度单元、32 个特殊功能单元 (SFU) 和 32 个加载/存储单元 (LD/ST)

#### 3.2.1. **SMX**处理核架构

每个 Kepler GK110/210 SMX 单元具有 192 单精度CUDA 核，每个核完全由‐浮点和整数算术逻辑单元组成。Kepler 完全保留 Fermi 引入的 IEEE 754-2008 标准的单精度和双精度算术，包括积和融加 (FMA) 运算。

Kepler GK110/210 SMX 的设计目标之一是大大提高 GPU 的双精度性能，因为双精度算术是许多 HPC 应用的核心。Kepler GK110/210 的 SMX 还保留了特殊功能单元 (SFU) 以达到和上一代 GPU 类似的快速超越运算，所提供的 SFU 数量是 Fermi GF110 SM 的 8 倍。

与 GK104 SMX 单元类似，GK110/210 SMX 单元内的核使用主 GPU 频率而不是 2 倍的着色频率。2x 着色频率在 G80 Tesla 架构的 GPU 中引入，并用于之后所有的 Tesla 和 Fermi‐架构的 GPU。在更高时钟频率上运行执行单元使芯片使用较少量的执行单元达到特定目标的吞吐量，这实质上是一个面积优化，但速度更快的内核的时钟逻辑更耗电。对于 Kepler，我们的首要任务是的性能/功率比。虽然我们做了很多面积和功耗方面的优化，但是我们更倾向优化功耗，甚至以增加面积成本为代价使大量处理核在能耗少、低 GPU 频率情况下运行。

#### 3.2.2. 4个warp调度器

SMX 以 32 个并行线程为一组的形式调度进程，这 32 个并行线程叫做 Warp。而每个 SMX 中拥有四组 Warp Scheduler 和八组 Instruction Dispatch 单元，允许四个 Warp 同时发出执行。Kepler 的 Quad Warp Scheduler 选择四个 Warp，在每个循环中可以指派每 Warp 2 个独立的指令。与 Fermi 不同，Fermi 不允许双精度指令和部分其他指令配对，而 Kepler GK110/210 允许双精度指令和其他特定没有注册文件读取的指令配对，例如加载/存储指令、纹理指令以及一些整数型指令。

![](https://pic4.zhimg.com/v2-b2922ccd82d40a926d33c33eeadf1701_1440w.jpg)

每个 Kepler SMX 包含 4 组 Warp Scheduler，每组 Warp Scheduler 包含两组 Instruction Dispatch 单元。单个 Warp Scheduler 单元如上所示

我们努力优化 SMX Warp Scheduler 逻辑中的能源。例如，Kepler 和 Fermi Scheduler 包含类似的硬件单元来处理调度功能。其中包括：

1. 记录长延迟操作（纹理和加载）的寄存器
2. Warp 内调度决定（例如在合格的候选 Warp 中挑选出最佳 Warp 运行）
3. 线程块级调度（例如，GigaThread 引擎）

然而，Fermi 的 scheduler 还包含复杂的硬件以防止数据相关性问题。多端口寄存器记录板会纪录任何没有有效数据的寄存器，依赖检查块针对记录板分析多个完全解码的 Warp 指令中寄存器的使用情况过，确定哪个有资格发出。

对于 Kepler，我们认识到这一信息是确定性的（数学管道延迟是不变量），因此，编译器可以提前确定指令何时准备发出，并在指令中提供此信息。这样一来，我们就可以用硬件块替换几个复杂、耗电的块，其中硬件块提取出之前确定的延迟信息并将其用于在 Warp 间调度阶段屏蔽 Warp，使其失去资格。

#### 3.2.3. 新ISA编码：每个线程 255 个寄存器

可由线程访问的**寄存器的数量**在 GK110 中已经**翻了两番**，允许线程最多访问 255 个寄存器。由于增加了每个线程可用的寄存器数量，Fermi 中承受很大寄存器压力或泄露行为的代码的速度能大大的提高。典型的例子是在 QUDA 库中使用 CUDA 执行格点 QCD（量子色动力学）计算。基于 QUDA fp64 的算法由于能够让每个线程使用更多寄存器并减少的本地内存泄漏，所以其性能提高了 5.3 倍。

GK210在GK110的基础上进一步改进，将每个SMX的**总寄存器文件容量翻倍**。通过这样做，它允许应用程序更容易地利用每个线程更多的寄存器，而不会牺牲每个SMX可同时容纳的线程数。例如，在GK110上，一个使用128个寄存器线程的CUDA核心只能在每个SMX上的2048个并发线程中占用512个，从而限制了可用的并行性。在这种情况下，GK210将并发性自动增加一倍，这**有助于覆盖算术和内存延迟，提高整体效率**。

#### 3.2.4. **Shuffle**指令

为了进一步提高性能，Kepler 采用 Shuffle 指令，它允许线程在 Warp 中共享数据。此前，Warp 内线程之间的数据共享需要存储和加载操作以通过共享内存传递数据。使用 Shuffle 指令，Warp 可以读取来自Warp 内其他线程中任意排列的值。Shuffle 支持任意索引引用（即任何线程读取任何其他线程）。有用的 Shuffle 子集包括下一线程（由固定量弥补抵消）和 Warp 中线程间 XOR “蝴蝶”式排列，也称为 CUDA 性。

Shuffle 性能优于共享内存，因此存储和加载操作能够一步完成。Shuffle 也可以减少每个线程块所需共享内存的数量，因为数据在 Warp 级交换也不需要放置在共享内存中。在 FFT 的情况下，需要共享一个 Warp 内的数据，通过使用 Shuffle 获得 6％的性能增益。

![](https://pica.zhimg.com/v2-6ea1d682793de7f159f0e6566721ea90_1440w.jpg)

变量可以在 Kepler 中使用 Shuffle 指令

#### 3.2.5. 原子运算

原子内存运算对并行编程十分重要，允许并发线程对共享数据结构执行正确的读‐修改‐写运算。原子运算如 add、min、max 和 compare，swap 在某种意义上也是原子运算（如果在没有其他线程干扰的情况下执行读、修改和写）。原子内存运算被广泛用于并行排序、归约运算、建制数据结构而同时不需要锁定线程顺序执行。

Kepler GK110/210 全局内存原子运算的吞吐量较 Fermi 时代有大幅的提高。普通全局内存地址的原子运算吞吐量相对于每频率一个运算来说提高了 **9 倍**。独立的全局地址的原子运算的吞吐量也明显加快，而且处理地址冲突的逻辑已经变得更有效。原子运算通常可以按照类似全局负载运算的速度进行处理。此速度的提高使得原子运算足够快得在内核内部循环中使用，消除之前一些算法整合结果所需要的单独的归约传递。Kepler GK110 还扩展了对全局内存中 64‐位原子运算的本机支持。除了 atomicAdd、atomicCAS 和 atomicExch（也受 Fermi 和 Kepler GK104 支持）之外，GK110 还支持以下功能：

- atomicMin
- atomicMax
- atomicAnd
- atomicOr
- atomicXor

其他不受本机支持的原子运算（例如 64 位浮点原子运算）可以使用 compare‐and‐swap (CAS) 指令模拟。

#### 3.2.6. 纹理改进

GPU 的专用硬件纹理单元对于需要取样或过滤图像数据的计算机程序来说是宝贵的资源。Kepler 中的纹理吞吐量与 Fermi 相比有明显提高，**每个 SMX 单元包含 16 纹理过滤单元**，对比 Fermi GF110 SM 提高了 **4 倍**。

此外，Kepler 改变了管理纹理状态的方法。在 Fermi 时代，为让 GPU 引用纹理，必须在固定大小绑定表中分配“槽”才能启动 Grid。表中槽数量最终限制程序一次可以读取多少个独特的纹理。最终，在 Fermi 中限制程序仅可以同时访问 128 个纹理。

Kepler 中有无绑定纹理，不需要额外步骤：纹理状态已保存为内存中的对象，硬件按需获取这些状态对象，这使得绑定表显得过时了。这有效地消除了计算程序引用独特纹理数量的任何限制。相反，程序可以随时映射纹理，并像使用其他指针一样传递纹理句柄。

### 3.3. **Kepler** 内存子系统 **– L1**、**L2**、**ECC**

Kepler架构的内存层次结构与Fermi类似。Kepler架构支持用于加载和存储的统一内存请求路径，每个SMX多处理器带有一个L1缓存。Kepler GK110还可以编译器指导使用额外的新缓存来读取**只读数据**，如下所述。

![](https://pic3.zhimg.com/v2-d3ea14d3c7bdc768ce327e4707310604_1440w.jpg)

#### 3.3.1. 可配置的共享内存和L1缓存

在 Kepler GK110 架构（如在上一代 Fermi 架构）中，每个 SMX 有 64 KB 的片上存储器，可配置为 48 KB 的 共享存储器和 16 KB 的 L1 缓存，或配置为 16 KB 的共享存储器和 48 KB 的 L1 缓存。Kepler 目前在配置共享存储器的分配和 L1 缓存方面的灵活性更大，允许共享存储器和 L1 缓存之间以 **32KB/32KB** 划分。为了支持 SMX 单元增加的吞吐量，用于 64 位或更大负载运算的共享存储器带宽相对 Fermi SM 也增加一倍，到每主频 **256B**。

对于GK210架构，可配置内存的总量增加到**128 KB**，其中最大的共享内存为112 KB，L1缓存为16 KB。其他可能的内存配置是32 KB L1缓存和96 KB共享内存，或48 KB L1缓存和80 KB共享内存。这种增加可以使线程并发性得到类似于上述寄存器文件容量改进的改善。

#### 3.3.2. 48KB只读‐数据缓存

除 L1 缓存之外，Kepler 为只读数据引入 48 KB 缓存，便于在函数执行期间使用。在 Fermi 时代，该缓存只能由纹理单元访问。为了提升数据加载的性能，专家程序员经常将数据映射为纹理来加载，但这种方法有很多局限性。

在Kepler架构中，除了显著增加纹理缓存容量和性能之外，我们决定将缓存直接暴露给SM，以进行一般的加载操作。使用只读路径是有益的，因为它使加载和工作集足迹都从共享/L1缓存路径上移除。此外，只读数据缓存的更高标记带宽支持全速非对齐内存访问模式，以及其他情况。

使用只读路径可以由编译器自动管理，也可以由程序员显式管理。通过程序员使用C99标准的“**const __restrict**”关键字，可以将任何已知为常量的变量或数据结构的访问标记为通过只读数据缓存进行加载。程序员还可以使用**__ldg()**内置函数来显式使用此路径。

#### 3.3.3. 改进的L2缓存

Kepler GK110/210 GPU具有1536KB专用的L2缓存内存，是Fermi架构中可用L2缓存数量的两倍。L2缓存是SMX单元之间数据统一的主要点，服务于所有加载、存储和纹理请求，并在GPU上提供高效、高速的数据共享。Kepler上的L2缓存每个时钟周期提供的带宽可达到Fermi的**2倍**。对于数据地址事先不知道的算法，例如物理求解器、光线追踪和稀疏矩阵乘法等，尤其从缓存层次结构中受益。需要多个SM读取相同数据的滤波器和卷积核也会受益。

#### 3.3.4. 内存保护支持

与Fermi一样，Kepler的寄存器文件、共享内存、L1缓存、L2缓存和DRAM内存都使用**单错误校正双错误检测**（Single-Error Correct Double-Error Detect SECDED）[ECC代码](https://zhida.zhihu.com/search?content_id=228979375&content_type=Article&match_order=1&q=ECC%E4%BB%A3%E7%A0%81&zhida_source=entity)进行保护。此外，只读数据缓存通过奇偶校验支持单错误纠正；如果发生奇偶校验错误，缓存单元会自动使失败的行失效，从L2中读取正确的数据。

从DRAM**获取ECC校验位**必然会**消耗一定量的DRAM带宽**，在特别注重内存带宽的应用程序中，这会导致启用ECC和禁用ECC的操作之间存在性能差异。Kepler GK110根据Fermi的经验实现了几个优化的ECC校验位获取处理，结果，在我们的内部计算应用程序测试套件中，ECC开启与关闭的性能差异平均**降低了66%**。

### 3.4. 动态并行

在CPU-GPU异构系统中，使应用程序中更多的并行代码能够在GPU内部高效地运行，可以提高可扩展性和性能，尤其是随着GPU在功耗性能方面不断提高。为了加速应用程序的这些额外并行部分，GPU必须支持更多种类的并行工作负载。

动态并行技术是在Kepler GK110中引入的，并且也包括在GK210中。它允许GPU为自己生成新的工作，同步结果，并通过专用的加速硬件路径控制工作的调度，所有这些都不需要涉及CPU。

费米处理器非常擅长处理大规模并行数据结构，当问题的规模和参数在核心启动时已知时。所有工作都是从主机CPU 启动（launch），GPU运行到完成，并返回结果到CPU。然后，该结果将被用作最终解决方案的一部分，或将由CPU进行分析，然后将额外的请求发送回GPU进行额外的处理。

在Kepler GK110/210中，任何**核心都可以启动另一个核心**，并可以创建必要的流、事件和管理所需的依赖关系，以便在不需要主机CPU交互的情况下处理额外的工作。这种架构创新使开发人员更容易创建和优化递归和数据相关的执行模式，并允许更多的程序直接在GPU上运行。系统CPU可以被释放出来用于其他任务，或者可以配置具有较低功率的CPU以执行相同的工作负载。

![](https://pic2.zhimg.com/v2-aa3b2ac892750a77b8c44000d0b5e735_1440w.jpg)

动态并行性允许应用程序中更多的并行代码直接由GPU启动到GPU本身（图右），而无需CPU干预（图左）

动态并行性使更多种类的并行算法能够在GPU上实现，包括具有不同并行度的嵌套循环、由串行控制任务线程组成的并行分组，或简单的串行控制代码被转移到GPU上，以促进应用程序的并行部分的数据局部性。

由于核心具有根据中间的GPU结果启动其他工作负载的能力，程序员现在可以智能地负载平衡工作，将大部分资源集中在需要最大处理能力或与解决方案最相关的问题区域上。

一个例子是动态设置数值模拟的网格 - 通常网格单元格集中在变化最大的区域，需要对数据进行昂贵的预处理。或者，可以使用均匀粗糙的网格来防止GPU资源浪费，或者可以使用均匀细网格来确保捕捉所有特征，但这些选项可能会错过模拟特征或在不太感兴趣的区域“过度消耗”计算资源。

使用动态并行性，可以根据数据动态地在运行时确定网格分辨率。从一个粗略的网格开始，模拟可以“放大”感兴趣的区域，同时避免在变化很小的区域进行不必要的计算。虽然这可以使用一系列由CPU启动的核心实现，但允许GPU自行分析数据并启动其他工作作为单个模拟核心的一部分会更简单，从而消除了CPU的中断和CPU与GPU之间的数据传输。

![](https://picx.zhimg.com/v2-1bdf057d1f75cc2fad701858432ac307_1440w.jpg)

上面的例子说明了在数值模拟中使用动态尺寸网格的好处。为了满足峰值精度要求，固定分辨率的模拟必须在整个模拟域内以过度细的分辨率运行，而多分辨率网格根据局部变化将正确的模拟分辨率应用于每个区域。

### 3.5. Hyper-Q

过去的一个挑战是如何让GPU从多个流中获得一个优化的工作负载。Fermi架构支持16路并发的核心启动，但最终这些流都会被复用到相同的硬件工作队列中。这会导致虚假的流内依赖性，也就是一个流中的核心依赖另一个流中的核心先执行。虽然通过使用广度优先启动可以在一定程度上缓解这个问题，但随着程序复杂性的增加，这将变得越来越难以有效地管理。

Kepler GK110/210通过其Hyper-Q特性改进了这种功能。Hyper-Q通过允许32个同时、硬件管理的连接（与Fermi可用的单个连接相比）增加了主机和GPU中CUDA工作分配器（CWD）逻辑之间的总连接数（工作队列）。Hyper-Q是一个灵活的解决方案，允许来自多个CUDA流、多个消息传递接口(MPI)进程或甚至是同一进程中的多个线程的连接。以前遇到过任务间虚假序列化的应用程序，限制了GPU利用率，现在可以在不更改任何现有代码的情况下看到高达32倍的性能提升。

![](https://picx.zhimg.com/v2-8c068b6f6ce5ba2b729b42eeb52be60d_1440w.jpg)

Hyper‐Q 允许CPU和GPU之间更多的并发连接

每个 CUDA 流在其自己硬件工作队列管理，优化流间的依赖关系，一个流中的运算将不再阻止其他流，使得流能够同时执行，无需特别定制的启动顺序，消除了可能的虚假依赖

Hyper-Q在基于 MPI 的并行计算机系统中使用会有明显的优。传统的基于MPI的算法通常是为多核CPU系统设计的，根据需要将工作分配给每个MPI进程。这可能会导致单个MPI进程的工作不足以充分占用GPU。虽然一直以来多个 MPI 进程都可以共享 GPU，但这些进程可能会受到虚假依赖的限制。Hyper-Q消除了这些虚假依赖，显着提高了MPI进程之间在GPU共享方面的效率。

![](https://pic4.zhimg.com/v2-0ba4214e5b4cb8add5ba23ad7c9a56c3_1440w.jpg)

Hyper‐Q 与 CUDA 流一起工作：左侧显示 Fermi 模式，仅 (C,P) 和 (R,X) 可以同时运行，因为单个硬件工作队列导致的流内依赖。Kepler Hyper‐Q 模式允许所有流使用单独的工作队列同时运

### 3.6. Grid管理单元—高效保持GPU利用率

Kepler GK110引入了一些新功能，例如通过动态并行使CUDA内核能够直接在GPU上启动工作，这要求Kepler中的CPU到GPU工作流程比Fermi设计提供更多的功能。在Fermi上，CPU会启动一个线程块网格，并始终运行到完成，在主机到SMs之间通过CUDA工作分配器（CUDA Work Distributor CWD）单元创建了简单的单向工作流。Kepler GK110/210通过允许GPU有效地管理CPU和CUDA创建的工作负载来改善CPU到GPU的工作流。

我们已经讨论了Kepler GK110 GPU允许内核直接在GPU上启动工作的能力，重要的是要理解Kepler GK110架构中为促进这些新功能所做的更改。在Kepler GK110/210中，网格可以像Fermi一样从CPU启动，但是新网格也可以在Kepler SMX单元内由CUDA编程创建。为了管理CUDA创建的和主机发起的网格，Kepler GK110引入了一个新的网格管理单元（Grid Management Unit GMU）。该控制单元管理和优先处理传递到CWD以发送到SMX单元执行的网格。

Kepler中的CWD保存准备好调度的网格，并且能够调度32个活动网格，这是Fermi CWD容量的两倍。Kepler CWD通过双向链接与GMU通信，允许GMU暂停新网格的调度，并保持待处理和挂起的网格，直到需要。GMU还直接连接到Kepler SMX单元，使通过动态并行启动附加工作负载的网格能够将新工作发送回GMU以进行优先处理和调度。如果调度附加工作负载的内核暂停，GMU将保持其处于非活动状态，直到依赖工作完成

![](https://pic4.zhimg.com/v2-cfe85896dbb4632f91f4151cf6b25a5b_1440w.jpg)

重新设计后的Kepler HOST到GPU的工作流程展示了新的网格管理单元，它允许管理正在调度的网格、暂停调度以及保持待处理和挂起的网格

  

### 3.7. Nvidia GPUDirect

在处理大量数据时，增加数据吞吐量并降低延迟对于提高计算性能至关重要。Kepler GK110/210支持NVIDIA GPUDirect中的RDMA功能，旨在通过允许第三方设备（如IB适配器、NIC和SSD）直接访问GPU内存来提高性能。当使用CUDA 5.0或更高版本时，GPUDirect提供以下重要功能：

- 在无需CPU端数据缓冲的情况下，实现NIC和GPU之间的直接内存访问（DMA）。
- 大大提高了GPU与网络中其他节点之间的MPISend/MPIRecv效率。
- 消除了CPU带宽和延迟瓶颈。
- 与各种第三方网络、捕获和存储设备一起工作

如逆时偏移（用于石油和天然气勘探地震成像）这样的应用程序，将大量影像数据分布在多个GPU上。数百个GPU必须协作处理数据，通常需要通信中间结果。GPUDirect通过P2P和RDMA功能，在服务器内和服务器之间，为GPU到GPU的通信场景提供了更高的聚合带宽。

Kepler GK110还支持其他GPUDirect功能，如点对点和视频GPUDirect。

![](https://pica.zhimg.com/v2-93e9455f946346b9ae5a64786c0395e0_1440w.jpg)

GPUDirect RDMA允许第三方设备（如网络适配器）直接访问GPU内存，从而实现跨节点之间GPU之间的直接数据传输

### 3.8. 总结

随着Fermi于2010年的推出，NVIDIA引领了一个新时代的高性能计算(HPC)行业，基于混合计算模型，其中CPU和GPU共同解决计算密集型工作负载。NVIDIA Kepler GK110/210 GPU再次提高了HPC行业的标准。

Kepler GK110和GK210旨在通过快速双精度计算来最大化计算性能和吞吐量计算。该架构有许多新的创新，如SMX，动态并行和Hyper-Q，使混合计算大大加快、更易编程，并适用于更广泛的应用程序。 Kepler GK110/210 GPU将被用于许多系统，从工作站到超级计算机，以应对HPC中最困难的挑战。

### 3.9.参考文档

[Nvidia Kepler架构白皮书](https://link.zhihu.com/?target=https%3A//www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/tesla-product-literature/NVIDIA-Kepler-GK110-GK210-Architecture-Whitepaper.pdf)

# Maxwell 
[Maxwell架构](https://zhida.zhihu.com/search?content_id=229167383&content_type=Article&match_order=1&q=Maxwell%E6%9E%B6%E6%9E%84&zhida_source=entity)是NVIDIA推出的一种GPU架构，是其先前[Kepler架构](https://zhida.zhihu.com/search?content_id=229167383&content_type=Article&match_order=1&q=Kepler%E6%9E%B6%E6%9E%84&zhida_source=entity)的升级版。它首次于2014年推出，并被应用于NVIDIA的[GTX 750 Ti](https://zhida.zhihu.com/search?content_id=229167383&content_type=Article&match_order=1&q=GTX+750+Ti&zhida_source=entity)显卡中。下面是Maxwell架构的主要特点：

## 1. 更高效的多处理器

Maxwell架构引入了全新的[流处理器](https://zhida.zhihu.com/search?content_id=229167383&content_type=Article&match_order=1&q=%E6%B5%81%E5%A4%84%E7%90%86%E5%99%A8&zhida_source=entity)（SM）设计，能够显著提高功率效率。尽管Kepler SMX设计在其时代非常高效，但在其开发过程中，NVIDIA的GPU架构师看到了另一个大的架构效率跃进的机会；Maxwell SM是这一愿景的实现。控制逻辑分区、工作负载平衡、时钟门控粒度、指令调度、每个时钟周期发出的指令数量等方面的改进，使得Maxwell SM（也称为“SMM”）的效率远远超过Kepler SMX。新的Maxwell SM架构使我们能够将SM的数量增加到GM107中的五个，而GK107中仅有两个，只增加了25%的芯片面积（每个多处理器拥有128个CUDA核心，相比Kepler架构的192个CUDA核心，数量有所减少，但是NVIDIA通过提高每个CUDA核心的效率来弥补这一不足）。

### 1.1. 指令调度提升

每个SM中的CUDA Core数量已降至2的幂次方，但是由于Maxwell的改进执行效率，每个SM的性能通常在Kepler性能范围内（相差不超过10%），且SM的面积效率提高意味着每个GPU中的CUDA Core数量将比相似的Fermi或Kepler芯片大得多。Maxwell SM保留了与Kepler设计相同的每个时钟周期的指令发出槽位数量，并降低了算术延迟。

与SMX一样，每个SMM具有四个warp调度器，但与SMX不同的是，所有核心的SMM功能单元都分配给特定的调度器，没有共享单元。每个分区中的CUDA Core数量是2的幂次方，简化了调度，因为SMM的每个warp调度器发出到一个相等于warp宽度的CUDA Core集合。每个warp调度器仍然具有双发射的灵活性（例如在同一个周期中向CUDA Core发出一个数学操作和向load/store单元发出一个内存操作），但单发射已足以充分利用所有CUDA Core。

![](https://pic3.zhimg.com/v2-96b107722d3bbe1079d4148cd3113904_1440w.jpg)

SMM

### 1.2. 现有代码的占用率增加

就CUDA计算能力而言，Maxwell的SM是CC 5.0。SMM在许多方面类似于Kepler架构的SMX，重点增强了效率，而无需从应用程序中要求每个SM的可用并行性显著增加。SMM的寄存器文件大小和并发warp的最大数量与SMX相同（分别为64k 32位寄存器和64个warp），每个线程的寄存器最大数量也相同（255）。但是，每个多处理器的活动线程块的最大数量已经是SMX的两倍，达到了32个，这应该会自动提高使用64个或更少线程的小线程块的内核的占用率（假设可用的寄存器和共享内存不是占用率限制器）。表1提供了Maxwell GM107和其前身Kepler GK107的关键特征比较。

|   |   |   |
|---|---|---|
|GPU|GK107 (Kepler)|GM107 (Maxwell)|
|CUDA Cores|384|640|
|Base Clock|1058 MHz|1020 MHz|
|GPU Boost Clock|N/A|1085 MHz|
|GFLOP/s|812.5|1305.6|
|Compute Capability|3.0|5.0|
|Shared Memory / SM|16KB / 48 KB|64 KB|
|Register File Size / SM|256 KB|256 KB|
|Active Blocks / SM|16|32|
|Memory Clock|5000 MHz|5400 MHz|
|Memory Bandwidth|80 GB/s|86.4 GB/s|
|L2 Cache Size|256 KB|2048 KB|
|TDP|64W|60W|
|Transistors|1.3 Billion|1.87 Billion|
|Die Size|118 mm2|148 mm2|
|Manufactoring Process|28 nm|28 nm|

### 1.3. 减少算数指令延迟

SMM的另一个重大改进是明显降低了相关算术指令的延迟。由于SMM上的占用率（转化为可用的warp级并行度）与SMX相同或更好，这些减少的延迟可以提高利用率和吞吐量。

## 2. 更大的专用共享内存

SMM的一个重大改进是它为**每个SM提供了64KB**的专用共享内存，而Fermi和Kepler将64KB内存分成L1缓存和共享内存两部分。在Maxwell上，**每个线程块**的共享内存限制仍然是**48KB**，但总可用的共享内存增加会使得占用率提高。在Maxwell中，专用的共享内存是通过将L1缓存和纹理缓存的功能合并为单个单元来实现的。

## 3. 快速的共享内存原子操作

Maxwell提供了针对共享内存原生的32位整数原子操作，以及32位和64位比较交换（CAS），这可以用于实现其他原子函数。相比之下，Fermi和Kepler架构使用锁定/更新/解锁模式实现了共享内存原子操作，在共享内存中特定位置的更新存在高竞争时可能会非常昂贵。

## 4. 支持动态并行

Kepler GK110引入了一项名为“动态并行”的新架构特性，它允许GPU为自身创建额外的工作。CUDA 5.0中引入了一种利用该特性的编程模型增强，使运行在GK110上的线程能够在同一GPU上启动其他内核。

SMM通过在整个产品线上支持Dynamic Parallelism将其引入主流，甚至在低功率芯片（如GM107）中也支持。这将使开发人员受益，因为这意味着应用程序将不再需要针对高端GPU的特殊算法实现，这些算法与在更受功率限制的环境中可用的算法不同。

## 5. 参考链接

[https://developer.nvidia.com/bl](https://link.zhihu.com/?target=https%3A//developer.nvidia.com/blog/5-things-you-should-know-about-new-maxwell-gpu-architecture/)


# Pascal

**有史以来最先进的数据中心加速器 搭载Pascal GP100，世界上最快的GPU（2016）**

将近十年前，NVIDIA®推出了G80 GPU和NVIDIA® CUDA®并行计算平台，率先采用GPU加速计算密集型工作负载。如今，NVIDIA® Tesla® GPU加速了许多领域的数千个高性能计算（HPC）应用程序，包括计算流体力学、医学研究、机器视觉、金融建模、量子化学、能源发现等。NVIDIA Tesla GPU安装在许多全球顶级超级计算机中，加速发现并在多个领域中实现越来越复杂的模拟。数据中心正在使用NVIDIA Tesla GPU加速许多HPC和大数据应用程序，同时也实现了领先的人工智能（AI）和深度学习系统。

NVIDIA的新NVIDIA Tesla P100加速器采用了突破性的新NVIDIA® Pascal™ GP100 GPU，将GPU计算推向了一个新的高度。本文详细介绍了Tesla P100加速器和Pascal GP100 GPU的架构。

![](https://pic3.zhimg.com/v2-025988e093f0f066b5e6fe784210e1f2_1440w.jpg)

NVIDIA Tesla P100 with Pascal GP100 GPU

## 1. Tesla P100: GPU计算的革命性性能和特性

搭载153亿个晶体管的GPU，新的高性能互连技术极大地加速了GPU之间和GPU与CPU之间的通信，新技术简化了GPU编程，功率效率也非常出色。Tesla P100不仅是最强大的GPU加速器架构，也是最复杂的GPU加速器架构。关键特性包括：

- 极致性能，适用于高性能计算、深度学习等GPU计算领域
- **NVLink**，NVIDIA的新一代高速、高带宽互连技术，实现最大的应用可扩展性
- **HBM2**，采用CoWoS（芯片垒叠）技术的快速、高容量、极高效的内存体系结构
- **统一内存、计算抢占和新的AI算法**，显著改进了编程模型和针对Pascal架构优化的先进AI软件
- **16nm** FinFET工艺，实现了更多的功能、更高的性能和更好的功率效率

![](https://pic4.zhimg.com/v2-f04b11b532d0ba9d6371a7f0ab9135b1_1440w.jpg)

Tesla P100新技术

### 1.1. 高性能计算和深度学习的极致性能

Tesla P100旨在为计算需求较大的应用提供卓越的性能，其中包括：

- 5.3 TFLOPS的双精度浮点（FP64）性能
- 10.6 TFLOPS的单精度（FP32）性能
- 21.2 TFLOPS的半精度（FP16）性能。

![](https://pic2.zhimg.com/v2-168ef38dcc8b409a06b63d07fbb78193_1440w.jpg)

Tesla P100显著超过了过去GPU的计算性能

除了多年来NVIDIA GPU加速的大量高性能计算领域，最近深度学习已成为GPU加速的一个非常重要的领域。NVIDIA GPU现在处于深度神经网络（DNN）和人工智能（AI）的前沿。与CPU相比，它们可以将各种应用程序中的DNN加速10倍至20倍，并将培训时间从几周缩短至几天。过去三年中，基于NVIDIA GPU的计算平台已将深度学习网络训练时间提速50倍。在过去的两年中，与NVIDIA在深度学习方面合作的公司数量增加了近35倍，达到了超过**3400家**。

我们Pascal架构中的新创新，包括本机16位浮点（FP）精度，使GP100可以为许多深度学习算法提供大幅加速。这些算法不需要高级浮点精度，但从FP16的计算能力和存储减少中获得了巨大的收益。

### 1.2. NVLink: 多GPU和GPU到CPU连接的非凡带宽

随着GPU加速计算的流行，越来越多的多GPU系统被部署在各个级别，从工作站到服务器，再到超级计算机。许多4-GPU和8-GPU系统配置现在用于解决更大、更复杂的问题。多组多GPU系统正在使用InfiniBand®和100 Gb以太网相互连接，形成更大、更强大的系统。GPU与CPU的比例也增加了。2012年最快的超级计算机Titan位于奥克岭国家实验室，每个CPU部署了一个GK110 GPU。如今，开发人员越来越多地利用并发性，使每个CPU与两个或更多的GPU配对。随着这一趋势的继续，多GPU系统级别的PCIe带宽成为一个更大的瓶颈。

为了解决这个问题，Tesla P100采用了NVIDIA的新高速接口NVLink，可提供高达**160GB/秒**的双向带宽的GPU到GPU数据传输，比PCIe Gen 3 x16高5倍的带宽。下图展示了NVLink连接八个Tesla P100加速器在混合立方体网格拓扑结构中的情况。

![](https://pic2.zhimg.com/v2-73e79c6ede77237709c10f6f3f4bd211_1440w.jpg)

NVLink可以将八个Tesla P100加速器连接在混合立方体网格拓扑结构中

下图展示了不同工作负载下的性能，使用NVLink连接高达八个GP100 GPU时服务器可以实现的性能可扩展性。（注意：这些数字是在预生产P100 GPU上测量的。）

  

![](https://pic3.zhimg.com/v2-17a03507e46947b738f61affcd4bb6f4_1440w.jpg)

八个通过NVLink连接的P100 GPU可以实现最大的性能提升

### 1.3. HBM2高速GPU内存架构

Tesla P100是世界上第一个支持HBM2内存的GPU架构。HBM2提供的内存带宽是Maxwell GM200 GPU的**三倍**。这使得P100可以在更高的带宽下处理更大的工作数据集，提高效率和计算吞吐量，并减少系统内存传输的频率。

由于HBM2内存是堆叠内存，位于与GPU相同的物理封装上，与传统的GDDR5相比可以节省大量空间，使我们可以更轻松地构建更密集的GPU服务器。

![](https://pic4.zhimg.com/v2-a466dcf488c8b01aa7028d4810e54f7b_1440w.jpg)

搭载HBM2的Tesla P100显著超过过去GPU代的内存带宽

### 1.4. 统一内存和计算抢占简化编程

**统一内存**是NVIDIA GPU计算的一个重大进展和Pascal GP100 GPU架构的一个重要的新硬件和软件特性。它提供了一个单一、无缝的统一虚拟地址空间，用于CPU和GPU内存。统一内存极大地简化了GPU编程和移植应用程序到GPU的过程，也减少了GPU计算学习曲线。程序员不再需要担心管理两个不同的虚拟内存系统之间的数据共享。GP100是第一个支持硬件页错误的NVIDIA GPU，当与新的49位（512 TB）虚拟寻址结合使用时，可以在GPU和CPU的完整虚拟地址空间之间实现数据的透明迁移。

**计算抢占**是另一个重要的新硬件和软件特性，被添加到GP100中，它允许计算任务在指令级别粒度上被抢占，而不像之前的Maxwell和Kepler GPU架构那样在线程块粒度上。计算抢占可以防止长时间运行的应用程序垄断系统（防止其他应用程序运行）或超时。程序员不再需要修改他们的长时间运行的应用程序以使其与其他GPU应用程序协作。使用GP100的计算抢占，应用程序可以运行所需的时间来处理大型数据集或等待各种条件发生，同时与其他任务一起安排。例如，交互式图形任务和交互式调试器可以与长时间运行的计算任务协同运行

## 2. GP100 GPU 硬件架构深入剖析

GP100是世界上最高性能的并行计算处理器，旨在满足由我们的Tesla P100加速器平台服务的GPU加速计算市场的需求。与之前的Tesla级别的GPU类似，GP100由一组图形处理器集群（GPC）、纹理处理器集群（TPC）、流多处理器（SM）和内存控制器组成。完整的GP100包括六个GPC，60个Pascal SM，30个TPC（每个包括两个SM）和八个512位的内存控制器（总共为4096位）。

GP100中的每个GPC都有10个SM。**每个SM有64个CUDA核心和4个纹理单元**。通过60个SM，GP100共有3840个单精度CUDA核心和240个纹理单元。每个内存控制器连接512 KB的L2缓存，每个HBM2 DRAM堆栈由一对内存控制器控制。完整的GPU包括总共4096 KB的L2缓存。

如图显示了具有60个SM单元的完整GP100 GPU（不同的产品可以使用不同的GP100配置）。Tesla P100加速器使用56个SM单元。

![](https://pic3.zhimg.com/v2-847d92db0a0bf42b37c1fd7bf109f5c6_1440w.jpg)

Pascal GP100拥有60个SM单元

### 2.1. 卓越的性能和能耗效率

提供更高的性能和改善能源效率是新GPU架构的两个关键目标。相比Kepler，Maxwell架构对SM进行了一些更改，提高了其效率。Pascal在此基础上进行了改进，增加了额外的改进，使我们能够进一步提高每瓦性能，超过Maxwell。尽管TSMC的16纳米FinFET制造工艺发挥了重要作用，但还实施了许多GPU架构修改，以进一步降低功耗，同时保持高性能。

![](https://pic2.zhimg.com/v2-c672b31b6d86eadde3a598ba1db41047_1440w.jpg)

与前代产品对比

### 2.2. Pascal SM

GP100的第六代SM架构提高了CUDA核心的利用率和功率效率，从而显著提高了GPU整体性能，与之前的GPU相比具有更高的核心时钟速度。

GP100的SM包含64个单精度（FP32）CUDA核心。相比之下，Maxwell和Kepler的SM分别有128和192个FP32 CUDA核心。GP100的SM被分成两个处理块，每个块都有32个单精度CUDA核心、一个指令缓冲区、一个warp调度器和两个分派单元。尽管一个GP100的SM拥有一半的CUDA核心数量，但它仍保持了相同的寄存器文件大小，并支持类似于warp和线程块的占用。GP100的SM具有与Maxwell GM200和Kepler GK110 SM相同数量的寄存器，但**整个GP100 GPU拥有更多的SM，因此总的寄存器数量更多**。这意味着GPU上的线程可以访问更多的寄存器，而GP100相比以前的GPU代数支持更多的线程、warp和线程块。

由于**SM数量增加**，GP100 GPU上的**共享内存总量也增加**，聚合共享内存带宽有效翻了一倍以上。在GP100中，每个SM的共享内存、寄存器和warp的比例更高，这使得SM更有效地执行代码。调度程序可以选择更多的warp指令，发起更多的加载操作，并为共享内存指定更多的单个线程带宽。

![](https://pic4.zhimg.com/v2-ef0c719d48878b1a2c17d13566750289_1440w.jpg)

SM

与Kepler相比，Pascal的SM具有更简单的数据通路组织，需要更少的芯片面积和更少的功率来管理SM内部的数据传输。Pascal还提供了优秀的调度和重叠的加载/存储指令，以提高浮点利用率。GP100中的新SM调度器架构改进了Maxwell调度器的先进性，更加智能，提供了更高的性能和更低的功耗。**每个warp调度器（每个处理块一个）能够每个时钟分派两个warp指令**。

GP100的FP32 CUDA核心新增了能够处理16位和32位精度指令和数据的能力，后面的论文会详细描述。FP16操作的吞吐量是FP32操作的两倍。

### 2.3. 为高性能双精度而设计

双精度算术是许多高性能计算应用程序的核心，例如线性代数、数值模拟和量子化学。因此，GP100的一个关键设计目标是显著提高这些用例的性能。

GP100中的每个SM都具有32个双精度（FP64）CUDA核心，这是FP32单精度CUDA核心数量的一半。完整的GP100 GPU具有1920个FP64 CUDA核心。这种单精度（SP）单元与双精度（DP）单元的**2:1比例更好地匹配GP100的新数据通路配置**，使GPU能够更有效地处理DP工作负载。与以前的GPU架构一样，GP100支持完整的IEEE 754-2008兼容单精度和双精度算术，包括对融合乘加（FMA）运算的支持以及对非规格化值的全速支持。

### 2.4. 支持深度学习FP16加速

深度学习是计算机领域发展最快的领域之一。它是许多重要应用程序的关键组成部分，包括实时语言翻译、高精度图像识别、自动图像字幕、自动驾驶物体识别、最优路径计算、避免碰撞等。深度学习是一个两步过程。

- 第一步，必须训练神经网络。
- 第二步，将网络部署在现场以运行推断计算，其中它利用先前训练的结果来分类、识别和处理未知的输入。

与CPU相比，GPU可以为深度学习训练和推断提供巨大的性能加速。

与其他需要高精度浮点计算的技术计算应用程序不同，深度神经网络架构由于其训练所使用的反向传播算法而对误差具有天然的韧性。事实上，为了避免网络过度拟合训练数据集，一些方法（如dropout）旨在确保训练好的网络具有很好的泛化能力，并不过于依赖任何给定单元的计算精度（或误差）。

与更高精度的FP32或FP64相比，存储FP16数据可以减少神经网络的内存使用，从而允许训练和部署更大的网络。使用FP16计算相对于FP32算术可以提高性能高达2倍，同样，FP16数据传输所需的时间也比FP32或FP64传输要少。

> 注意：在GP100中，可以使用单个配对操作指令执行两个FP16操作。

GP100中的架构改进，以及对FP16数据类型的支持，使得与去年可实现的相比，深度学习处理时间大大缩短。

### 2.5. 更好的原子操作

原子内存操作在并行编程中非常重要，允许并发线程对共享数据结构执行正确的读取-修改-写入操作。

Kepler具有与Fermi相同形式的共享内存原子操作。这两种架构使用锁定/更新/解锁模式实现共享内存原子操作，这在共享内存中特定位置的更新有高争用的情况下可能是昂贵的。

Maxwell通过为32位整数实现本机硬件支持的共享内存原子操作和本机共享内存32位和64位比较和交换（CAS）来改进原子操作，可以用于实现其他具有减少开销的原子函数（与使用软件实现的Fermi和Kepler方法相比）。

GP100在Maxwell的基础上，通过使用新的统一内存和NVLink功能（在下面的段落中描述）也改进了原子操作。全局内存中的**原子加法操作已扩展到包括FP64**数据。 CUDA中的atomicAdd（）函数现在适用于32位和64位整数和浮点数据。对于所有浮点原子加法操作，**浮点舍入模式为“就近偶数舍入”**（以前，FP32原子加法使用了“向零舍入”）。

### 2.6. L1/L2 Cache 变化

Fermi和Kepler GPU具有64 KB可配置的共享内存和L1缓存，可以根据工作负载将内存分配在L1和共享内存功能之间。但从Maxwell开始，缓存层次结构发生了改变。GP100 **SM具有其自己的专用共享内存池（64 KB / SM）**和**L1缓存**，可以根据工作负载也用作纹理缓存。统一的L1 /纹理缓存作为内存访问的汇聚缓冲区，将Warp线程请求的数据收集起来，然后交付给Warp。

> 注意：一个CUDA线程块不能单独分配64 KB的共享内存，但两个线程块可以使用32 KB，以此类推。

每个SM都有一个专用的共享内存，这意味着应用程序不再需要选择L1 /共享拆分的首选项以获得最佳性能-每个SM的完整64 KB始终可用于共享内存。

GP100具有**统一的4096 KB L2缓存**，可在GPU上提供高效的高速数据共享。相比之下，GK110的L2缓存为1536 KB，而GM200的L2缓存为3072 KB。由于芯片上的缓存更多，因此需要更少的对GPU的DRAM的请求，这降低了整个板的功率，降低了内存带宽需求，并提高了性能。

### 2.7. GPUDirect 加强

无论您正在处理大量地质数据，还是研究复杂科学问题的解决方案，您都需要一种提供最高数据吞吐量和最低延迟的计算平台。GPUDirect是一种功能，它使单个计算机内的GPU或位于网络中的不同服务器上的GPU能够直接交换数据，而无需经过CPU /系统内存。

GPUDirect中的RDMA功能在Kepler GK110中引入，**允许第三方设备（如InfiniBand（IB）适配器，网络接口卡（NIC）和SSD）在同一系统中直接访问多个GPU上的内存**，消除不必要的内存复制，大大降低CPU开销，并显着减少MPI发送和接收消息到/来自GPU内存的延迟。它还减少了系统内存带宽的需求，并释放GPU DMA引擎供其他CUDA任务使用。

GP100将RDMA**带宽翻倍**，读取源GPU内存中的数据并将其写入目标NIC内存的PCIe。将GPUDirect的带宽翻倍对许多用例非常重要，特别是深度学习。实际上，深度学习机器的GPU与CPU的比例很高（在某些情况下每个CPU有8个GPU），因此GPU快速与IO交互而不会回退到CPU进行数据传输非常重要。

### 2.8. Compute Capability

GP100 GPU支持新的计算能力6.0。下面比较了不同NVIDIA GPU架构的计算能力参数。

![](https://pic1.zhimg.com/v2-b3088a17a4ea72ffe8775c4195ec027c_1440w.jpg)

GK110 vs GM200 vs GP100

### 2.9. Tesla P100: 世界第一款搭配HBM2的GPU

随着近年来使用GPU加速计算应用的增加，许多应用程序对数据的需求也随之增加。GPU解决的问题越来越大，需要更大的数据集和更高的DRAM带宽需求。为了满足对更高原始带宽的需求，Tesla P100是第一款使用高带宽内存2（High Bandwidth Memory 2 HBM2）的GPU加速器。

HBM2通过根本性地改变DRAM的封装方式和与GPU的连接方式，实现了DRAM带宽的显著提升。与传统的GDDR5 GPU板设计中需要许多离散的存储器芯片不同，HBM2包括一个或多个垂直堆叠的多个存储器芯片。存储器芯片使用通过硅通孔和微凸点创建的微小线路连接。一个8 Gb HBM2芯片包含超过5,000个通过硅通孔孔。然后使用无源硅中介层连接内存堆叠和GPU芯片。将HBM2堆栈，GPU芯片和硅中介层组合在一个55mm x 55mm的单一BGA封装中。

![](https://pic3.zhimg.com/v2-5c48ffec0c05db0e2f37d481009fa250_1440w.jpg)

GP100和两个HBM2堆栈的说明

![](https://pic3.zhimg.com/v2-4a425e224302f69a22f06d3fd7840f04_1440w.jpg)

GPU和内存的实际P100的照片

图中的光学显微照片显示了Tesla P100 HBM2堆栈和GP100 GPU的横截面。左上方的HBM2堆栈由五个芯片组成，包括一个基本芯片和四个内存芯片。顶部的内存芯片层非常厚。当组装时，顶部芯片和GPU被研磨到相同的高度，以呈现一个共面表面用于散热器。

与之前的HBM1一代相比，HBM2提供更高的内存容量和内存带宽。HBM2支持每个堆栈四个或八个DRAM芯片，而HBM1仅支持每个堆栈四个DRAM芯片。HBM2每个DRAM芯片支持高达8 Gb，而HBM1每个芯片仅支持2 Gb。HBM1每个堆栈的带宽限制为125 GB / sec，而P100使用HBM2每个堆栈的带宽为180GB / sec。

正如GP100全片块图所示，GP100 GPU连接到四个HBM2 DRAM堆栈。每个HBM2堆栈连接两个512位内存控制器，形成一个有效的4096位宽的HBM2内存接口。最初，Tesla P100加速器将使用四个4芯片HBM2堆栈，总共具有16 GB的HBM2内存。

### 2.10. 更优质的内存错误校验

HBM2内存的另一个好处是原生支持纠错码（ECC）功能。ECC为那些对数据损坏敏感的计算应用程序提供更高的可靠性。在大规模集群计算环境中，GPU处理非常大的数据集和/或运行应用程序的时间很长，因此ECC尤为重要。

ECC技术在影响系统之前检测和纠正单比特软错误。相比之下，GDDR5不提供内部ECC保护内存内容，仅限于对GDDR5总线的错误检测。内存控制器或DRAM本身的错误不会被检测。

GK110 Kepler GPU通过将一些可用内存分配为显式ECC存储来为GDDR5提供ECC保护。**总的GDDR5的6.25％保留用于ECC位**。例如，对于一个12 GB的Tesla K40，其总内存中的750 MB被保留用于ECC操作，导致Tesla K40启用ECC时可用内存为11.25 GB（总内存为12 GB）。同时，访问ECC位会导致内存带宽在典型工作**负载上降低**12-15％，相比于非ECC情况。由于HBM2原生支持ECC，因此**Tesla P100不会受到容量开销的影响**，并且ECC可以始终处于活动状态，而不会受到带宽惩罚。与GK110 GPU一样，GP100 GPU的寄存器文件、共享内存、L1高速缓存、L2高速缓存以及Tesla P100加速器的HBM2 DRAM均受到单错误更正双错误检测（SECDED）ECC码的保护。

### 2.11. Tesla P100 设计

Tesla P100系统架构中最令人兴奋的新特性之一是其新的板设计，该设计包含GP100 GPU和HBM2内存堆栈，并提供NVLink和PCIe连接性。一个或多个P100加速器可以用于工作站、服务器和大规模计算系统。P100加速器的尺寸为140mm x 78mm，包括高效电压调节器，用于为GPU提供所需的各种电压。P100的额定功率为300W。

下面两个图分别显示了Tesla P100加速器的正面和背面。

![](https://pic3.zhimg.com/v2-f115c38aa1907c1f40fa3f27ca5dea34_1440w.jpg)

正面

![](https://picx.zhimg.com/v2-9e02d315d2a54361c70bf84ae64c17c9_1440w.jpg)

反面

## 3. NVLink 高速互连

NVLink是NVIDIA用于GPU加速计算的新高速互连技术。NVLink目前已实现在Tesla P100加速器板和Pascal GP100 GPU中，它显著提高了GPU到GPU通信和GPU访问系统内存的性能。

多个GPU通常用于高性能计算集群节点。每个节点上最多可以使用八个GPU，而在多处理系统中，强大的互连非常有价值。我们的愿景是创建一个针对GPU的互连，它将提供比PCI Express Gen 3（PCIe）更高的带宽，并与GPU ISA兼容，以支持共享内存多处理工作负载。

![](https://pic2.zhimg.com/v2-f2b4795e96ac191f6fcb59103e5e9d91_1440w.jpg)

NVIDIA DGX-1 搭配8个 NVIDIA Tesla P100 GPUs

通过NVLink连接的GPU，程序可以直接在连接到另一个GPU的内存上执行，也可以在本地内存上执行，并且内存操作仍然正确（例如提供对Pascal原子操作的完全支持）。

NVLink使用NVIDIA的新高速信号互连（NVHS）。NVHS通过以高达**20 Gb / sec**运行的差分对传输数据。这八个差分连接形成一个子链接，向一个方向发送数据，而两个子链接（每个方向一个）形成一个链接，将两个处理器（GPU到GPU或GPU到CPU）连接在一起。**单个链接支持端点之间的双向带宽高达40 GB / sec**。多个链接可以组合成带有更高带宽连接的Gangs。Tesla P100中的**NVLink实现支持最多四个链接**，可以实现聚合最大双向带宽为160 GB / sec的组态。

### 3.1. NVLink 配置

#### 3.1.1. GPU与GPU 通过NVLink连接

下图显示了一个包含两个完全NVLink连接的GPU四联体的8-GPU混合立方体网格，四联体之间的NVLink连接以及每个四联体内的GPU直接通过PCIe连接到各自的CPU。通过使用单独的NVLink连接跨越两个四联体之间的间隙，可以减轻对每个CPU的PCIe上行链路的压力，并且避免通过系统内存和跨CPU链接路线路由传输。

![](https://pic1.zhimg.com/v2-352487d20e575deeca69f0b16b81f3e6_1440w.jpg)

八GPU混合立方体网格架构

注意，8-GPU混合立方体网格的每一半都可以作为共享内存多处理器运行，而远程节点也可以通过对等DMA共享内存。由于所有GPU到GPU的流量都通过NVLink进行，因此PCIe现在完全可用于连接到NIC（未显示）或用于访问系统内存流量。这种配置通常适用于通用的深度学习应用程序，并已实现在NVIDIA的新DGX-1服务器中。

下图展示了一个四个GPU的集群，其中每个GPU都通过单个NVLink连接到其对等方。在这种情况下，对等方可以双向通信，达到40 GB / sec的双向带宽（双重链接的双向带宽为80GB / sec），从而实现GPU之间的强大数据共享。

![](https://pic1.zhimg.com/v2-5845014448831fece159e6336ea21a98_1440w.jpg)

使用PCIe连接CPU，NVLink连接四个GPU

#### 3.1.2. CPU与GPU通过NVLink连接

虽然NVLink主要集中在将多个NVIDIA Tesla P100加速器连接在一起，但它也可以用作CPU到GPU的互连。例如，Tesla P100加速器可以通过NVIDIA NVLink技术连接到IBM的POWER8。POWER8与NVLink™支持四个NVLink。

下图显示了一个单GPU连接到启用NVLink的CPU。在这种情况下，GPU可以以高达160 GB / sec的双向带宽访问系统内存，比PCIe提供的带宽高5倍。

![](https://pic2.zhimg.com/v2-f1bba4d3f361b80e21286122004453af_1440w.jpg)

NVLink GPU与CPU互连

下图显示了一个系统，其中每个GPU与CPU之间有两个NVLink。每个GPU上剩余的两个链接用于对等方通信

![](https://picx.zhimg.com/v2-848afa55aa76ddae4758a1282e23caf1_1440w.jpg)

两个GPU和一个CPU连接，使用80 GB/sec双向带宽NVLink接口

### 3.2. NVLink在Tesla P100中的接口

如Tesla P100设计部分所述，NVLink互连在P100加速器上。P100包括两个400针高速连接器。其中一个连接器用于模块上/下的NVLink信号；另一个用于供电、控制信号和PCIe I/O。

Tesla P100加速器可以安装到更大的GPU载体或系统板中。GPU载体可以与其他P100加速器或PCIE控制器建立必要的连接。由于与传统GPU板相比，P100加速器的尺寸更小，因此客户可以轻松构建装有比以往更多GPU的服务器。通过NVLink提供的额外带宽，GPU到GPU的通信不会因PCIe带宽的限制而成为瓶颈，为GPU聚类提供以前不可用的机会。

在GPU架构接口层面上，NVLink控制器通过另一个名为High-Speed Hub（HSHUB）的新块与GPU内部通信。HSHUB直接访问GPU宽交叉开关和其他系统元素，例如高速复制引擎（HSCE），可用于以最高NVLink速率将数据移动进入和移出GPU。下图展示了NVLink与HSHUB以及GP100 GPU中的一些高级块之间的关系。

![](https://pica.zhimg.com/v2-32573f62ee291143a337a368cc243570_1440w.jpg)

NVLink与其他主要模块的关系

## 4. 统一内存

统一内存是CUDA编程模型的一个重要特性，通过为系统中的所有CPU和GPU内存提供单一的、统一的虚拟地址空间，大大简化了应用程序在GPU上的编程和移植。新的Pascal GP100特性通过**扩展统一内存的能力和提高性能**，为GPU计算提供了重大进展。

在现代处理器上实现高性能的**关键是确保硬件计算单元能够快速、直接地访问数据**。多年来，NVIDIA不断改进和简化GPU内存访问和数据共享，以便GPU程序员能够更多地专注于构建并行应用程序，而不是管理内存分配和GPU与CPU之间的数据传输。

多年来，在典型的PC或集群节点中，CPU和每个GPU的内存是物理上不同的，通过一个互连总线（通常是PCIe）进行分离。在早期的CUDA版本中，GPU程序员必须显式地管理CPU和GPU内存分配和数据传输。这是具有挑战性的，因为任何在CPU和GPU之间共享的数据都需要在系统内存和GPU内存中分别进行两次分配。程序员必须使用显式的内存复制调用来在它们之间移动最新的数据。在正确的时间和地点保持数据增加了应用程序的复杂性，也增加了新的GPU程序员的学习曲线。

显式数据传输也可能在稀疏内存访问的情况下降低性能，例如，在CPU仅写入少量随机字节后将整个数组复制回GPU会增加传输延迟开销。管理内存传输、改进内存局部性以及使用异步内存复制等技术可以提高性能，但这些都需要更加细心的编程。

### 4.1. 统一内存历史

NVIDIA Fermi GPU架构于2009年推出，实现了跨越三个主要GPU内存空间（线程私有局部内存、线程块共享内存和全局内存）的统一GPU地址空间。这个统一的地址空间仅适用于GPU内存寻址，主要通过**启用单个load/store指令和指针地址**来访问任何GPU内存空间（全局、局部或共享内存），而不是为每个内存空间使用不同的指令和指针，从而实现了更简单的编译。这也启用了完全的C和C++指针支持，这在当时是一项重大进展。

2011年，**CUDA 4**引入了**统一虚拟寻址（UVA）**来为CPU和GPU内存提供单一的虚拟内存地址空间，并使指针能够从GPU代码中访问它们所在的系统中的任何位置，无论是在GPU内存（在同一或不同的GPU上）、CPU内存还是芯片上的共享内存中。UVA使得Zero-Copy内存成为可能，这时GPU代码可以直接通过PCIe访问的固定CPU内存，无需进行memcpy。Zero-Copy提供了统一内存一些便利，但性能一般，因为它始终由GPU通过PCIe的低带宽和高延迟进行访问。

**CUDA 6**引入了**统一内存**，它创建了一个由CPU和GPU共享的托管内存池，弥合了CPU和GPU之间的差距。使用单个指针，托管内存对CPU和GPU都是可访问的。CUDA系统软件会自动在GPU和CPU之间迁移分配在统一内存中的数据，以便在CPU上运行的代码看起来像CPU内存，而在GPU上运行的代码看起来像GPU内存。但是，CUDA 6统一内存受到Kepler和Maxwell GPU架构特性的限制：CPU访问的所有托管内存都**必须在任何内核启动之前与GPU进行同步**。CPU和GPU无法同时访问托管内存分配，统一内存地址空间也受限于GPU物理内存的大小。

![](https://pic1.zhimg.com/v2-5d854cf59a3405395f9d259ef1c2e43a_1440w.jpg)

CUDA6 统一内存

下面展示了在CUDA 6中，统一内存如何通过提供单个指针来访问数据，简化了将代码移植到GPU上的过程，使显式的CPU-GPU内存复制成为一种优化而不是必需品。

![](https://pic3.zhimg.com/v2-1bfd64ee44d494bfa6f6a7080838986a_1440w.jpg)

CUDA 6的统一内存通过提供一个新的托管内存分配器，返回一个指针，可以从CPU或GPU代码访问数据，从而简化了将代码移植到GPU上的过程。这个新的托管内存分配器创建了一个由CPU和GPU共享的托管内存池，使CPU和GPU都可以访问内存，而无需显式地进行CPU-GPU内存复制。

### 4.2. Pascal GP100 统一内存

基于CUDA 6统一内存进一步扩展，Pascal GP100增加了新的功能，进一步简化了CPU和GPU之间内存的共享和编程，使CPU并行计算应用程序更易于移植到GPU上以获得巨大加速效果。这些改进的主要硬件特性是支持大地址空间和页错误能力。

GP100扩展了GPU寻址能力，支持49位虚拟寻址。这足以覆盖现代CPU的48位虚拟地址空间以及GPU自身的内存。这使得GP100统一内存程序可以作为单一虚拟地址空间访问系统中所有CPU和GPU的完整地址空间，不受任何处理器物理内存大小的限制。

GP100的**内存页错误**支持是一个关键的新特性，提供了更无缝的统一内存功能。结合系统范围的虚拟地址空间，页错误提供了几个优点。首先，页错误意味着CUDA系统软件不需要在每个内核启动之前将所有托管内存分配与GPU同步。如果在GPU上运行的内核访问不驻留在其内存中的页面，它将产生错误，从而允许页面按需自动迁移至GPU内存。或者，页面也可以映射到GPU地址空间以通过PCIe或NVLink互连进行访问（访问映射有时比迁移更快）。请注意，统一内存是系统范围的：GPU（和CPU）可以从CPU内存或系统中其他GPU的内存中故障和迁移内存页面。

![](https://pic2.zhimg.com/v2-d06e8ebb980d4808686a069cdd7833e9_1440w.jpg)

Pascal GP100统一内存不受GPU物理内存大小的限制

使用新的页错误机制，统一内存可以保证全局数据一致性。这意味着在GP100上，CPU和GPU可以访问统一内存分配，而无需进行任何程序员同步。在Kepler和Maxwell GPUs上，这是非法的，因为如果CPU在GPU内核处于活动状态时访问统一内存分配，将无法保证一致性。

> 注：与任何并行应用程序一样，开发人员需要确保正确的同步，以避免处理器之间的数据冲突。

最后，在支持操作系统平台方面，使用默认的操作系统分配器（例如，malloc或new）分配的内存可以使用相同的指针从GPU代码和CPU代码访问。在这些系统上，**统一内存可以作为默认选项**：无需使用特殊的分配器或创建特殊的托管内存池。此外，GP100的大虚拟地址空间和页错误能力使应用程序能够访问整个系统虚拟内存。这意味着应用程序可以超额使用内存系统：换句话说，它们可以**分配、访问和共享大于系统总物理容量的数组**，从而实现非常大数据集的外围处理。

为了使用系统分配器启用统一内存，需要对操作系统进行一定的修改。 NVIDIA正在与Red Hat合作，并在Linux社区内开展工作，以启用这个强大的功能。

![](https://pic4.zhimg.com/v2-99a176f9b7bfef21625ed75b396a39b3_1440w.jpg)

有了操作系统的支持，Pascal可以使用默认的系统分配器支持统一内存。这意味着只需要使用malloc分配的内存就可以在系统中的任何CPU或GPU上访问。

### 4.3. 统一内存的效益

程序员从统一内存中受益主要有两个方面：

- **更简单的编程和内存模型**。统一内存通过将显式的设备内存管理作为一种优化而不是必需品，降低了在GPU上进行并行编程的门槛。统一内存使程序员专注于开发并行代码，而不会被分配和复制设备内存的细节所困扰。这使得学习GPU编程更容易，将现有代码移植到GPU上更简单。但这不仅适用于初学者，统一内存也使得在GPU上**使用复杂的数据结构和C++类更加容易**。在支持默认系统分配器的系统上，任何分层或嵌套的数据结构都可以自动从系统中的任何处理器访问。在GP100上，应用程序可以对大于系统总内存大小的数据集进行外围操作。
- **通过数据局部性实现更高的性能**。通过根据需要在CPU和GPU之间迁移数据，统一内存可以提供与GPU本地数据相同的性能，同时提供全局共享数据的易用性。这种功能的复杂性被隐藏在CUDA驱动程序和运行时中，确保应用程序代码更简单。迁移的目的是实现每个处理器的全带宽；**高性能HBM2内存**带宽对于支持GP100 GPU的计算吞吐量至关重要。在GP100上使用**页错误**时，即使对于数据访问较稀疏的程序，也可以确保局部性，其中CPU或GPU访问的页面不能提前知道，并且CPU和GPU同时访问相同数组分配的部分。

一个重要的点是，CUDA程序员仍然有必要显式优化数据管理和CPU-GPU并发性的工具：CUDA 8将引入有用的API，提供内存使用提示和显式预取。这些工具允许与显式内存复制和固定API相同的功能，而不会回到显式GPU内存分配的限制。

## 5. 计算抢占

新的Pascal GP100 Compute Preemption功能允许在GPU上运行的计算任务在指令级别粒度上被中断，并将其上下文切换到GPU DRAM。这允许其他应用程序被切换并运行，然后将原始任务的上下文切换回来，继续执行中断前的操作。

计算抢占（Compute Preemption）解决了长时间运行或行为不正常的应用程序可能垄断系统，导致系统变得不响应，等待任务完成可能会导致操作系统或CUDA驱动程序超时或被终止的重要问题。在Pascal之前，在计算和显示任务在同一GPU上运行的系统上，长时间运行的计算内核可能会导致操作系统和其他视觉应用程序变得不响应和非交互，直到内核超时。因此，程序员不得不安装专门的仅用于计算的GPU或仔细编写应用程序，以绕过先前GPU的限制，将其工作负载分成较小的执行时间片，以便它们不会超时或被操作系统终止。

实际上，许多应用程序确实需要长时间运行的过程，并且在GP100中使用Compute Preemption时，这些应用程序现在可以在处理大型数据集或等待特定条件发生时运行所需的时间，而视觉应用程序仍然保持流畅和交互性，但不会以程序员努力使代码运行在小时间片的代价为代价。

Compute Preemption还允许在单个GPU系统上交互式调试计算内核。这对于开发人员的生产力是一个重要的功能。相比之下，Kepler GPU架构仅在计算内核的线程块级别上提供了更粗粒度的抢占。这种块级别的抢占要求线程块的所有线程完成，然后硬件才能上下文切换到不同的上下文。但是，当使用调试器并在线程块内的指令上触发GPU断点时，线程块尚未完成，从而阻止了块级别的抢占。尽管Kepler和Maxwell仍然能够通过在编译过程中添加仪器来提供调试器的核心功能，但GP100能够支持更强大和轻量级的调试器实现。

## 6. 总结

NVIDIA的新NVIDIA Tesla P100 GPU加速器采用了Pascal架构，集成了突破性的创新，使客户能够解决以前无法解决的计算问题。从上到下，NVIDIA Tesla P100具有惊人的创新——计算性能、内存带宽、容量、连接性和功率效率——这些创新是下一代高性能计算和人工智能系统的计算引擎所必需的。

## 7. 参考链接

[pascal-architecture-whitepaper](https://link.zhihu.com/?target=https%3A//images.nvidia.cn/content/pdf/tesla/whitepaper/pascal-architecture-whitepaper.pdf)

# Volta架构

https://www.nvidia.cn/data-center/volta-gpu-architecture/


# Turing架构

https://www.nvidia.cn/design-visualization/technologies/turing-architecture/

# Ampere架构

https://www.nvidia.cn/data-center/ampere-architecture/
# Ada Lovelace

https://www.nvidia.com/en-us/technologies/ada-architecture/

# Hopper架构

https://www.nvidia.cn/about-nvidia/ai-computing/
# Blackwell架构

https://www.nvidia.cn/data-center/technologies/blackwell-architecture/



