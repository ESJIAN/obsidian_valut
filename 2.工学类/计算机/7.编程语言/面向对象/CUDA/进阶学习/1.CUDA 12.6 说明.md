- 参考：[CUDA 12.6 Update 3 Release Notes (nvidia.com)](https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#cusolver-library)


<!-- Media -->

<img src="https://cdn.noedgeai.com/01944594-c9f3-70f6-8c1f-029891167ffb_0.jpg?x=204&y=241&w=1390&h=484&r=0"/>

<!-- Media -->

Release Notes

发布说明

Release 12.6

版本 12.6

NVIDIA Corporation

NVIDIA Corporation

Nov 14, 2024

2024年11月14日

## Contents

## 目录

1 CUDA Toolkit Major Component Versions 3

1 CUDA 工具包主要组件版本 3

2 New Features 9

2 新特性 9

2.1 General CUDA 9

2.1 通用 CUDA 9

2.2 CUDA Compiler 9

2.2 CUDA 编译器 9

2.3 CUDA Developer Tools 10

2.3 CUDA 开发者工具 10

3 Resolved Issues 11

3 已解决的问题 11

3.1 CUDA Compiler 11

3.1 CUDA 编译器 11

4 Known Issues and Limitations 13

4 已知问题和限制 13

5 Deprecated or Dropped Features 15

5 已弃用或删除的功能 15

5.1 Deprecated or Dropped Operating Systems 15

5.1 已弃用或删除的操作系统 15

5.2 Deprecated Toolchains 15

5.2 已弃用的工具链 15

5.3 CUDA Tools 15

5.3 CUDA 工具 15

6 CUDA Libraries 17

6 CUDA 库 17

6.1 cuBLAS Library 17

6.1 cuBLAS 库 17

6.1.1 cuBLAS: Release 12.6 Update 3 17

6.1.1 cuBLAS: 版本 12.6 更新 3 17

6.1.2 cuBLAS: Release 12.6 Update 2 18

6.1.2 cuBLAS: 版本 12.6 更新 2 18

6.1.3 cuBLAS: Release 12.6 Update 1 19

6.1.3 cuBLAS: 版本 12.6 更新 1 19

6.1.4 cuBLAS: Release 12.6 19

6.1.4 cuBLAS: 版本 12.6 19

6.1.5 cuBLAS: Release 12.5 Update 1 20

6.1.5 cuBLAS: 版本 12.5 更新 1 20

6.1.6 cuBLAS: Release 12.5 20

6.1.6 cuBLAS: 版本 12.5 20

6.1.7 cuBLAS: Release 12.4 Update 1 21

6.1.7 cuBLAS: 版本 12.4 更新 1 21

6.1.8 cuBLAS: Release 12.4 21

6.1.8 cuBLAS: 版本 12.4 21

6.1.9 cuBLAS: Release 12.3 Update 1 22

6.1.9 cuBLAS: 版本 12.3 更新 1 22

6.1.10 cuBLAS: Release 12.3 22

6.1.10 cuBLAS: 版本 12.3 22

6.1.11 cuBLAS: Release 12.2 Update 2 23

6.1.11 cuBLAS: 版本 12.2 更新 2 23

6.1.12 cuBLAS: Release 12.2 23

6.1.12 cuBLAS: 版本 12.2 23

6.1.13 cuBLAS: Release 12.1 Update 1 23

6.1.13 cuBLAS: 版本 12.1 更新 1 23

6.1.14 cuBLAS: Release 12.0 Update 1 24

6.1.14 cuBLAS: 版本 12.0 更新 1 24

6.1.15 cuBLAS: Release 12.0 24

6.1.15 cuBLAS: 版本 12.0 24

6.2 cuFFT Library 25

6.2 cuFFT 库 25

6.2.1 cuFFT: Release 12.6 Update 2 25

6.2.1 cuFFT: 版本 12.6 更新 2 25

6.2.2 cuFFT: Release 12.6 26

6.2.2 cuFFT: 版本 12.6 26

6.2.3 cuFFT: Release 12.5 26

6.2.3 cuFFT: 版本 12.5 26

6.2.4 cuFFT: Release 12.4 Update 1 26

6.2.4 cuFFT: 版本 12.4 更新 1 26

6.2.5 cuFFT: Release 12.4 26

6.2.5 cuFFT: 版本 12.4 26

6.2.6 cuFFT: Release 12.3 Update 1 27

6.2.6 cuFFT: 版本 12.3 更新 1 27

6.2.7 cuFFT: Release 12.3 27

6.2.7 cuFFT: 版本 12.3 27

6.2.8 cuFFT: Release 12.2 27

6.2.8 cuFFT: 版本 12.2 27

6.2.9 cuFFT: Release 12.1 Update 1 28

6.2.9 cuFFT: 版本 12.1 更新 1 28

6.2.10 cuFFT: Release 12.1 28

6.2.10 cuFFT: 版本 12.1 28

6.2.11 cuFFT: Release 12.0 Update 1 28

6.2.11 cuFFT: 版本 12.0 更新 1 28

6.2.12 cuFFT: Release 12.0 28

6.2.12 cuFFT: 版本 12.0 28

6.3 cuSOLVER Library 29

6.3 cuSOLVER 库 29

6.3.1 cuSOLVER: Release 12.6 Update 2 29

6.3.1 cuSOLVER: 版本 12.6 更新 2 29

6.3.2 cuSOLVER: Release 12.6 29

6.3.2 cuSOLVER: 版本 12.6 29

6.3.3 cuSOLVER: Release 12.5 Update 1 29

6.3.3 cuSOLVER: 版本 12.5 更新 1 29

6.3.4 cuSOLVER: Release 12.5 29

6.3.4 cuSOLVER: 版本 12.5 29

6.3.5 cuSOLVER: Release 12.4 Update 1 30

6.3.5 cuSOLVER: 版本 12.4 更新 1 30

6.3.6 cuSOLVER: Release 12.4 30

6.3.6 cuSOLVER: 版本 12.4 30

6.3.7 cuSOLVER: Release 12.2 Update 2 31

6.3.7 cuSOLVER: 版本 12.2 更新 2 31

6.3.8 cuSOLVER: Release 12.2 31

6.3.8 cuSOLVER: 版本 12.2 31

6.4 cuSPARSE Library 31

6.4 cuSPARSE 库 31

6.4.1 cuSPARSE: Release 12.6 Update 2 31

6.4.1 cuSPARSE: 版本 12.6 更新 2 31

6.4.2 cuSPARSE: Release 12.6 32

6.4.2 cuSPARSE: 版本 12.6 32

6.4.3 cuSPARSE: Release 12.5 Update 1 32

6.4.3 cuSPARSE: 版本 12.5 更新 1 32

6.4.4 cuSPARSE: Release 12.5 32

6.4.4 cuSPARSE: 版本 12.5 32

6.4.5 cuSPARSE: Release 12.4 33

6.4.5 cuSPARSE: 版本 12.4 33

6.4.6 cuSPARSE: Release 12.3 Update 1 33

6.4.6 cuSPARSE: 版本 12.3 更新 1 33

6.4.7 cuSPARSE: Release 12.3 33

6.4.7 cuSPARSE: 版本 12.3 33

6.4.8 cuSPARSE: Release 12.2 Update 1 34

6.4.8 cuSPARSE: 版本 12.2 更新 1 34

6.4.9 cuSPARSE: Release 12.1 Update 1 34

6.4.9 cuSPARSE: 版本 12.1 更新 1 34

6.4.10 cuSPARSE: Release 12.0 Update 1 34

6.4.10 cuSPARSE: 12.0 更新 1 34

6.4.11 cuSPARSE: Release 12.0 35

6.4.11 cuSPARSE: 12.0 35

6.5 Math Library 35

6.5 数学库 35

6.5.1 CUDA Math: Release 12.6 Update 1 35

6.5.1 CUDA 数学: 12.6 更新 1 35

6.5.2 CUDA Math: Release 12.6 36

6.5.2 CUDA 数学: 12.6 36

6.5.3 CUDA Math: Release 12.5 36

6.5.3 CUDA 数学: 12.5 36

6.5.4 CUDA Math: Release 12.4 36

6.5.4 CUDA 数学：版本 12.4 36

6.5.5 CUDA Math: Release 12.3 36

6.5.5 CUDA 数学：版本 12.3 36

6.5.6 CUDA Math: Release 12.2 37

6.5.6 CUDA 数学：版本 12.2 37

6.5.7 CUDA Math: Release 12.1 37

6.5.7 CUDA 数学：版本 12.1 37

6.5.8 CUDA Math: Release 12.0 37

6.5.8 CUDA 数学：版本 12.0 37

6.6 NVIDIA Performance Primitives (NPP) 38

6.6 NVIDIA 性能原语 (NPP) 38

6.6.1 NPP: Release 12.4 38

6.6.1 NPP: 版本 12.4 38

6.6.2 NPP: Release 12.0 38

6.6.2 NPP: 版本 12.0 38

6.7 nvJPEG Library 38

6.7 nvJPEG 库 38

6.7.1 nvJPEG: Release 12.4 38

6.7.1 nvJPEG: 版本 12.4 38

6.7.2 nvJPEG: Release 12.3 Update 1 38

6.7.2 nvJPEG: 版本 12.3 更新 1 38

6.7.3 nvJPEG: Release 12.2 39

6.7.3 nvJPEG: 版本 12.2 39

6.7.4 nvJPEG: Release 12.0 39

6.7.4 nvJPEG: 版本 12.0 39

7 Notices 41

7 通知 41

7.1 Notice 41

7.1 通知 41

7.2 OpenCL 42

7.2 OpenCL 42

7.3 Trademarks 42

7.3 商标 42

## NVIDIA CUDA Toolkit Release Notes

## NVIDIA CUDA 工具包发布说明

## The Release Notes for the CUDA Toolkit.

## CUDA 工具包的发布说明。

The release notes for the NVIDIA® CUDA® Toolkit can be found online at https://docs.nvidia.com/cuda/ cuda-toolkit-release-notes/index.html.

NVIDIA® CUDA® 工具包的发布说明可以在 https://docs.nvidia.com/cuda/ cuda-toolkit-release-notes/index.html 在线找到。

Note: The release notes have been reorganized into two major sections: the general CUDA release notes, and the CUDA libraries release notes including historical information for 12.x releases. Release Notes, Release 12.6

注意：发布说明已重新组织为两个主要部分：通用 CUDA 发布说明和 CUDA 库发布说明，包括 12.x 版本的历史信息。发布说明，12.6 版

# Chapter 1. CUDA Toolkit Major Component Versions

# 第 1 章. CUDA 工具包主要组件版本

## CUDA Components

## CUDA 组件

Starting with CUDA 11, the various components in the toolkit are versioned independently.

从 CUDA 11 开始，工具包中的各个组件是独立版本化的。

For CUDA 12.6 Update 3, the table below indicates the versions:

对于 CUDA 12.6 更新 3，下表显示了版本：

<!-- Media -->

Table 1: CUDA 12.6 Update 3 Component Versions

表 1：CUDA 12.6 更新 3 组件版本

<table><tr><td colspan="2">Component Name</td><td>Version Informa- tion</td><td>Supported Archi- tectures</td><td>Supported Plat- forms</td></tr><tr><td rowspan="4">CUDA C++ Core Compute Li- braries</td><td>Thrust</td><td>2.5.0</td><td rowspan="4">x86_64, arm64- sbsa, aarch64- jetson</td><td rowspan="4">Linux, Windows</td></tr><tr><td>CUB</td><td>2.5.0</td></tr><tr><td>libcu++</td><td>2.5.0</td></tr><tr><td>Cooperative Groups</td><td>12.6.77</td></tr><tr><td colspan="2">CUDA Compatibility</td><td>12.6.36890662</td><td>aarch64-jetson</td><td>Linux</td></tr><tr><td colspan="2">CUDA Runtime (cudart)</td><td>12.6.77</td><td>x86_64, arm64- sbsa, aarch64- jetson</td><td>Linux, Windows, WSL</td></tr><tr><td colspan="2">cuobjdump</td><td>12.6.77</td><td>x86_64, arm64- sbsa, aarch64- jetson</td><td>Linux, Windows</td></tr><tr><td colspan="2">CUPTI</td><td>12.6.80</td><td>x86_64, arm64- sbsa, aarch64- jetson</td><td>Linux, Windows, WSL</td></tr><tr><td colspan="2">CUDA cuxxfilt (demangler)</td><td>12.6.77</td><td>x86_64, sbsa, jetson</td><td>Linux, Windows</td></tr><tr><td colspan="2">CUDA Demo Suite</td><td>12.6.77</td><td>x86_64</td><td>Linux, Windows</td></tr><tr><td colspan="2">CUDA GDB</td><td>12.6.77</td><td>x86_64, arm64- sbsa, aarch64- jetson</td><td>Linux, WSL</td></tr></table>

<table><tbody><tr><td colspan="2">组件名称</td><td>版本信息</td><td>支持的架构</td><td>支持的平台</td></tr><tr><td rowspan="4">CUDA C++ 核心计算库</td><td>Thrust</td><td>2.5.0</td><td rowspan="4">x86_64, arm64- sbsa, aarch64- jetson</td><td rowspan="4">Linux, Windows</td></tr><tr><td>CUB</td><td>2.5.0</td></tr><tr><td>libcu++</td><td>2.5.0</td></tr><tr><td>协作组</td><td>12.6.77</td></tr><tr><td colspan="2">CUDA 兼容性</td><td>12.6.36890662</td><td>aarch64-jetson</td><td>Linux</td></tr><tr><td colspan="2">CUDA 运行时 (cudart)</td><td>12.6.77</td><td>x86_64, arm64- sbsa, aarch64- jetson</td><td>Linux, Windows, WSL</td></tr><tr><td colspan="2">cuobjdump</td><td>12.6.77</td><td>x86_64, arm64- sbsa, aarch64- jetson</td><td>Linux, Windows</td></tr><tr><td colspan="2">CUPTI</td><td>12.6.80</td><td>x86_64, arm64- sbsa, aarch64- jetson</td><td>Linux, Windows, WSL</td></tr><tr><td colspan="2">CUDA cuxxfilt（反混淆器）</td><td>12.6.77</td><td>x86_64, sbsa, jetson</td><td>Linux, Windows</td></tr><tr><td colspan="2">CUDA 演示套件</td><td>12.6.77</td><td>x86_64</td><td>Linux, Windows</td></tr><tr><td colspan="2">CUDA GDB</td><td>12.6.77</td><td>x86_64, arm64- sbsa, aarch64- jetson</td><td>Linux, WSL</td></tr></tbody></table>

continues on next page

继续下一页

Table 1 - continued from previous page

表 1 - 续上页

<table><tr><td>Component Name</td><td>Version Informa- tion</td><td>Supported Archi- tectures</td><td>Supported Plat- forms</td></tr><tr><td>CUDA Nsight Eclipse Plugin</td><td>12.6.77</td><td>x86_64</td><td>Linux</td></tr><tr><td>CUDA NVCC</td><td>12.6.85</td><td>x86_64, sbsa, aarch64- jetson</td><td>Linux, Windows, WSL</td></tr><tr><td>CUDA nvdisasm</td><td>12.6.77</td><td>x86_64, arm64- sbsa, aarch64- jetson</td><td>Linux, Windows</td></tr><tr><td>CUDA NVML Headers</td><td>12.6.77</td><td>x86_64, arm64- sbsa, aarch64- jetson</td><td>Linux, Windows, WSL</td></tr><tr><td>CUDA nvprof</td><td>12.6.80</td><td>x86_64</td><td>Linux, Windows</td></tr><tr><td>CUDA nvprune</td><td>12.6.77</td><td>x86_64, arm64- sbsa, aarch64- jetson</td><td>Linux, Windows, WSL</td></tr><tr><td>CUDA NVRTC</td><td>12.6.85</td><td>x86_64, arm64- sbsa, aarch64- jetson</td><td>Linux, Windows, WSL</td></tr><tr><td>NVTX</td><td>12.6.77</td><td>x86_64, arm64- sbsa, aarch64- jetson</td><td>Linux, Windows, WSL</td></tr><tr><td>CUDA NVVP</td><td>12.6.80</td><td>x86_64</td><td>Linux, Windows</td></tr><tr><td>CUDA OpenCL</td><td>12.6.77</td><td>x86_64</td><td>Linux, Windows</td></tr><tr><td>CUDA Profiler API</td><td>12.6.77</td><td>x86_64, arm64- sbsa, aarch64- jetson</td><td>Linux, Windows, WSL</td></tr><tr><td>CUDA Compute Sanitizer API</td><td>12.6.77</td><td>x86_64, arm64- sbsa, aarch64- jetson</td><td>Linux, Windows, WSL</td></tr><tr><td>CUDA cuBLAS</td><td>12.6.4.1</td><td>x86_64, arm64- sbsa, aarch64- jetson</td><td>Linux, Windows, WSL</td></tr><tr><td>CUDLA</td><td>12.6.77</td><td>aarch64-jetson</td><td>Linux</td></tr><tr><td>CUDA cuFFT</td><td>11.3.0.4</td><td>x86_64, arm64- sbsa, aarch64- jetson</td><td>Linux, Windows, WSL</td></tr><tr><td>CUDA cuFile</td><td>1.11.1.6</td><td>x86_64, arm64- sbsa, aarch64- jetson</td><td>Linux</td></tr></table>

<table><tbody><tr><td>组件名称</td><td>版本信息</td><td>支持的架构</td><td>支持的平台</td></tr><tr><td>CUDA Nsight Eclipse 插件</td><td>12.6.77</td><td>x86_64</td><td>Linux</td></tr><tr><td>CUDA NVCC</td><td>12.6.85</td><td>x86_64, sbsa, aarch64- jetson</td><td>Linux, Windows, WSL</td></tr><tr><td>CUDA nvdisasm</td><td>12.6.77</td><td>x86_64, arm64- sbsa, aarch64- jetson</td><td>Linux, Windows</td></tr><tr><td>CUDA NVML 头文件</td><td>12.6.77</td><td>x86_64, arm64- sbsa, aarch64- jetson</td><td>Linux, Windows, WSL</td></tr><tr><td>CUDA nvprof</td><td>12.6.80</td><td>x86_64</td><td>Linux, Windows</td></tr><tr><td>CUDA nvprune</td><td>12.6.77</td><td>x86_64, arm64- sbsa, aarch64- jetson</td><td>Linux, Windows, WSL</td></tr><tr><td>CUDA NVRTC</td><td>12.6.85</td><td>x86_64, arm64- sbsa, aarch64- jetson</td><td>Linux, Windows, WSL</td></tr><tr><td>NVTX</td><td>12.6.77</td><td>x86_64, arm64- sbsa, aarch64- jetson</td><td>Linux, Windows, WSL</td></tr><tr><td>CUDA NVVP</td><td>12.6.80</td><td>x86_64</td><td>Linux, Windows</td></tr><tr><td>CUDA OpenCL</td><td>12.6.77</td><td>x86_64</td><td>Linux, Windows</td></tr><tr><td>CUDA 分析器 API</td><td>12.6.77</td><td>x86_64, arm64- sbsa, aarch64- jetson</td><td>Linux, Windows, WSL</td></tr><tr><td>CUDA 计算消毒器 API</td><td>12.6.77</td><td>x86_64, arm64- sbsa, aarch64- jetson</td><td>Linux, Windows, WSL</td></tr><tr><td>CUDA cuBLAS</td><td>12.6.4.1</td><td>x86_64, arm64- sbsa, aarch64- jetson</td><td>Linux, Windows, WSL</td></tr><tr><td>CUDLA</td><td>12.6.77</td><td>aarch64-jetson</td><td>Linux</td></tr><tr><td>CUDA cuFFT</td><td>11.3.0.4</td><td>x86_64, arm64- sbsa, aarch64- jetson</td><td>Linux, Windows, WSL</td></tr><tr><td>CUDA cuFile</td><td>1.11.1.6</td><td>x86_64, arm64- sbsa, aarch64- jetson</td><td>Linux</td></tr></tbody></table>

continues on next page

继续下一页

Table 1 - continued from previous page

表 1 - 接上页

<table><tr><td>Component Name</td><td>Version Informa- tion</td><td>Supported Archi- tectures</td><td>Supported Plat- forms</td></tr><tr><td>CUDA cuRAND</td><td>10.3.7.77</td><td>x86_64, arm64- sbsa, aarch64- jetson</td><td>Linux, Windows, WSL</td></tr><tr><td>CUDA cuSOLVER</td><td>11.7.1.2</td><td>x86_64, arm64- sbsa, aarch64- jetson</td><td>Linux, Windows, WSL</td></tr><tr><td>CUDA cuSPARSE</td><td>12.5.4.2</td><td>x86_64, arm64- sbsa, aarch64- jetson</td><td>Linux, Windows, WSL</td></tr><tr><td>CUDA NPP</td><td>12.3.1.54</td><td>x86_64, arm64- sbsa, aarch64- jetson</td><td>Linux, Windows, WSL</td></tr><tr><td>CUDA nvFatbin</td><td>12.6.77</td><td>x86_64, arm64- sbsa, aarch64- jetson</td><td>Linux, Windows, WSL</td></tr><tr><td>CUDA nvJitLink</td><td>12.6.85</td><td>x86_64, arm64- sbsa, aarch64- jetson</td><td>Linux, Windows, WSL</td></tr><tr><td>CUDA nvJPEG</td><td>12.3.3.54</td><td>x86_64, arm64- sbsa, aarch64- jetson</td><td>Linux, Windows, WSL</td></tr><tr><td>Nsight Compute</td><td>2024.3.2.3</td><td>x86_64, arm64- sbsa, aarch64- jetson</td><td>Linux, Windows, WSL (Windows 11)</td></tr><tr><td>Nsight Systems</td><td>2024.5.1.113</td><td>x86_64, arm64- sbsa</td><td>Linux, Windows, WSL</td></tr><tr><td>Nsight Visual Studio Edition (VSE)</td><td>2024.3.0.24164</td><td>x86_64 (Win- dows)</td><td>Windows</td></tr><tr><td>nvidia_fs1</td><td>2.22.3</td><td>x86_64, arm64- sbsa, aarch64- jetson</td><td>Linux</td></tr><tr><td>Visual Studio Integration</td><td>12.6.77</td><td>x86_64 (Win- dows)</td><td>Windows</td></tr><tr><td>NVIDIA Linux Driver</td><td>560.35.05</td><td>x86_64, arm64- sbsa</td><td>Linux</td></tr><tr><td>NVIDIA Windows Driver</td><td>561.17</td><td>x86_64 (Win- dows)</td><td>Windows, WSL</td></tr></table>

<table><tbody><tr><td>组件名称</td><td>版本信息</td><td>支持的架构</td><td>支持的平台</td></tr><tr><td>CUDA cuRAND</td><td>10.3.7.77</td><td>x86_64, arm64-sbsa, aarch64-jetson</td><td>Linux, Windows, WSL</td></tr><tr><td>CUDA cuSOLVER</td><td>11.7.1.2</td><td>x86_64, arm64-sbsa, aarch64-jetson</td><td>Linux, Windows, WSL</td></tr><tr><td>CUDA cuSPARSE</td><td>12.5.4.2</td><td>x86_64, arm64-sbsa, aarch64-jetson</td><td>Linux, Windows, WSL</td></tr><tr><td>CUDA NPP</td><td>12.3.1.54</td><td>x86_64, arm64-sbsa, aarch64-jetson</td><td>Linux, Windows, WSL</td></tr><tr><td>CUDA nvFatbin</td><td>12.6.77</td><td>x86_64, arm64-sbsa, aarch64-jetson</td><td>Linux, Windows, WSL</td></tr><tr><td>CUDA nvJitLink</td><td>12.6.85</td><td>x86_64, arm64-sbsa, aarch64-jetson</td><td>Linux, Windows, WSL</td></tr><tr><td>CUDA nvJPEG</td><td>12.3.3.54</td><td>x86_64, arm64-sbsa, aarch64-jetson</td><td>Linux, Windows, WSL</td></tr><tr><td>Nsight Compute</td><td>2024.3.2.3</td><td>x86_64, arm64-sbsa, aarch64-jetson</td><td>Linux, Windows, WSL (Windows 11)</td></tr><tr><td>Nsight Systems</td><td>2024.5.1.113</td><td>x86_64, arm64- sbsa</td><td>Linux, Windows, WSL</td></tr><tr><td>Nsight Visual Studio Edition (VSE)</td><td>2024.3.0.24164</td><td>x86_64 (Windows)</td><td>Windows</td></tr><tr><td>nvidia_fs1</td><td>2.22.3</td><td>x86_64, arm64-sbsa, aarch64-jetson</td><td>Linux</td></tr><tr><td>Visual Studio 集成</td><td>12.6.77</td><td>x86_64 (Windows)</td><td>Windows</td></tr><tr><td>NVIDIA Linux 驱动程序</td><td>560.35.05</td><td>x86_64, arm64- sbsa</td><td>Linux</td></tr><tr><td>NVIDIA Windows 驱动程序</td><td>561.17</td><td>x86_64 (Windows)</td><td>Windows, WSL</td></tr></tbody></table>

<!-- Media -->

## CUDA Driver

## CUDA 驱动程序

Running a CUDA application requires the system with at least one CUDA capable GPU and a driver that is compatible with the CUDA Toolkit. See Table 3. For more information various GPU

运行 CUDA 应用程序需要系统至少具备一个支持 CUDA 的 GPU 和一个与 CUDA 工具包兼容的驱动程序。参见表 3。有关各种 GPU 的更多信息

${}^{1}$ Only available on select Linux distros products that are CUDA capable, visit https://developer.nvidia.com/cuda-gpus.

${}^{1}$ 仅在部分支持 CUDA 的 Linux 发行版产品上可用，请访问 https://developer.nvidia.com/cuda-gpus。

Each release of the CUDA Toolkit requires a minimum version of the CUDA driver. The CUDA driver is backward compatible, meaning that applications compiled against a particular version of the CUDA will continue to work on subsequent (later) driver releases.

每个 CUDA 工具包版本都需要一个最低版本的 CUDA 驱动程序。CUDA 驱动程序是向后兼容的，这意味着针对特定 CUDA 版本编译的应用程序将继续在后续（更新）的驱动程序版本上工作。

More information on compatibility can be found at https://docs.nvidia.com/cuda/ cuda-c-best-practices-guide/index.html#cuda-compatibility-and-upgrades.

有关兼容性的更多信息，请访问 https://docs.nvidia.com/cuda/ cuda-c-best-practices-guide/index.html#cuda-compatibility-and-upgrades。

Note: Starting with CUDA 11.0, the toolkit components are individually versioned, and the toolkit itself is versioned as shown in the table below.

注意：从 CUDA 11.0 开始，工具包组件分别进行版本控制，工具包本身的版本如下表所示。

The minimum required driver version for CUDA minor version compatibility is shown below. CUDA minor version compatibility is described in detail in https://docs.nvidia.com/deploy/ cuda-compatibility/index.html

CUDA 次要版本兼容性所需的最低驱动程序版本如下所示。CUDA 次要版本兼容性在 https://docs.nvidia.com/deploy/cuda-compatibility/index.html 中有详细描述。

<!-- Media -->

Table 2: CUDA Toolkit and Minimum Required Driver Version for CUDA Minor Version Compatibility

表 2：CUDA 工具包及 CUDA 次要版本兼容性所需的最低驱动程序版本

<table><tr><td>CUDA Toolkit</td><td colspan="2">Minimum Required Driver Version for CUDA Minor Version Compatibility*</td></tr><tr><td/><td>Linux x86_64 Driver Version</td><td>Windows x86_64 Driver Version</td></tr><tr><td>CUDA 12.x</td><td>>=525.60.13</td><td>>=528.33</td></tr><tr><td>CUDA 11.8.x CUDA 11.7.x CUDA 11.6.x CUDA 11.5.x CUDA 11.4.x CUDA 11.3.x CUDA 11.2.x CUDA 11.1.x</td><td>>=450.80.02</td><td>>=452.39</td></tr><tr><td>CUDA 11.0 (11.0.3)</td><td>>=450.36.06**</td><td>>=451.22**</td></tr></table>

<table><tbody><tr><td>CUDA 工具包</td><td colspan="2">CUDA 次要版本兼容性的最低要求驱动程序版本*</td></tr><tr><td></td><td>Linux x86_64 驱动程序版本</td><td>Windows x86_64 驱动程序版本</td></tr><tr><td>CUDA 12.x</td><td>>=525.60.13</td><td>>=528.33</td></tr><tr><td>CUDA 11.8.x CUDA 11.7.x CUDA 11.6.x CUDA 11.5.x CUDA 11.4.x CUDA 11.3.x CUDA 11.2.x CUDA 11.1.x</td><td>>=450.80.02</td><td>>=452.39</td></tr><tr><td>CUDA 11.0 (11.0.3)</td><td>>=450.36.06**</td><td>>=451.22**</td></tr></tbody></table>

<!-- Media -->

* Using a Minimum Required Version that is different from Toolkit Driver Version could be allowed in compatibility mode - please read the CUDA Compatibility Guide for details.

* 在兼容模式下，允许使用与工具包驱动程序版本不同的最低要求版本 - 详情请参阅 CUDA 兼容性指南。

** CUDA 11.0 was released with an earlier driver version, but by upgrading to Tesla Recommended Drivers 450.80.02 (Linux) / 452.39 (Windows), minor version compatibility is possible across the CUDA 11.x family of toolkits.

** CUDA 11.0 发布时附带了一个较早的驱动程序版本，但通过升级到 Tesla 推荐驱动程序 450.80.02（Linux）/ 452.39（Windows），可以在 CUDA 11.x 系列工具包之间实现次要版本兼容性。

The version of the development NVIDIA GPU Driver packaged in each CUDA Toolkit release is shown below.

每个 CUDA 工具包版本中包含的开发 NVIDIA GPU 驱动程序的版本如下所示。

<!-- Media -->

Table 3: CUDA Toolkit and Corresponding Driver Versions

表 3：CUDA 工具包及相应的驱动程序版本

<table><tr><td>CUDA Toolkit</td><td colspan="2">Toolkit Driver Version</td></tr><tr><td/><td>Linux x86_64 Driver Version</td><td>Windows x86_64 Driver Ver- sion</td></tr><tr><td>CUDA 12.6 Update 3</td><td>>=560.35.05</td><td>>=561.17</td></tr><tr><td>CUDA 12.6 Update 2</td><td>>=560.35.03</td><td>>=560.94</td></tr><tr><td>CUDA 12.6 Update 1</td><td>>=560.35.03</td><td>>=560.94</td></tr><tr><td>CUDA 12.6 GA</td><td>>=560.28.03</td><td>>=560.76</td></tr><tr><td>CUDA 12.5 Update 1</td><td>>=555.42.06</td><td>>=555.85</td></tr><tr><td>CUDA 12.5 GA</td><td>>=555.42.02</td><td>>=555.85</td></tr></table>

<table><tbody><tr><td>CUDA 工具包</td><td colspan="2">工具包驱动程序版本</td></tr><tr><td></td><td>Linux x86_64 驱动程序版本</td><td>Windows x86_64 驱动程序版本</td></tr><tr><td>CUDA 12.6 更新 3</td><td>>=560.35.05</td><td>>=561.17</td></tr><tr><td>CUDA 12.6 更新 2</td><td>>=560.35.03</td><td>>=560.94</td></tr><tr><td>CUDA 12.6 更新 1</td><td>>=560.35.03</td><td>>=560.94</td></tr><tr><td>CUDA 12.6 正式发布</td><td>>=560.28.03</td><td>>=560.76</td></tr><tr><td>CUDA 12.5 更新 1</td><td>>=555.42.06</td><td>>=555.85</td></tr><tr><td>CUDA 12.5 正式发布</td><td>>=555.42.02</td><td>>=555.85</td></tr></tbody></table>

continues on next page

继续下一页

Table 3 - continued from previous page

表 3 - 接上页

<table><tr><td>CUDA Toolkit</td><td colspan="2">Toolkit Driver Version</td></tr><tr><td>CUDA 12.4 Update 1</td><td>>=550.54.15</td><td>>=551.78</td></tr><tr><td>CUDA 12.4 GA</td><td>>=550.54.14</td><td>>=551.61</td></tr><tr><td>CUDA 12.3 Update 1</td><td>>=545.23.08</td><td>>=546.12</td></tr><tr><td>CUDA 12.3 GA</td><td>>=545.23.06</td><td>>=545.84</td></tr><tr><td>CUDA 12.2 Update 2</td><td>>=535.104.05</td><td>>=537.13</td></tr><tr><td>CUDA 12.2 Update 1</td><td>>=535.86.09</td><td>>=536.67</td></tr><tr><td>CUDA 12.2 GA</td><td>>=535.54.03</td><td>>=536.25</td></tr><tr><td>CUDA 12.1 Update 1</td><td>>=530.30.02</td><td>>=531.14</td></tr><tr><td>CUDA 12.1 GA</td><td>>=530.30.02</td><td>>=531.14</td></tr><tr><td>CUDA 12.0 Update 1</td><td>>=525.85.12</td><td>>=528.33</td></tr><tr><td>CUDA 12.0 GA</td><td>>=525.60.13</td><td>>=527.41</td></tr><tr><td>CUDA 11.8 GA</td><td>>=520.61.05</td><td>>=520.06</td></tr><tr><td>CUDA 11.7 Update 1</td><td>>=515.48.07</td><td>>=516.31</td></tr><tr><td>CUDA 11.7 GA</td><td>>=515.43.04</td><td>>=516.01</td></tr><tr><td>CUDA 11.6 Update 2</td><td>>=510.47.03</td><td>>=511.65</td></tr><tr><td>CUDA 11.6 Update 1</td><td>>=510.47.03</td><td>>=511.65</td></tr><tr><td>CUDA 11.6 GA</td><td>>=510.39.01</td><td>>=511.23</td></tr><tr><td>CUDA 11.5 Update 2</td><td>>=495.29.05</td><td>>=496.13</td></tr><tr><td>CUDA 11.5 Update 1</td><td>>=495.29.05</td><td>>=496.13</td></tr><tr><td>CUDA 11.5 GA</td><td>>=495.29.05</td><td>>=496.04</td></tr><tr><td>CUDA 11.4 Update 4</td><td>>=470.82.01</td><td>>=472.50</td></tr><tr><td>CUDA 11.4 Update 3</td><td>>=470.82.01</td><td>>=472.50</td></tr><tr><td>CUDA 11.4 Update 2</td><td>>=470.57.02</td><td>>=471.41</td></tr><tr><td>CUDA 11.4 Update 1</td><td>>=470.57.02</td><td>>=471.41</td></tr><tr><td>CUDA 11.4.0 GA</td><td>>=470.42.01</td><td>>=471.11</td></tr><tr><td>CUDA 11.3.1 Update 1</td><td>>=465.19.01</td><td>>=465.89</td></tr><tr><td>CUDA 11.3.0 GA</td><td>>=465.19.01</td><td>>=465.89</td></tr><tr><td>CUDA 11.2.2 Update 2</td><td>>=460.32.03</td><td>>=461.33</td></tr><tr><td>CUDA 11.2.1 Update 1</td><td>>=460.32.03</td><td>>=461.09</td></tr><tr><td>CUDA 11.2.0 GA</td><td>>=460.27.03</td><td>>=460.82</td></tr><tr><td>CUDA 11.1.1 Update 1</td><td>>=455.32</td><td>>=456.81</td></tr><tr><td>CUDA 11.1 GA</td><td>>=455.23</td><td>>=456.38</td></tr></table>

<table><tbody><tr><td>CUDA 工具包</td><td colspan="2">工具包驱动程序版本</td></tr><tr><td>CUDA 12.4 更新 1</td><td>>=550.54.15</td><td>>=551.78</td></tr><tr><td>CUDA 12.4 GA</td><td>>=550.54.14</td><td>>=551.61</td></tr><tr><td>CUDA 12.3 更新 1</td><td>>=545.23.08</td><td>>=546.12</td></tr><tr><td>CUDA 12.3 GA</td><td>>=545.23.06</td><td>>=545.84</td></tr><tr><td>CUDA 12.2 更新 2</td><td>>=535.104.05</td><td>>=537.13</td></tr><tr><td>CUDA 12.2 更新 1</td><td>>=535.86.09</td><td>>=536.67</td></tr><tr><td>CUDA 12.2 正式发布</td><td>>=535.54.03</td><td>>=536.25</td></tr><tr><td>CUDA 12.1 更新 1</td><td>>=530.30.02</td><td>>=531.14</td></tr><tr><td>CUDA 12.1 正式发布</td><td>>=530.30.02</td><td>>=531.14</td></tr><tr><td>CUDA 12.0 更新 1</td><td>>=525.85.12</td><td>>=528.33</td></tr><tr><td>CUDA 12.0 GA</td><td>>=525.60.13</td><td>>=527.41</td></tr><tr><td>CUDA 11.8 GA</td><td>>=520.61.05</td><td>>=520.06</td></tr><tr><td>CUDA 11.7 更新 1</td><td>>=515.48.07</td><td>>=516.31</td></tr><tr><td>CUDA 11.7 GA</td><td>>=515.43.04</td><td>>=516.01</td></tr><tr><td>CUDA 11.6 更新 2</td><td>>=510.47.03</td><td>>=511.65</td></tr><tr><td>CUDA 11.6 更新 1</td><td>>=510.47.03</td><td>>=511.65</td></tr><tr><td>CUDA 11.6 GA</td><td>>=510.39.01</td><td>>=511.23</td></tr><tr><td>CUDA 11.5 更新 2</td><td>>=495.29.05</td><td>>=496.13</td></tr><tr><td>CUDA 11.5 更新 1</td><td>>=495.29.05</td><td>>=496.13</td></tr><tr><td>CUDA 11.5 GA</td><td>>=495.29.05</td><td>>=496.04</td></tr><tr><td>CUDA 11.4 更新 4</td><td>>=470.82.01</td><td>>=472.50</td></tr><tr><td>CUDA 11.4 更新 3</td><td>>=470.82.01</td><td>>=472.50</td></tr><tr><td>CUDA 11.4 更新 2</td><td>>=470.57.02</td><td>>=471.41</td></tr><tr><td>CUDA 11.4 更新 1</td><td>>=470.57.02</td><td>>=471.41</td></tr><tr><td>CUDA 11.4.0 GA</td><td>>=470.42.01</td><td>>=471.11</td></tr><tr><td>CUDA 11.3.1 更新 1</td><td>>=465.19.01</td><td>>=465.89</td></tr><tr><td>CUDA 11.3.0 GA</td><td>>=465.19.01</td><td>>=465.89</td></tr><tr><td>CUDA 11.2.2 更新 2</td><td>>=460.32.03</td><td>>=461.33</td></tr><tr><td>CUDA 11.2.1 更新 1</td><td>>=460.32.03</td><td>>=461.09</td></tr><tr><td>CUDA 11.2.0 GA</td><td>>=460.27.03</td><td>>=460.82</td></tr><tr><td>CUDA 11.1.1 更新 1</td><td>>=455.32</td><td>>=456.81</td></tr><tr><td>CUDA 11.1 GA</td><td>>=455.23</td><td>>=456.38</td></tr></tbody></table>

continues on next page

继续下一页

Table 3 - continued from previous page

表 3 - 接上页

<table><tr><td>CUDA Toolkit</td><td colspan="2">Toolkit Driver Version</td></tr><tr><td>CUDA 11.0.3 Update 1</td><td>>= 450.51.06</td><td>>= 451.82</td></tr><tr><td>CUDA 11.0.2 GA</td><td>>= 450.51.05</td><td>>= 451.48</td></tr><tr><td>CUDA 11.0.1 RC</td><td>>= 450.36.06</td><td>>= 451.22</td></tr><tr><td>CUDA 10.2.89</td><td>>= 440.33</td><td>>= 441.22</td></tr><tr><td>CUDA 10.1 (10.1.105 general release, and updates)</td><td>>= 418.39</td><td>>= 418.96</td></tr><tr><td>CUDA 10.0.130</td><td>>= 410.48</td><td>>= 411.31</td></tr><tr><td>CUDA 9.2 (9.2.148 Update 1)</td><td>>= 396.37</td><td>>= 398.26</td></tr><tr><td>CUDA 9.2 (9.2.88)</td><td>>= 396.26</td><td>>= 397.44</td></tr><tr><td>CUDA 9.1 (9.1.85)</td><td>>= 390.46</td><td>>= 391.29</td></tr><tr><td>CUDA 9.0 (9.0.76)</td><td>>= 384.81</td><td>>= 385.54</td></tr><tr><td>CUDA 8.0 (8.0.61 GA2)</td><td>>= 375.26</td><td>>= 376.51</td></tr><tr><td>CUDA 8.0 (8.0.44)</td><td>>= 367.48</td><td>>= 369.30</td></tr><tr><td>CUDA 7.5 (7.5.16)</td><td>>= 352.31</td><td>>= 353.66</td></tr><tr><td>CUDA 7.0 (7.0.28)</td><td>>= 346.46</td><td>>= 347.62</td></tr></table>

<table><tbody><tr><td>CUDA 工具包</td><td colspan="2">工具包驱动程序版本</td></tr><tr><td>CUDA 11.0.3 更新 1</td><td>>= 450.51.06</td><td>>= 451.82</td></tr><tr><td>CUDA 11.0.2 GA</td><td>>= 450.51.05</td><td>>= 451.48</td></tr><tr><td>CUDA 11.0.1 RC</td><td>>= 450.36.06</td><td>>= 451.22</td></tr><tr><td>CUDA 10.2.89</td><td>>= 440.33</td><td>>= 441.22</td></tr><tr><td>CUDA 10.1 (10.1.105 正式发布版及更新)</td><td>>= 418.39</td><td>>= 418.96</td></tr><tr><td>CUDA 10.0.130</td><td>>= 410.48</td><td>>= 411.31</td></tr><tr><td>CUDA 9.2 (9.2.148 更新 1)</td><td>>= 396.37</td><td>>= 398.26</td></tr><tr><td>CUDA 9.2 (9.2.88)</td><td>>= 396.26</td><td>>= 397.44</td></tr><tr><td>CUDA 9.1 (9.1.85)</td><td>>= 390.46</td><td>>= 391.29</td></tr><tr><td>CUDA 9.0 (9.0.76)</td><td>>= 384.81</td><td>>= 385.54</td></tr><tr><td>CUDA 8.0 (8.0.61 GA2)</td><td>>= 375.26</td><td>>= 376.51</td></tr><tr><td>CUDA 8.0 (8.0.44)</td><td>>= 367.48</td><td>>= 369.30</td></tr><tr><td>CUDA 7.5 (7.5.16)</td><td>>= 352.31</td><td>>= 353.66</td></tr><tr><td>CUDA 7.0 (7.0.28)</td><td>>= 346.46</td><td>>= 347.62</td></tr></tbody></table>

<!-- Media -->

For convenience, the NVIDIA driver is installed as part of the CUDA Toolkit installation. Note that this driver is for development purposes and is not recommended for use in production with Tesla GPUs.

为了方便起见，NVIDIA 驱动程序作为 CUDA 工具包安装的一部分进行安装。请注意，此驱动程序用于开发目的，不建议在生产环境中与 Tesla GPU 一起使用。

For running CUDA applications in production with Tesla GPUs, it is recommended to download the latest driver for Tesla GPUs from the NVIDIA driver downloads site at https://www.nvidia.com/drivers.

为了在生产环境中使用 Tesla GPU 运行 CUDA 应用程序，建议从 NVIDIA 驱动程序下载站点 https://www.nvidia.com/drivers 下载适用于 Tesla GPU 的最新驱动程序。

During the installation of the CUDA Toolkit, the installation of the NVIDIA driver may be skipped on Windows (when using the interactive or silent installation) or on Linux (by using meta packages).

在安装 CUDA 工具包期间，可以在 Windows 上（使用交互式或静默安装时）或 Linux 上（通过使用元包）跳过 NVIDIA 驱动程序的安装。

For more information on customizing the install process on Windows, see https://docs.nvidia.com/ cuda/cuda-installation-guide-microsoft-windows/index.html#install-cuda-software.

有关在 Windows 上自定义安装过程的更多信息，请参阅 https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html#install-cuda-software。

For meta packages on Linux, see https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#package-manager-metas.

有关 Linux 上的元包，请参阅 https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#package-manager-metas。

## Chapter 2. New Features

## 第 2 章 新特性

This section lists new general CUDA and CUDA compilers features.

本节列出了新的通用 CUDA 和 CUDA 编译器功能。

### 2.1. General CUDA

### 2.1. 通用 CUDA

- The default Linux driver installation changes in this release, preferring NVIDIA GPU Open Kernel Modules to proprietary drivers. The open source drivers are now the default and recommended installation option.

- 此版本中默认的 Linux 驱动程序安装发生了变化，优先选择 NVIDIA GPU 开放内核模块而非专有驱动程序。开源驱动程序现在是默认且推荐的安装选项。

Important: The GPU Open Kernel Modules drivers are only compatible with Turing and newer GPUs. If your GPU is from an older family (Maxwell, Pascal, or Volta) you must continue to use the proprietary drivers.

重要提示：GPU 开放内核模块驱动程序仅与 Turing 及更新的 GPU 兼容。如果您的 GPU 来自较旧的系列（Maxwell、Pascal 或 Volta），则必须继续使用专有驱动程序。

For additional information, refer to this blog post: https://developer.nvidia.com/blog/ nvidia-transitions-fully-towards-open-source-gpu-kernel-modules/.

有关更多信息，请参阅此博客文章：https://developer.nvidia.com/blog/ nvidia-transitions-fully-towards-open-source-gpu-kernel-modules/。

And, for full details, the CUDA Installation Guide for Linux: https://docs.nvidia.com/cuda/ cuda-installation-guide-linux/index.html

有关完整详细信息，请参阅 Linux 的 CUDA 安装指南：https://docs.nvidia.com/cuda/ cuda-installation-guide-linux/index.html

- New nvidia-open meta-packages are available to improve driver installation of NVIDIA Open GPU kernel modules. [4752203]

- 新的 nvidia-open 元包可用于改进 NVIDIA Open GPU 内核模块的驱动程序安装。[4752203]

### 2.2. CUDA Compiler

### 2.2. CUDA 编译器

- For changes to PTX, refer to https://docs.nvidia.com/cuda/parallel-thread-execution/ #ptx-isa-version-8-5.

- 有关 PTX 的更改，请参阅 https://docs.nvidia.com/cuda/parallel-thread-execution/ #ptx-isa-version-8-5。

Latest host compiler Clang-18 support.

最新主机编译器 Clang-18 支持。

Support for Stack Canaries in device code. CUDA compilers can now insert stack canaries in device code. The NVCC flag --device-stack-protector=true enables this feature. Stack canaries make it more difficult to exploit certain types of memory safety bugs involving stack-local variables. The compiler uses heuristics to assess the risk of such a bug in each function. Only those functions which are deemed high-risk make use of a stack canary.

支持设备代码中的堆栈保护。CUDA 编译器现在可以在设备代码中插入堆栈保护。NVCC 标志 --device-stack-protector=true 启用此功能。堆栈保护使得利用涉及堆栈局部变量的某些类型的内存安全漏洞变得更加困难。编译器使用启发式方法来评估每个函数中此类漏洞的风险。只有那些被认为高风险的函数才会使用堆栈保护。

Added a new compiler option - forward-slash-prefix-opts (Windows only).

添加了一个新的编译器选项 - forward-slash-prefix-opts（仅限 Windows）。

If this flag is specified, and forwarding unknown options to host toolchain is enabled (-forward-unknown-opts or -forward-unknown-to-host-linker or -forward-unknown-to-host-compiler), then a command line argument beginning with '/' is forwarded to the host toolchain. For example:

如果指定了此标志，并且启用了将未知选项转发到主机工具链（-forward-unknown-opts 或 -forward-unknown-to-host-linker 或 -forward-unknown-to-host-compiler），则以 '/' 开头的命令行参数将被转发到主机工具链。例如：

nvcc -forward-slash-prefix-opts -forward-unknown-opts /T foo.cu

nvcc -forward-slash-prefix-opts -forward-unknown-opts /T foo.cu

will forward the flag /T to the host compiler and linker. When this flag is not specified, a command line argument beginning with / is treated as an input file. For example, nvcc /T foo.cu will treat /T as an input file, and the Windows API function GetFullPathName(   ) is used to determine the full path name.

将把 /T 标志转发到主机编译器和链接器。如果未指定此标志，则以 / 开头的命令行参数将被视为输入文件。例如，nvcc /T foo.cu 将把 /T 视为输入文件，并使用 Windows API 函数 GetFullPathName(   ) 来确定完整路径名。

Note: This flag is only supported on Windows.

注意：此标志仅在 Windows 上受支持。

For more details, refer to nvcc-help.

有关更多详细信息，请参阅 nvcc-help。

- An environment variable NVCC_CCBIN is introduced for NVCC: Users can set NVCC_CCBIN to specify the host compiler, but it has lower priority than command-line option -ccbin. If NVCC_CCBIN and -ccbin are both set, NVCC uses the host compiler specified by -ccbin.

- 为 NVCC 引入了环境变量 NVCC_CCBIN：用户可以设置 NVCC_CCBIN 来指定主机编译器，但其优先级低于命令行选项 -ccbin。如果同时设置了 NVCC_CCBIN 和 -ccbin，NVCC 将使用由 -ccbin 指定的主机编译器。

### 2.3. CUDA Developer Tools

### 2.3. CUDA 开发者工具

- For changes to nvprof and Visual Profiler, see the changelog.

- 有关 nvprof 和 Visual Profiler 的更改，请参阅变更日志。

For new features, improvements, and bug fixes in Nsight Systems, see the changelog.

有关 Nsight Systems 的新功能、改进和错误修复，请参阅变更日志。

For new features, improvements, and bug fixes in Nsight Visual Studio Edition, see the changelog.

有关 Nsight Visual Studio Edition 的新功能、改进和错误修复，请参阅变更日志。

For new features, improvements, and bug fixes in CUPTI, see the changelog.

有关 CUPTI 的新功能、改进和错误修复，请参阅变更日志。

For new features, improvements, and bug fixes in Nsight Compute, see the changelog.

有关 Nsight Compute 的新功能、改进和错误修复，请参阅变更日志。

For new features, improvements, and bug fixes in Compute Sanitizer, see the changelog.

有关 Compute Sanitizer 的新功能、改进和错误修复，请参阅变更日志。

For new features, improvements, and bug fixes in CUDA-GDB, see the changelog.

有关 CUDA-GDB 的新功能、改进和错误修复，请参阅变更日志。

## Chapter 3. Resolved Issues

## 第 3 章 已解决的问题

### 3.1. CUDA Compiler

### 3.1. CUDA 编译器

<!-- Media -->

- NVIDIA has found that under certain rare conditions, the ptxas compiler may incorrectly optimize a CUDA kernel on the sm90 (Hopper) GPU architecture, potentially omitting the abs(   ) operation in sequences of CUDA C++ or PTX assembly that are equivalent to:

- NVIDIA 发现，在某些罕见情况下，ptxas 编译器可能会在 sm90（Hopper）GPU 架构上错误地优化 CUDA 内核，可能会在等效于以下 CUDA C++ 或 PTX 汇编序列中省略 abs(   ) 操作：

---

int a, a1, b, b1, c;

a1 = abs(a);

		c = 0;

			// signed integer

			// and, or b1 = abs(b);

// 'c' can be proven to be zero at compile time

---

result = max(max(a1, b1), c); // or result = max(min(a1, b1), c);

result = max(max(a1, b1), c); // 或 result = max(min(a1, b1), c);

<!-- Media -->

An equivalent problematic sequence is:

一个等效的问题序列是：

__vimin_s32_relu(abs(a), b)

__vimin_s32_relu(abs(a), b)

To workaround this issue, users can either:

为了解决这个问题，用户可以：

- Update the ptxas compiler to the current release (12.6.3), or

- 将 ptxas 编译器更新到当前版本（12.6.3），或者

- Compile the kernel at ptxas -00 (or "nvcc -Xptxas -00"), or

- 使用 ptxas -00（或 "nvcc -Xptxas -00"）编译内核，或者

- Inject inline PTX asm "min.s32 a1, a1, 0x7fffffff" before the inner max/min operation. For the above example:

- 在内部的最大/最小操作之前插入内联 PTX 汇编 "min.s32 a1, a1, 0x7fffffff"。对于上述示例：

---

1 = abs(a);

c = 0;

asm volatile(   )

	"min.s32 %0, %1, 0x7fffffff;\\n"

	: "=r"(a1) : "r"(a1)

)；

result = max(max(a1, b1), c);

---

This issue has been addressed in the current CUDA toolkit release.

此问题已在当前的CUDA工具包版本中得到解决。

- Added NVCC_CCBIN environment variable to allow system admins to globally specify the host compiler.

- 添加了NVCC_CCBIN环境变量，允许系统管理员全局指定主机编译器。

If NVCC_CCBIN is set by a system admin and -ccbin is set by a user, nvcc will choose the host compiler specified by -ccbin. If NVCC_CCBIN is set and -ccbin is not set, nvcc will choose the host compiler specified by NVCC_CCBIN. If neither of them are set, nvcc will use the default compiler.

如果NVCC_CCBIN由系统管理员设置且-ccbin由用户设置，nvcc将选择由-ccbin指定的主机编译器。如果NVCC_CCBIN已设置且-ccbin未设置，nvcc将选择由NVCC_CCBIN指定的主机编译器。如果两者均未设置，nvcc将使用默认编译器。

For more details, refer to nvcc-help. Release Notes, Release 12.6

更多详情，请参阅nvcc-help。发布说明，版本12.6

# Chapter 4. Known Issues and Limitations

# 第四章 已知问题和限制

- There is a possibility of a hang happening when invoking a CUDA Dynamic Parallelism (CDP) tail launch from within a graph launch. [4718251]

- 在图形启动中调用CUDA动态并行（CDP）尾部启动时，可能会出现挂起的情况。[4718251]

To upgrade using the cuda metapackage: [4752050]

要使用 cuda 元包进行升级：[4752050]

- On Ubuntu 20.04, first switch to open kernel modules:

- 在 Ubuntu 20.04 上，首先切换到开放内核模块：

---

\$ sudo apt-get install -V nvidia-kernel-source-open

	\$ sudo apt-get install nvidia-open

---

On dnf-based distros, module streams must be disabled:

在基于 dnf 的发行版上，必须禁用模块流：

---

	\$ echo "module_hotfixes=1" | tee -a /etc/yum.repos.d/cuda*.repo

\$ sudo dnf install --allowerasing nvidia-open

\$ sudo dnf module reset nvidia-driver

---

- On Azure Linux, to load NVIDIA kernel modules, the kernel_lockdown boot parameter must be disabled by removing lockdown=integrity from the GRUB bootloader entry. [4721469]

- 在 Azure Linux 上，要加载 NVIDIA 内核模块，必须通过从 GRUB 引导加载程序条目中删除 lockdown=integrity 来禁用 kernel_lockdown 引导参数。[4721469]

- When installing Arm SBSA drivers on SLES 15.6, for installation to complete correctly the system must be rebooted immediately. This will allow modprobe to set permissions for /dev/nvidia* device nodes correctly. [4775942]

- 在 SLES 15.6 上安装 Arm SBSA 驱动程序时，为了正确完成安装，必须立即重新启动系统。这将允许 modprobe 正确设置 /dev/nvidia* 设备节点的权限。[4775942]

- If this is not done, and nvidia-smi is run as root, device nodes may be created with incorrect permissions. If this happens, it can be fixed with:

- 如果不这样做，并且以 root 身份运行 nvidia-smi，设备节点可能会以错误的权限创建。如果发生这种情况，可以通过以下方式修复：

---

\$ sudo chown -R :video /dev/nvidia*

---

- Users may experience build failures with the error LNK2001: unresolved external symbol guard_check_icall\$fo\$ when using the recently released Windows SDK 10.0.26100 (May 2024). This issue affects projects(including CUDA samples) built with Visual Studio 2019 and toolset v142. And users can fix this issue by below workarounds before Microsoft provides an official solution. [4783292]

- 用户在使用最近发布的 Windows SDK 10.0.26100（2024 年 5 月）时，可能会遇到构建失败，错误为 LNK2001: unresolved external symbol guard_check_icall\$fo\$。此问题影响使用 Visual Studio 2019 和工具集 v142 构建的项目（包括 CUDA 示例）。在 Microsoft 提供官方解决方案之前，用户可以通过以下解决方法修复此问题。[4783292]

## Workarounds:

## 解决方法：

- Use Visual Studio 2022 with toolset v143;

- 使用 Visual Studio 2022 和工具集 v143；

- Select previous Windows SDK version when building with Visual Studio 2019 and toolset v142. Release Notes, Release 12.6

- 在使用 Visual Studio 2019 和工具集 v142 构建时，选择之前的 Windows SDK 版本。发布说明，版本 12.6

## Chapter 5. Deprecated or Dropped Features

## 第 5 章. 已弃用或删除的功能

Features deprecated in the current release of the CUDA software still work in the current release, but their documentation may have been removed, and they will become officially unsupported in a future release. We recommend that developers employ alternative solutions to these features in their software.

在当前版本的 CUDA 软件中已弃用的功能在当前版本中仍然有效，但其文档可能已被删除，并且在未来的版本中将不再受官方支持。我们建议开发人员在其软件中使用这些功能的替代解决方案。

### 5.1. Deprecated or Dropped Operating Systems

### 5.1. 已弃用或停止支持的操作系统

Support for Microsoft Windows 10 21H2 is dropped in 12.6.

在 12.6 版本中停止了对 Microsoft Windows 10 21H2 的支持。

Support for Microsoft Windows 10 21H2 (SV1) is deprecated.

对 Microsoft Windows 10 21H2 (SV1) 的支持已被弃用。

Support for Debian 11.9 is deprecated.

对 Debian 11.9 的支持已被弃用。

### 5.2. Deprecated Toolchains

### 5.2. 已弃用的工具链

CUDA Toolkit 12.6 deprecated support for the following host compilers:

CUDA Toolkit 12.6 弃用了对以下主机编译器的支持：

Microsoft Visual C/C++ (MSVC) 2017

Microsoft Visual C/C++ (MSVC) 2017

All GCC versions prior to GCC 7.3

所有 GCC 7.3 之前的 GCC 版本

### 5.3. CUDA Tools

### 5.3. CUDA 工具

---

Support for the macOS host client of CUDA-GDB is deprecated. It will be dropped in an upcoming

	release.

---

Release Notes, Release 12.6

发布说明，版本 12.6

## Chapter 6. CUDA Libraries

## 第6章. CUDA 库

This section covers CUDA Libraries release notes for 12.x releases.

本节涵盖 12.x 版本的 CUDA 库发布说明。

CUDA Math Libraries toolchain uses C++11 features, and a C++11-compatible standard library (libstdc++ >= 20150422) is required on the host.

CUDA 数学库工具链使用 C++11 特性，主机上需要兼容 C++11 的标准库（libstdc++ >= 20150422）。

### 6.1. cuBLAS Library

### 6.1. cuBLAS 库

#### 6.1.1. cuBLAS: Release 12.6 Update 3

#### 6.1.1. cuBLAS: 12.6 更新 3

## Resolved Issues

## 已解决的问题

- The cuBLASLt library increased stack memory usage by up to 320 KiB which could result in application termination if it exceeded the OS defined limit. [4938719]

- cuBLASLt 库增加了高达 320 KiB 的堆栈内存使用量，如果超过操作系统定义的限制，可能会导致应用程序终止。[4938719]

- A memory leak could occur with cublasLtMatmul when running FP8, FP16 or BF16 Matmul on Hopper GPUs. The memory leak occurred only for algorithms with CUBLASLT_ALGO_CONFIG_ID equal to 66. [4937170]

- 在 Hopper GPU 上运行 FP8、FP16 或 BF16 Matmul 时，cublasLtMatmul 可能会出现内存泄漏。内存泄漏仅发生在 CUBLASLT_ALGO_CONFIG_ID 等于 66 的算法中。[4937170]

- When running FP8 computations on Hopper GPUs, cublasLtMatmul could incorrectly compute the maximum of absolute values of the output matrix (CUBLASLT_MATMUL_DESC_AMAX_D_POINTER). The issue was observed only for algorithms with CUBLASLT_ALGO_CONFIG_ID equal to 66. [4941052, CUB-7595]

- 在 Hopper GPU 上运行 FP8 计算时，cublasLtMatmul 可能会错误地计算输出矩阵的绝对值的最大值（CUBLASLT_MATMUL_DESC_AMAX_D_POINTER）。该问题仅在 CUBLASLT_ALGO_CONFIG_ID 等于 66 的算法中观察到。[4941052, CUB-7595]

- When running FP8 computations on Hopper GPUs, cublasLtMatmul might have ignored CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_AMAX_POINTER. The issue was observed only for algorithms with CUBLASLT_ALGO_CONFIG_ID equal to 66. [CUB-7596]

- 在 Hopper GPU 上运行 FP8 计算时，cublasLtMatmul 可能忽略了 CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_AMAX_POINTER。该问题仅在 CUBLASLT_ALGO_CONFIG_ID 等于 66 的算法中观察到。[CUB-7596]

- When running cublasLtMatmul with algorithms (cublasLtMatmulAlgo_t) that have CUBLASLT_ALGO_CONFIG_ID equal to 66, alignment checks on the contiguous dimension of matrix D may have been omitted. [CUB-7612]

- 当使用 CUBLASLT_ALGO_CONFIG_ID 等于 66 的算法（cublasLtMatmulAlgo_t）运行 cublasLtMatmul 时，可能会忽略矩阵 D 的连续维度上的对齐检查。[CUB-7612]

#### 6.1.2. cuBLAS: Release 12.6 Update 2

#### 6.1.2. cuBLAS: 版本 12.6 更新 2

## New Features

## 新特性

Broad performance improvement on all Hopper GPUs for FP8, FP16 and BF16 matmuls. This improvement also includes the following fused epilogues CUBLASLT_EPILOGUE_BIAS, CUBLASLT_EPILOGUE_RELU, CUBLASLT_EPILOGUE_RELU_BIAS, CUBLASLT_EPILOGUE_RELU_AUX, CUBLASLT_EPILOGUE_RELU_AUX_BIAS, CUBLASLT_EPILOGUE_GELU, and CUBLASLT_EPILOGUE_GELU_BIAS.

在所有 Hopper GPU 上对 FP8、FP16 和 BF16 矩阵乘法进行了广泛的性能改进。此改进还包括以下融合的尾声：CUBLASLT_EPILOGUE_BIAS、CUBLASLT_EPILOGUE_RELU、CUBLASLT_EPILOGUE_RELU_BIAS、CUBLASLT_EPILOGUE_RELU_AUX、CUBLASLT_EPILOGUE_RELU_AUX_BIAS、CUBLASLT_EPILOGUE_GELU 和 CUBLASLT_EPILOGUE_GELU_BIAS。

## $>$ Known Issues

## $>$ 已知问题

S cuBLAS in multi context scenarios may hang with R535 Driver for version below <535.91. [CUB-7024]

在多上下文场景中，S cuBLAS 在低于 535.91 版本的 R535 驱动下可能会挂起。[CUB-7024]

- Users may observe suboptimal performance on Hopper GPUs for FP64 GEMMs. A potential workaround is to conditionally turn on swizzling. To do this, users can take the algo returned via cublasLtMatmulAlgoGetHeuristic and query if swizzling can be enabled by calling cublasLtMatmulAlgoCapGetAttribute with CUBLASLT_ALGO_CAP_CTA_SWIZZLING_SUPPORT. If swizzling is supported, you can enable swizzling by calling cublasLtMatmulAlgoConfigSetAttribute with CUBLASLT_ALGO_CONFIG_CTA_SWIZZLING.[4872420]

- 用户可能会在 Hopper GPU 上观察到 FP64 GEMM 的性能不佳。一个潜在的解决方法是条件性地开启 swizzling。为此，用户可以通过 cublasLtMatmulAlgoGetHeuristic 返回的算法，并通过调用 cublasLtMatmulAlgoCapGetAttribute 与 CUBLASLT_ALGO_CAP_CTA_SWIZZLING_SUPPORT 查询是否可以启用 swizzling。如果支持 swizzling，则可以通过调用 cublasLtMatmulAlgoConfigSetAttribute 与 CUBLASLT_ALGO_CONFIG_CTA_SWIZZLING 来启用 swizzling。[4872420]

- The cuBLASLt library increased stack memory usage by up to 320 KiB which can result in application termination if it exceeded the OS defined limit. [4938719]

- cuBLASLt 库的堆栈内存使用量增加了最多 320 KiB，如果超过操作系统定义的限制，可能会导致应用程序终止。[4938719]

- A memory leak can occur with cublasLtMatmul when running FP8, FP16 or BF16 Mat-mul on Hopper GPUs. The memory leak is proportional to the number of different FP8, FP16, and BF16 kernels that cublasLtMatmul uses. It is not proportional to the number of times cublasLtMatmul is called. The memory leak occurs only for algorithms with CUBLASLT_ALGO_CONFIG_ID equal to 66. [4937170]

- 在 Hopper GPU 上运行 FP8、FP16 或 BF16 矩阵乘法时，cublasLtMatmul 可能会出现内存泄漏。内存泄漏与 cublasLtMatmul 使用的不同 FP8、FP16 和 BF16 内核的数量成正比，而与 cublasLtMatmul 的调用次数无关。内存泄漏仅发生在 CUBLASLT_ALGO_CONFIG_ID 等于 66 的算法中。[4937170]

- When running FP8 computations on Hopper GPUs, cublasLtMatmul can incorrectly compute the maximum of absolute values of the output matrix (CUBLASLT_MATMUL_DESC_AMAX_D_POINTER). The issue is observed only for algorithms with CUBLASLT_ALGO_CONFIG_ID equal to 66. [4941052, CUB-7595]

- 在 Hopper GPU 上运行 FP8 计算时，cublasLtMatmul 可能会错误地计算输出矩阵的绝对值的最大值（CUBLASLT_MATMUL_DESC_AMAX_D_POINTER）。该问题仅在 CUBLASLT_ALGO_CONFIG_ID 等于 66 的算法中观察到。[4941052, CUB-7595]

- When running FP8 computations on Hopper GPUs, cublasLtMatmul might ignore CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_AMAX_POINTER. The issue is observed only for algorithms with CUBLASLT_ALGO_CONFIG_ID equal to 66. [CUB-7596]

- 在 Hopper GPU 上运行 FP8 计算时，cublasLtMatmul 可能会忽略 CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_AMAX_POINTER。此问题仅在 CUBLASLT_ALGO_CONFIG_ID 等于 66 的算法中观察到。[CUB-7596]

- When running cublasLtMatmul with algorithms (cublasLtMatmulAlgo_t) that have CUBLASLT_ALGO_CONFIG_ID equal to 66, alignment checks on the contiguous dimension of matrix D may be omitted. This occurs when the cublasLtMatmulAlgo_t is reused from heuristics for different input shapes. The alignment requirements are listed in Tensor Core Usage. [CUB-7612]

- 当使用 CUBLASLT_ALGO_CONFIG_ID 等于 66 的算法（cublasLtMatmulAlgo_t）运行 cublasLtMatmul 时，可能会忽略矩阵 D 的连续维度上的对齐检查。当从启发式方法中重用 cublasLtMatmulAlgo_t 以处理不同的输入形状时，会发生这种情况。对齐要求在 Tensor Core 使用中列出。[CUB-7612]

## Resolved Issues

## 已解决的问题

cublasLtMatmul could ignore the user specified Bias or Aux data types (CUBLASLT_MATMUL_DESC_BIAS_DATA_TYPE and CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_DATA_TYPE) for FP8 matmul operations if these data types do not match the documented limitations in cublasLtMatmulDescAt-tributes_t. [44750343, 4801528]

cublasLtMatmul 可能会忽略用户指定的 Bias 或 Aux 数据类型（CUBLASLT_MATMUL_DESC_BIAS_DATA_TYPE 和 CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_DATA_TYPE），如果这些数据类型与 cublasLtMatmulDescAttributes_t 中记录的局限性不匹配。[44750343, 4801528]

- Setting CUDA_MODULE_LOADING to EAGER could lead to longer library load times on Hopper GPUs due to JIT compilation of PTX kernels. This can be mitigated by setting this environment variable to LAZY. [4720601]

- 将 CUDA_MODULE_LOADING 设置为 EAGER 可能会导致 Hopper GPU 上的库加载时间变长，这是由于 PTX 内核的 JIT 编译。可以通过将此环境变量设置为 LAZY 来缓解此问题。[4720601]

- cublasLtMatmul with INT8 inputs, INT32 accumulation, INT8 outputs, and FP32 scaling factors could have produced numerical inaccuracies when a splitk reduction was used. [4751576]

- 使用 INT8 输入、INT32 累加、INT8 输出和 FP32 缩放因子的 cublasLtMatmul 在使用 splitk 归约时可能会产生数值不准确。[4751576]

#### 6.1.3. cuBLAS: Release 12.6 Update 1

#### 6.1.3. cuBLAS: 版本 12.6 更新 1

## Known Issues

## 已知问题

cublasLtMatmul could ignore the user specified Bias or Aux data types (CUBLASLT_MATMUL_DESC_BIAS_DATA_TYPE and CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_DATA_TYPE) for FP8 matmul operations if these data types do not match the documented limitations in cublasLtMatmulDescAt-tributes_t. [4750343]

cublasLtMatmul 可能会忽略用户指定的 Bias 或 Aux 数据类型（CUBLASLT_MATMUL_DESC_BIAS_DATA_TYPE 和 CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_DATA_TYPE），如果这些数据类型与 cublasLtMatmulDescAttributes_t 中记录的 FP8 矩阵乘法操作的限制不匹配。[4750343]

- Setting CUDA_MODULE_LOADING to EAGER could lead to longer library load times on Hopper GPUs due to JIT compilation of PTX kernels. This can be mitigated by setting this environment variable to LAZY. [4720601]

- 将 CUDA_MODULE_LOADING 设置为 EAGER 可能会导致 Hopper GPU 上的库加载时间变长，这是由于 PTX 内核的 JIT 编译。可以通过将此环境变量设置为 LAZY 来缓解此问题。[4720601]

- cublasLtMatmul with INT8 inputs, INT32 accumulation, INT8 outputs, and FP32 scaling factors may produce accuracy issues when a splitk reduction is used. To workaround this issue, you can use cublasLtMatmulAlgoConfigSetAttribute to set the reduction scheme to none and set the splitk value to 1. [4751576]

- 使用 INT8 输入、INT32 累加、INT8 输出和 FP32 缩放因子的 cublasLtMatmul 在使用 splitk 归约时可能会产生精度问题。要解决此问题，可以使用 cublasLtMatmulAlgoConfigSetAttribute 将归约方案设置为 none，并将 splitk 值设置为 1。[4751576]

#### 6.1.4. cuBLAS: Release 12.6

#### 6.1.4. cuBLAS: 版本 12.6

## Known Issues

## 已知问题

- Computing matrix multiplication and an epilogue with INT8 inputs, INT8 outputs, and FP32 scaling factors can have numerical errors in cases when a second kernel is used to compute the epilogue. This happens because the first GEMM kernel converts the intermediate result from FP32 into INT8 and stores it for the subsequent epilogue kernel to use. If a value is outside of the range of INT8 before the epilogue and the epilogue would bring it into the range of INT8, there will be numerical errors. This issue has existed since before CUDA 12 and there is no known workaround. [CUB-6831]

- 在使用 INT8 输入、INT8 输出和 FP32 缩放因子计算矩阵乘法和尾声时，如果使用第二个内核来计算尾声，可能会出现数值错误。这是因为第一个 GEMM 内核将中间结果从 FP32 转换为 INT8 并存储以供后续尾声内核使用。如果某个值在尾声之前超出了 INT8 的范围，而尾声会将其带入 INT8 范围内，则会出现数值错误。此问题自 CUDA 12 之前就已存在，目前尚无已知的解决方法。[CUB-6831]

cublasLtMatmul could ignore the user specified Bias or Aux data types (CUBLASLT_MATMUL_DESC_BIAS_DATA_TYPE and CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_DATA_TYPE) for FP8 matmul operations if these data types do not match the documented limitations in cublasLtMatmulDescAt-tributes_t. [4750343]

cublasLtMatmul 可能会忽略用户指定的 Bias 或 Aux 数据类型（CUBLASLT_MATMUL_DESC_BIAS_DATA_TYPE 和 CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_DATA_TYPE），如果这些数据类型与 cublasLtMatmulDescAttributes_t 中记录的局限性不匹配。[4750343]

## Resolved Issues

## 已解决问题

- cublasLtMatmul produced incorrect results when data types of matrices A and B were different FP8 (for example, A is CUDA_R_8F_E4M3 and B is CUDA_R_8F_E5M2) and matrix D layout was CUBLASLT_ORDER_ROW. [4640468]

- 当矩阵 A 和 B 的数据类型为不同的 FP8（例如，A 是 CUDA_R_8F_E4M3，B 是 CUDA_R_8F_E5M2）且矩阵 D 的布局为 CUBLASLT_ORDER_ROW 时，cublasLtMatmul 产生了不正确的结果。[4640468]

- cublasLt may return not supported on Hopper GPUs in some cases when A, B, and C are of type CUDA_R_8I and the compute type is CUBLAS_COMPUTE_32I. [4381102]

- 在某些情况下，当 A、B 和 C 的类型为 CUDA_R_8I 且计算类型为 CUBLAS_COMPUTE_32I 时，cublasLt 可能会在 Hopper GPU 上返回不支持。[4381102]

- cuBLAS could produce floating point exceptions when running GEMM with K equal to 0. [4614629]

- cuBLAS 在运行 K 等于 0 的 GEMM 时可能会产生浮点异常。[4614629]

#### 6.1.5. cuBLAS: Release 12.5 Update 1

#### 6.1.5. cuBLAS: 12.5 更新 1

## New Features

## 新特性

- Performance improvement to matrix multiplication targeting large language models, specifically for small batch sizes on Hopper GPUs.

- 针对大型语言模型的矩阵乘法性能改进，特别是针对 Hopper GPU 上的小批量大小。

## Known Issues

## 已知问题

- The bias epilogue (without ReLU or GeLU) may be not supported on Hopper GPUs for strided batch cases. A workaround is to implement batching manually. This will be fixed in a future release.

- 在 Hopper GPU 上，对于跨步批处理情况，可能不支持偏置尾声（不带 ReLU 或 GeLU）。解决方法是手动实现批处理。此问题将在未来的版本中修复。

- cublasGemmGroupedBatchedEx and cublas<t>gemmGroupedBatched have large CPU overheads. This will be addressed in an upcoming release.

- cublasGemmGroupedBatchedEx 和 cublas<t>gemmGroupedBatched 有较大的 CPU 开销。这将在即将发布的版本中得到解决。

## Resolved Issues

## 已解决的问题

- Under rare circumstances, executing SYMM/HEMM concurrently with GEMM on Hopper GPUs might have caused race conditions in the host code, which could lead to an Illegal Memory Access CUDA error. [4403010]

- 在极少数情况下，在 Hopper GPU 上同时执行 SYMM/HEMM 和 GEMM 可能会导致主机代码中的竞争条件，从而可能导致非法内存访问 CUDA 错误。[4403010]

- cublasLtMatmul could produce an Illegal Instruction CUDA error on Pascal GPUs under the following conditions: batch is greater than 1, and beta is not equal to 0 , and the computations are out-of-place (C != D). [4566993]

- 在以下条件下，cublasLtMatmul 可能会在 Pascal GPU 上产生非法指令 CUDA 错误：批次大于 1，且 beta 不等于 0，并且计算是异地进行的（C != D）。[4566993]

#### 6.1.6. cuBLAS: Release 12.5

#### 6.1.6. cuBLAS: 版本 12.5

## New Features

## 新特性

- cuBLAS adds an experimental API to support mixed precision grouped batched GEMMs. This enables grouped batched GEMMs with FP16 or BF16 inputs/outputs with the FP32 compute type. Refer to cublasGemmGroupedBatchedExfor more details.

- cuBLAS 添加了一个实验性 API 以支持混合精度的分组批量 GEMM。这使得可以使用 FP16 或 BF16 输入/输出以及 FP32 计算类型的分组批量 GEMM。有关更多详细信息，请参阅 cublasGemmGroupedBatchedEx。

## Known Issues

## 已知问题

- cublasLtMatmul ignores inputs to CUBLASLT_MATMUL_DESC_D_SCALE_POINTER and CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_SCALE_POINTER if the elements of the respective matrix are not of FP8 types.

- cublasLtMatmul 忽略了对 CUBLASLT_MATMUL_DESC_D_SCALE_POINTER 和 CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_SCALE_POINTER 的输入，如果相应矩阵的元素不是 FP8 类型。

## Resolved Issues

## 已解决问题

- cublasLtMatmul ignored the mismatch between the provided scale type and the implied by the documentation, assuming the latter. For instance, an unsupported configuration of cublasLtMatmul with the scale type being FP32 and all other types being FP16 would run with the implicit assumption that the scale type is FP16 and produce incorrect results.

- cublasLtMatmul 忽略了提供的缩放类型与文档中隐含的缩放类型之间的不匹配，假设后者。例如，cublasLtMatmul 的一个不支持的配置，缩放类型为 FP32 而所有其他类型为 FP16，将运行并假设缩放类型为 FP16，从而产生不正确的结果。

cuBLAS SYMV failed for large n dimension: 131072 and above for ssymv, 92673 and above for csymv and dsymv, and 65536 and above for zsymv.

cuBLAS SYMV 对于大 n 维度失败：对于 ssymv 为 131072 及以上，对于 csymv 和 dsymv 为 92673 及以上，对于 zsymv 为 65536 及以上。

#### 6.1.7. cuBLAS: Release 12.4 Update 1

#### 6.1.7. cuBLAS: 版本 12.4 更新 1

## Known Issues

## 已知问题

- Setting a cuBLAS handle stream to cudaStreamPerThread and setting the workspace via cublasSetWorkspace will cause any subsequent cublasSetWorkspace calls to fail. This will be fixed in an upcoming release.

- 将 cuBLAS 句柄流设置为 cudaStreamPerThread 并通过 cublasSetWorkspace 设置工作空间将导致任何后续的 cublasSetWorkspace 调用失败。此问题将在未来的版本中修复。

cublasLtMatmulignores mismatches between the provided scale type and the scale type implied by the documentation and assumes the latter. For example, an unsupported configuration of cublasLtMatmul with the scale type being FP32 and all other types being FP16 would run with the implicit assumption that the scale type is FP16 which can produce incorrect results. This will be fixed in an upcoming release.

cublasLtMatmul 忽略提供的缩放类型与文档中隐含的缩放类型之间的不匹配，并假设后者。例如，cublasLtMatmul 的配置不支持缩放类型为 FP32 而所有其他类型为 FP16 的情况，它将隐式假设缩放类型为 FP16，这可能导致错误的结果。此问题将在未来的版本中修复。

## Resolved Issues

## 已解决的问题

- cublasLtMatmul ignored the CUBLASLT_MATMUL_DESC_AMAX_D_POINTER for unsupported configurations instead of returning an error. In particular, computing absolute maximum of D is currently supported only for FP8 Matmul when the output data type is also FP8 (CUDA_R_8F_E4M3 or CUDA_R_8F_E5M2).

- cublasLtMatmul 在不受支持的配置中忽略了 CUBLASLT_MATMUL_DESC_AMAX_D_POINTER，而不是返回错误。特别是，计算 D 的绝对最大值目前仅在输出数据类型也是 FP8（CUDA_R_8F_E4M3 或 CUDA_R_8F_E5M2）时支持 FP8 矩阵乘法。

PReduced host-side overheads for some of the cuBLASLt APIs: cublasLtMatmul(   ), cublasLtMatmulAlgoCheck(   ), and cublasLtMatmulAlgoGetHeuristic(   ). The issue was introduced in CUDA Toolkit 12.4.

减少了一些 cuBLASLt API 的主机端开销：cublasLtMatmul(   )、cublasLtMatmulAlgoCheck(   ) 和 cublasLtMatmulAlgoGetHeuristic(   )。该问题在 CUDA Toolkit 12.4 中引入。

- cublasLtMatmul(   ) and cublasLtMatmulAlgoGetHeuristic(   ) could have resulted in floating point exceptions (FPE) on some Hopper-based GPUs, including Multi-Instance GPU (MIG). The issue was introduced in cuBLAS 11.8.

- cublasLtMatmul(   ) 和 cublasLtMatmulAlgoGetHeuristic(   ) 在某些基于 Hopper 的 GPU 上可能导致浮点异常 (FPE)，包括多实例 GPU (MIG)。该问题在 cuBLAS 11.8 中引入。

#### 6.1.8. cuBLAS: Release 12.4

#### 6.1.8. cuBLAS: 版本 12.4

## New Features

## 新特性

P cuBLAS adds experimental APIs to support grouped batched GEMM for single precision and double precision. Single precision also supports the math mode, CUBLAS_TF32_TENSOR_OP_MATH. Grouped batch mode allows you to concurrently solve GEMMs of different dimensions (m, n, k), leading dimensions (Ida, Idb, Idc), transpositions (transa, transb), and scaling factors (alpha, beta). Please see gemmGroupedBatched for more details.

P cuBLAS 添加了实验性 API 以支持单精度和双精度的分组批处理 GEMM。单精度还支持数学模式 CUBLAS_TF32_TENSOR_OP_MATH。分组批处理模式允许您同时解决不同维度 (m, n, k)、前导维度 (Ida, Idb, Idc)、转置 (transa, transb) 和缩放因子 (alpha, beta) 的 GEMM。详情请参阅 gemmGroupedBatched。

## Known Issues

## 已知问题

- When the current context has been created using cuGreenCtxCreate(   ), cuBLAS does not properly detect the number of SMs available. The user may provide the corrected SM count to cuBLAS using an API such as cublasSetSmCountTarget (   ).

- 当使用 cuGreenCtxCreate() 创建当前上下文时，cuBLAS 无法正确检测可用的 SM 数量。用户可以通过 cublasSetSmCountTarget() 等 API 向 cuBLAS 提供正确的 SM 计数。

- BLAS level 2 and 3 functions might not treat alpha in a BLAS compliant manner when alpha is zero and the pointer mode is set to CUBLAS_POINTER_MODE_DEVICE. This is the same known issue documented in cuBLAS 12.3 Update 1.

- 当 alpha 为零且指针模式设置为 CUBLAS_POINTER_MODE_DEVICE 时，BLAS 级别 2 和 3 的函数可能不会以符合 BLAS 的方式处理 alpha。这与 cuBLAS 12.3 更新 1 中记录的已知问题相同。

- cublasLtMatmul with K equals 1 and epilogue CUBLASLT_EPILOGUE_D\{RELU, GELU\}_BGRAD could out-of-bound access the workspace. The issue exists since cuBLAS 11.3 Update 1.

- 当 K 等于 1 且 epilogue 为 CUBLASLT_EPILOGUE_D\{RELU, GELU\}_BGRAD 时，cublasLtMatmul 可能会越界访问工作空间。该问题自 cuBLAS 11.3 更新 1 以来一直存在。

- cublasLtMatmul with K equals 1 and epilogue CUBLASLT_EPILOGUE_D\{RELU,GELU\} could produce illegal memory access if no workspace is provided. The issue exists since cuBLAS 11.6.

- 当 K 等于 1 且 epilogue 为 CUBLASLT_EPILOGUE_D\{RELU,GELU\} 时，如果未提供工作空间，cublasLtMatmul 可能会产生非法内存访问。该问题自 cuBLAS 11.6 以来一直存在。

- When captured in CUDA Graph stream capture, cuBLAS routines can create memory nodes through the use of stream-ordered allocation APIs, cudaMallocAsync and cud-aFreeAsync. However, as there is currently no support for memory nodes in child graphs or graphs launched from the device, attempts to capture cuBLAS routines in such scenarios may fail. To avoid this issue, use the cublasSetWorkspace(   ) function to provide user-owned workspace memory.

- 在 CUDA Graph 流捕获中捕获时，cuBLAS 例程可以通过使用流顺序分配 API（如 cudaMallocAsync 和 cudaFreeAsync）创建内存节点。然而，由于目前不支持子图或从设备启动的图中的内存节点，尝试在此类场景中捕获 cuBLAS 例程可能会失败。为避免此问题，请使用 cublasSetWorkspace() 函数提供用户拥有的工作空间内存。

#### 6.1.9. cuBLAS: Release 12.3 Update 1

#### 6.1.9. cuBLAS: 版本 12.3 更新 1

## New Features

## 新特性

- Improved performance of heuristics cache for workloads that have a high eviction rate.

- 改进了具有高淘汰率工作负载的启发式缓存性能。

## Known Issues

## 已知问题

- BLAS level 2 and 3 functions might not treat alpha in a BLAS compliant manner when alpha is zero and the pointer mode is set to CUBLAS_POINTER_MODE_DEVICE. The expected behavior is that the corresponding computations would be skipped. You may encounter the following issues: (1) HER\{,2,X,K,2K\} may zero the imaginary part on the diagonal elements of the output matrix; and (2) HER\{,2,X,K,2K\}, SYR\{,2,X,K,2K\} and others may produce NaN resulting from performing computation on matrices A and B which would otherwise be skipped. If strict compliance with BLAS is required, the user may manually check for alpha value before invoking the functions or switch to CUBLAS_POINTER_MODE_HOST.

- 当 alpha 为零且指针模式设置为 CUBLAS_POINTER_MODE_DEVICE 时，BLAS 级别 2 和 3 函数可能不会以符合 BLAS 的方式处理 alpha。预期行为是跳过相应的计算。您可能会遇到以下问题：(1) HER\{,2,X,K,2K\} 可能会将输出矩阵对角线元素的虚部设为零；(2) HER\{,2,X,K,2K\}、SYR\{,2,X,K,2K\} 和其他函数可能会在矩阵 A 和 B 上执行计算时产生 NaN，而这些计算本应被跳过。如果需要严格符合 BLAS，用户可以在调用函数之前手动检查 alpha 值或切换到 CUBLAS_POINTER_MODE_HOST。

## Resolved Issues

## 已解决问题

cuBLASLt matmul operations might have computed the output incorrectly under the following conditions: the data type of matrices A and B is FP8, the data type of matrices C and D is FP32, FP16, or BF16, the beta value is 1.0, the C and D matrices are the same, the epilogue contains GELU activation function.

cuBLASLt 矩阵乘法操作在以下条件下可能错误地计算了输出：矩阵 A 和 B 的数据类型为 FP8，矩阵 C 和 D 的数据类型为 FP32、FP16 或 BF16，beta 值为 1.0，矩阵 C 和 D 相同，且 epilogue 包含 GELU 激活函数。

- When an application compiled with cuBLASLt from CUDA Toolkit 12.2 update 1 or earlier runs with cuBLASLt from CUDA Toolkit 12.2 update 2 or CUDA Toolkit 12.3, matrix multiply descriptors initialized using cublasLtMatmulDescInit(   ) sometimes did not respect attribute changes using cublasLtMatmulDescSetAttribute(   ).

- 当使用 CUDA Toolkit 12.2 更新 1 或更早版本编译的应用程序与 CUDA Toolkit 12.2 更新 2 或 CUDA Toolkit 12.3 中的 cuBLASLt 运行时，使用 cublasLtMatmulDescInit() 初始化的矩阵乘法描述符有时不会尊重使用 cublasLtMatmulDescSetAttribute() 进行的属性更改。

- Fixed creation of cuBLAS or cuBLASLt handles on Hopper GPUs under the Multi-Process Service (MPS).

- 修复了在 Hopper GPU 上使用多进程服务 (MPS) 创建 cuBLAS 或 cuBLASLt 句柄的问题。

: cublasLtMatmul with K equals 1 and epilogue CUBLASLT_EPILOGUE_BGRAD $\{ A,B\}$ might have returned incorrect results for the bias gradient.

: 当 K 等于 1 且 epilogue 为 CUBLASLT_EPILOGUE_BGRAD $\{ A,B\}$ 时，cublasLtMatmul 可能会返回错误的偏置梯度结果。

#### 6.1.10. cuBLAS: Release 12.3

#### 6.1.10. cuBLAS: 版本 12.3

## New Features

## 新特性

- Improved performance on NVIDIA L40S Ada GPUs.

- 在 NVIDIA L40S Ada GPU 上提高了性能。

## Known Issues

## 已知问题

- cuBLASLt matmul operations may compute the output incorrectly under the following conditions: the data type of matrices A and B is FP8, the data type of matrices C and D is FP32, FP16, or BF16, the beta value is 1.0, the C and D matrices are the same, the epilogue contains GELU activation function.

- 在以下条件下，cuBLASLt 矩阵乘法操作可能会错误地计算输出：矩阵 A 和 B 的数据类型为 FP8，矩阵 C 和 D 的数据类型为 FP32、FP16 或 BF16，beta 值为 1.0，矩阵 C 和 D 相同，且 epilogue 包含 GELU 激活函数。

- When an application compiled with cuBLASLt from CUDA Toolkit 12.2 update 1 or earlier runs with cuBLASLt from CUDA Toolkit 12.2 update 2 or later, matrix multiply descriptors initialized using cublasLtMatmulDescInit(   ) may not respect attribute changes using cublasLtMatmulDescSetAttribute(   ). To workaround this issue, create the matrix multiply descriptor using cublasLtMatmulDescCreate(   ) instead of cublasLtMatmulDe-scInit (   ). This will be fixed in an upcoming release.

- 当使用 CUDA Toolkit 12.2 更新 1 或更早版本编译的应用程序与 CUDA Toolkit 12.2 更新 2 或更高版本的 cuBLASLt 一起运行时，使用 cublasLtMatmulDescInit() 初始化的矩阵乘法描述符可能不会尊重使用 cublasLtMatmulDescSetAttribute() 进行的属性更改。要解决此问题，请使用 cublasLtMatmulDescCreate() 而不是 cublasLtMatmulDescInit() 创建矩阵乘法描述符。此问题将在未来的版本中修复。

#### 6.1.11. cuBLAS: Release 12.2 Update 2

#### 6.1.11. cuBLAS: 12.2 更新 2 版本

## New Features

## 新特性

- cuBLASLt will now attempt to decompose problems that cannot be run by a single gemm kernel. It does this by partitioning the problem into smaller chunks and executing the gemm kernel multiple times. This improves functional coverage for very large $m,n$ ,or batch size cases and makes the transition from the cuBLAS API to the cuBLASLt API more reliable.

- cuBLASLt 现在将尝试分解无法由单个 gemm 内核运行的问题。它通过将问题分割成较小的块并多次执行 gemm 内核来实现这一点。这提高了对于非常大的 $m,n$ 或批量大小情况的功能覆盖范围，并使从 cuBLAS API 到 cuBLASLt API 的转换更加可靠。

## Known Issues

## 已知问题

- cuBLASLt matmul operations may compute the output incorrectly under the following conditions: the data type of matrices A and B is FP8, the data type of matrices C and D is FP32, FP16,or BF16,the beta value is 1.0,the C and D matrices are the same,the epilogue contains GELU activation function.

- 在以下条件下，cuBLASLt 矩阵乘法操作可能会错误地计算输出：矩阵 A 和 B 的数据类型为 FP8，矩阵 C 和 D 的数据类型为 FP32、FP16 或 BF16，beta 值为 1.0，矩阵 C 和 D 相同，且 epilogue 包含 GELU 激活函数。

#### 6.1.12. cuBLAS: Release 12.2

#### 6.1.12. cuBLAS: 版本 12.2

## Known Issues

## 已知问题

- cuBLAS initialization fails on Hopper architecture GPUs when MPS is in use with CUDA_MPS_ACTIVE_THREAD_PERCENTAGE set to a value less than 100%. There is currently no workaround for this issue.

- 当 MPS 在使用且 CUDA_MPS_ACTIVE_THREAD_PERCENTAGE 设置为小于 100% 的值时，cuBLAS 初始化在 Hopper 架构的 GPU 上失败。目前没有解决此问题的方法。

- Some Hopper kernels produce incorrect results for batched matmuls with CUBLASLT_EPILOGUE_RELU_BIAS or CUBLASLT_EPILOGUE_GELU_BIAS and a nonzero CUBLASLT_MATMUL_DESC_BIAS_BATCH_STRIDE. The kernels apply the first batch's bias vector to all batches. This will be fixed in a future release.

- 一些 Hopper 内核在使用 CUBLASLT_EPILOGUE_RELU_BIAS 或 CUBLASLT_EPILOGUE_GELU_BIAS 以及非零 CUBLASLT_MATMUL_DESC_BIAS_BATCH_STRIDE 的批量矩阵乘法时会产生错误结果。这些内核将第一个批次的偏置向量应用于所有批次。此问题将在未来的版本中修复。

#### 6.1.13. cuBLAS: Release 12.1 Update 1

#### 6.1.13. cuBLAS: 12.1 更新 1

## New Features

## 新特性

Support for FP8 on NVIDIA Ada GPUs.

支持 NVIDIA Ada GPU 上的 FP8。

Improved performance on NVIDIA L4 Ada GPUs.

在 NVIDIA L4 Ada GPU 上性能提升。

- Introduced an API that instructs the cuBLASLt library to not use some CPU instructions. This is useful in some rare cases where certain CPU instructions used by cuBLASLt heuristics negatively impact CPU performance. Refer to https://docs.nvidia.com/cuda/cublas/index.html#disabling-cpu-instructions.

- 引入了一个 API，指示 cuBLASLt 库不使用某些 CPU 指令。这在某些罕见情况下非常有用，即 cuBLASLt 启发式算法使用的某些 CPU 指令对 CPU 性能产生负面影响。请参阅 https://docs.nvidia.com/cuda/cublas/index.html#disabling-cpu-instructions。

Known Issues

已知问题

- When creating a matrix layout using the cublasLtMatrixLayoutCreate(   ) function, the object pointed at by cublasLtMatrixLayout_t is smaller than cublasLtMatrixLay-outOpaque_t (but enough to hold the internal structure). As a result, the object should not be dereferenced or copied explicitly, as this might lead to out of bound accesses. If one needs to serialize the layout or copy it, it is recommended to manually allocate an object of size sizeof(cublasLtMatrixLayoutOpaque_t) bytes, and initialize it using cublasLtMatrixLayoutInit(   ) function. The same applies to cublasLt-MatmulDesc_t and cublasLtMatrixTransformDesc_t. The issue will be fixed in future releases by ensuring that cublasLtMatrixLayoutCreate(   ) allocates at least sizeof(cublasLtMatrixLayoutOpaque_t) bytes.

- 当使用 cublasLtMatrixLayoutCreate() 函数创建矩阵布局时，cublasLtMatrixLayout_t 指向的对象比 cublasLtMatrixLayoutOpaque_t 小（但足以容纳内部结构）。因此，不应显式解引用或复制该对象，因为这可能导致越界访问。如果需要序列化布局或复制它，建议手动分配一个大小为 sizeof(cublasLtMatrixLayoutOpaque_t) 字节的对象，并使用 cublasLtMatrixLayoutInit() 函数进行初始化。同样适用于 cublasLtMatmulDesc_t 和 cublasLtMatrixTransformDesc_t。该问题将在未来的版本中通过确保 cublasLtMatrixLayoutCreate() 至少分配 sizeof(cublasLtMatrixLayoutOpaque_t) 字节来解决。

#### 6.1.14. cuBLAS: Release 12.0 Update 1

#### 6.1.14. cuBLAS: 12.0 更新 1

## New Features

## 新特性

- Improved performance on NVIDIA H100 SXM and NVIDIA H100 PCIe GPUs.

- 在 NVIDIA H100 SXM 和 NVIDIA H100 PCIe GPU 上性能提升。

## Known Issues

## 已知问题

- For optimal performance on NVIDIA Hopper architecture, cuBLAS needs to allocate a bigger internal workspace (64 MiB) than on the previous architectures (8 MiB). In the current and previous releases, cuBLAS allocates 256 MiB. This will be addressed in a future release. A possible workaround is to set the CUBLAS_WORKSPACE_CONFIG environment variable to :32768:2 when running cuBLAS on NVIDIA Hopper architecture.

- 为了在 NVIDIA Hopper 架构上获得最佳性能，cuBLAS 需要分配比之前架构（8 MiB）更大的内部工作空间（64 MiB）。在当前和之前的版本中，cuBLAS 分配了 256 MiB。这将在未来的版本中解决。一个可能的解决方法是，在 NVIDIA Hopper 架构上运行 cuBLAS 时，将 CUBLAS_WORKSPACE_CONFIG 环境变量设置为 :32768:2。

## Resolved Issues

## 已解决的问题

- Reduced cuBLAS host-side overheads caused by not using the cublasLt heuristics cache. This began in the CUDA Toolkit 12.0 release.

- 减少了由于未使用 cublasLt 启发式缓存而导致的 cuBLAS 主机端开销。这始于 CUDA Toolkit 12.0 版本。

Added forward compatible single precision complex GEMM that does not require workspace.

添加了不需要工作空间的前向兼容单精度复数 GEMM。

#### 6.1.15. cuBLAS: Release 12.0

#### 6.1.15. cuBLAS: 12.0 版本

## New Features

## 新特性

- cublasLtMatmul now supports FP8 with a non-zero beta.

- cublasLtMatmul 现在支持带有非零 beta 的 FP8。

- Added int64 APIs to enable larger problem sizes; refer to 64-bit integer interface.

- 添加了 int64 API 以支持更大的问题规模；请参阅 64 位整数接口。

Added more Hopper-specific kernels for cublasLtMatmul with epilogues:

为 cublasLtMatmul 添加了更多 Hopper 特定的内核，包括以下 epilogues：

- CUBLASLT_EPILOGUE_BGRAD\{A,B\}

- CUBLASLT_EPILOGUE_BGRAD\{A,B\}

- CUBLASLT_EPILOGUE_\{RELU,GELU\}_AUX

- CUBLASLT_EPILOGUE_\{RELU,GELU\}_AUX

- CUBLASLT_EPILOGUE_D\{RELU,GELU\}

- CUBLASLT_EPILOGUE_D\{RELU,GELU\}

- Improved Hopper performance on arm64-sbsa by adding Hopper kernels that were previously supported only on the x86_64 architecture for Windows and Linux.

- 通过在 arm64-sbsa 上添加之前仅在 Windows 和 Linux 的 x86_64 架构上支持的 Hopper 内核，提高了 Hopper 的性能。

## Known Issues

## 已知问题

- There are no forward compatible kernels for single precision complex gemms that do not require workspace. Support will be added in a later release.

- 目前没有不需要工作空间的单精度复数 gemms 的前向兼容内核。支持将在后续版本中添加。

Resolved Issues

已解决的问题

- Fixed an issue on NVIDIA Ampere architecture and newer GPUs where cublasLtMatmul with epilogue CUBLASLT_EPILOGUE_BGRAD $\{ A,B\}$ and a nontrivial reduction scheme (that is, not CUBLASLT_REDUCTION_SCHEME_NONE) could return incorrect results for the bias gradient.

- 修复了在 NVIDIA Ampere 架构及更新的 GPU 上，使用 cublasLtMatmul 和 epilogue CUBLASLT_EPILOGUE_BGRAD $\{ A,B\}$ 以及非平凡归约方案（即不是 CUBLASLT_REDUCTION_SCHEME_NONE）时，可能返回错误的偏置梯度结果的问题。

- cublasLtMatmul for gemv-like cases (that is, $m$ or $n$ equals 1) might ignore bias with the CUBLASLT_EPILOGUE_RELU_BIAS and CUBLASLT_EPILOGUE_BIAS epilogues.

- 在 gemv 类似情况下（即 $m$ 或 $n$ 等于 1），cublasLtMatmul 可能会在使用 CUBLASLT_EPILOGUE_RELU_BIAS 和 CUBLASLT_EPILOGUE_BIAS epilogues 时忽略偏置。

## Deprecations

## 弃用

- Disallow including cublas .h and cublas_v2 .h in the same translation unit.

- 禁止在同一翻译单元中包含 cublas .h 和 cublas_v2 .h。

- Removed:

- 已移除：

CUBLAS_MATMUL_STAGES_16x80 and CUBLAS_MATMUL_STAGES_64x80 from cublasLtMatmulStages_t. No kernels utilize these stages anymore.

CUBLAS_MATMUL_STAGES_16x80 和 CUBLAS_MATMUL_STAGES_64x80 从 cublasLtMatmulStages_t 中移除。不再有内核使用这些阶段。

- cublasLt3mMode_t, CUBLASLT_MATMUL_PREF_MATH_MODE_MASK, and CUBLASLT_MATMUL_PREF_GAUSSIAN_MODE_MASK from cublasLtMatmulPref-erenceAttributes_t. Instead, use the corresponding flags from cublasLtNumeri-calImplFlags_t.

- cublasLt3mMode_t、CUBLASLT_MATMUL_PREF_MATH_MODE_MASK 和 CUBLASLT_MATMUL_PREF_GAUSSIAN_MODE_MASK 从 cublasLtMatmulPreferenceAttributes_t 中移除。请改用 cublasLtNumericalImplFlags_t 中的相应标志。

- CUBLASLT_MATMUL_PREF_POINTER_MODE_MASK, CUBLASLT_MATMUL_PREF_EPILOGUE_MASK, and CUBLASLT_MATMUL_PREF_SM_COUNT_TARGET from cublasLtMatmulPref-erenceAttributes_t. The corresponding parameters are taken directly from cublasLtMatmulDesc_t.

- CUBLASLT_MATMUL_PREF_POINTER_MODE_MASK、CUBLASLT_MATMUL_PREF_EPILOGUE_MASK 和 CUBLASLT_MATMUL_PREF_SM_COUNT_TARGET 从 cublasLtMatmulPreferenceAttributes_t 中移除。相应的参数直接从 cublasLtMatmulDesc_t 中获取。

CUBLASLT_POINTER_MODE_MASK_NO_FILTERING from cublasLtPointerMode-Mask_t. This mask was only applicable to CUBLASLT_MATMUL_PREF_MATH_MODE_MASK which was removed.

CUBLASLT_POINTER_MODE_MASK_NO_FILTERING 从 cublasLtPointerModeMask_t 中移除。此掩码仅适用于已移除的 CUBLASLT_MATMUL_PREF_MATH_MODE_MASK。

### 6.2. cuFFT Library

### 6.2. cuFFT 库

#### 6.2.1. cuFFT: Release 12.6 Update 2

#### 6.2.1. cuFFT: 12.6 更新 2

## New Features

## 新特性

- Introduced LTO callbacks as a replacement for the deprecated legacy callbacks. LTO callbacks offer: - Additional performance vs. legacy callbacks - Support for callbacks on Windows and on dynamic (shared) libraries See the cuFFT documentation page for more information.

- 引入了 LTO 回调以替代已弃用的旧版回调。LTO 回调提供了：- 相比旧版回调的额外性能 - 支持在 Windows 和动态（共享）库上的回调。更多信息请参阅 cuFFT 文档页面。

## Resolved Issues

## 已解决的问题

- Several issues present in our cuFFT LTO EA preview binary have been addressed. Deprecations

- 我们 cuFFT LTO EA 预览二进制文件中的几个问题已得到解决。弃用

- cuFFT LTO EA, our preview binary for LTO callback support, is deprecated and will be removed in the future.

- cuFFT LTO EA，我们用于 LTO 回调支持的预览二进制文件，已被弃用，并将在未来移除。

#### 6.2.2. cuFFT: Release 12.6

#### 6.2.2. cuFFT: 版本 12.6

## Known Issues

## 已知问题

- FFT of size 1 with istride/ostride > 1 is currently not supported for FP16. There is a known memory issue for this use case in CTK 12.1 or before. A CUFFT_INVALID_SIZE error is thrown in CTK 12.2 or after. [4662222]

- 目前不支持 istride/ostride > 1 的 FP16 大小为 1 的 FFT。在 CTK 12.1 或更早版本中，此用例存在已知的内存问题。在 CTK 12.2 或更高版本中会抛出 CUFFT_INVALID_SIZE 错误。[4662222]

#### 6.2.3. cuFFT: Release 12.5

#### 6.2.3. cuFFT: 版本 12.5

## New Features

## 新特性

- AddedJust-In-Time Link-Time Optimized (JIT LTO) kernels for improved performance in R2C and C2R FFTs for many sizes.

- 添加了即时链接时间优化（JIT LTO）内核，以提高多种尺寸的 R2C 和 C2R FFT 的性能。

- We recommend testing your R2C / C2R use cases with and without JIT LTO kernels and comparing the resulting performance. You can enable JIT LTO kernels using the per-plan properties $\mathsf{{cuFFT}\;{API}}.$

- 我们建议在有和没有 JIT LTO 内核的情况下测试您的 R2C / C2R 用例，并比较结果性能。您可以使用每个计划的属性 $\mathsf{{cuFFT}\;{API}}.$ 启用 JIT LTO 内核。

#### 6.2.4. cuFFT: Release 12.4 Update 1

#### 6.2.4. cuFFT: 版本 12.4 更新 1

## Resolved Issues

## 已解决的问题

- A routine from the cuFFT LTO EA library was added by mistake to the cuFFT Advanced API header (cufftXt.h) in CUDA 12.4. This routine has now been removed from the header.

- 在 CUDA 12.4 中，cuFFT LTO EA 库中的一个例程被错误地添加到了 cuFFT 高级 API 头文件（cufftXt.h）中。该例程现已被从头文件中移除。

#### 6.2.5. cuFFT: Release 12.4

#### 6.2.5. cuFFT: 版本 12.4

## New Features

## 新特性

- Added Just-In-Time Link-Time Optimized (JIT LTO) kernels for improved performance in FFTs with 64-bit indexing.

- 添加了即时链接时间优化（JIT LTO）内核，以提高64位索引FFT的性能。

- Added per-plan properties to the cuFFT API. These new routines can be leveraged to give users more control over the behavior of cuFFT. Currently they can be used to enable JIT LTO kernels for 64-bit FFTs.

- 向cuFFT API添加了每个计划的属性。这些新例程可以用来让用户更好地控制cuFFT的行为。目前，它们可以用于启用64位FFT的JIT LTO内核。

- Improved accuracy for certain single-precision (fp32) FFT cases, especially involving FFTs for larger sizes.

- 提高了某些单精度（fp32）FFT情况的准确性，特别是涉及较大尺寸的FFT。

## Known Issues

## 已知问题

- A routine from the cuFFT LTO EA library was added by mistake to the cuFFT Advanced API header (cufftXt.h). This routine is not supported by cuFFT, and will be removed from the header in a future release.

- cuFFT LTO EA库中的一个例程被错误地添加到了cuFFT高级API头文件（cufftXt.h）中。此例程不受cuFFT支持，并将在未来的版本中从头文件中移除。

## Resolved Issues

## 已解决的问题

- Fixed an issue that could cause overwriting of user data when performing out-of-place real-to-complex (R2C) transforms with user-specified output strides (i.e. using the ostride component of the Advanced Data Layout API).

- 修复了在使用用户指定的输出步幅（即使用高级数据布局 API 的 ostride 组件）执行异地实到复（R2C）变换时可能导致用户数据覆盖的问题。

- Fixed inconsistent behavior between libcufftw and FFTW when both inembed and onem-bed are nullptr / NULL. From now on, as in FFTW, passing nullptr / NULL as inembed/ onembed parameter is equivalent to passing $n$ ,that is,the logical size for that dimension.

- 修复了当 inembed 和 onembed 均为 nullptr / NULL 时，libcufftw 和 FFTW 之间的行为不一致问题。从现在开始，与 FFTW 一样，将 nullptr / NULL 作为 inembed/onembed 参数传递等同于传递 $n$，即该维度的逻辑大小。

#### 6.2.6. cuFFT: Release 12.3 Update 1

#### 6.2.6. cuFFT: 12.3 更新 1

## Known Issues

## 已知问题

- Executing a real-to-complex (R2C) or complex-to-real (C2R) plan in a context different to the one used to create the plan could cause undefined behavior. This issue will be fixed in an upcoming release of cuFFT.

- 在与创建计划时不同的上下文中执行实到复（R2C）或复到实（C2R）计划可能导致未定义行为。此问题将在 cuFFT 的后续版本中修复。

## Resolved Issues

## 已解决的问题

- Complex-to-complex (C2C) execution functions (cufftExec and similar) now properly error-out in case of error during kernel launch, for example due to a missing CUDA context.

- 复杂到复杂（C2C）执行函数（如 cufftExec 等）现在在核启动期间发生错误时（例如由于缺少 CUDA 上下文）会正确报错。

#### 6.2.7. cuFFT: Release 12.3

#### 6.2.7. cuFFT: 版本 12.3

## New Features

## 新特性

- Callback kernels are more relaxed in terms of resource usage, and will use fewer registers.

- 回调内核在资源使用方面更加宽松，并且将使用更少的寄存器。

- Improved accuracy for double precision prime and composite FFT sizes with factors larger than 127.

- 提高了双精度素数和复合 FFT 尺寸的精度，因子大于 127。

- Slightly improved planning times for some FFT sizes.

- 针对某些 FFT 大小的规划时间略有改善。

#### 6.2.8. cuFFT: Release 12.2

#### 6.2.8. cuFFT: 版本 12.2

## New Features

## 新特性

- cufftSetStream can be used in multi-GPU plans with a stream from any GPU context, instead of from the primary context of the first GPU listed in cufftXtSetGPUs.

- cufftSetStream 可用于多 GPU 计划中的任何 GPU 上下文的流，而不是来自 cufftXtSetGPUs 中列出的第一个 GPU 的主上下文。

- Improved performance of 1000+ of FFTs of sizes ranging from 62 to 16380. The improved performance spans hundreds of single precision and double precision cases for FFTs with contiguous data layout, across multiple GPU architectures (from Maxwell to Hopper GPUs) via PTX JIT.

- 改进了 62 到 16380 大小的 1000 多个 FFT 的性能。通过 PTX JIT，改进的性能涵盖了从 Maxwell 到 Hopper GPU 的多种 GPU 架构中，具有连续数据布局的 FFT 的数百个单精度和双精度情况。

- Reduced the size of the static libraries when compared to cuFFT in the 12.1 release.

- 与 12.1 版本中的 cuFFT 相比，减少了静态库的大小。

## Resolved Issues

## 已解决的问题

S cuFFT no longer exhibits a race condition when threads simultaneously create and access plans with more than 1023 plans alive.

当线程同时创建和访问超过1023个活动计划时，S cuFFT 不再表现出竞争条件。

- cuFFT no longer exhibits a race condition when multiple threads call cufftXtSetGPUs concurrently.

- 当多个线程同时调用 cufftXtSetGPUs 时，cuFFT 不再表现出竞争条件。

#### 6.2.9. cuFFT: Release 12.1 Update 1

#### 6.2.9. cuFFT: 12.1 更新 1

## $>$ Known Issues

## $>$ 已知问题

coFFT exhibits a race condition when one thread calls cufftCreate (or cufftDestroy) and another thread calls any API (except cufftCreate or cufftDestroy), and when the total number of plans alive exceeds 1023.

当一个线程调用 cufftCreate（或 cufftDestroy）而另一个线程调用任何 API（除了 cufftCreate 或 cufftDestroy）时，并且当活动计划的总数超过1023时，coFFT 表现出竞争条件。

P cuFFT exhibits a race condition when multiple threads call cufftXtSetGPUs concurrently on different plans.

P cuFFT 在多个线程在不同计划上同时调用 cufftXtSetGPUs 时表现出竞争条件。

#### 6.2.10. cuFFT: Release 12.1

#### 6.2.10. cuFFT: 版本 12.1

## New Features

## 新特性

- Improved performance on Hopper GPUs for hundreds of FFTs of sizes ranging from 14 to 28800. The improved performance spans over 542 cases across single and double precision for FFTs with contiguous data layout.

- 在 Hopper GPU 上，对于大小从 14 到 28800 的数百个 FFT，性能得到了提升。性能提升涵盖了 542 种情况，包括单精度和双精度的连续数据布局的 FFT。

## Known Issues

## 已知问题

- Starting from CUDA 11.8, CUDA Graphs are no longer supported for callback routines that load data in out-of-place mode transforms. An upcoming release will update the cuFFT callback implementation, removing this limitation. cuFFT deprecated callback functionality based on separate compiled device code in cuFFT 11.4.

- 从 CUDA 11.8 开始，CUDA Graphs 不再支持在异地模式转换中加载数据的回调例程。即将发布的版本将更新 cuFFT 回调实现，消除此限制。cuFFT 在 11.4 版本中弃用了基于单独编译设备代码的回调功能。

## Resolved Issues

## 已解决的问题

- cuFFT no longer produces errors with compute-sanitizer at program exit if the CUDA context used at plan creation was destroyed prior to program exit.

- 如果在程序退出之前销毁了计划创建时使用的 CUDA 上下文，cuFFT 在程序退出时不再产生 compute-sanitizer 错误。

#### 6.2.11. cuFFT: Release 12.0 Update 1

#### 6.2.11. cuFFT: 12.0 更新 1

## Resolved Issues

## 已解决的问题

Scratch space requirements for multi-GPU, single-batch, 1D FFTs were reduced.

多 GPU、单批次、一维 FFT 的暂存空间需求已减少。

#### 6.2.12. cuFFT: Release 12.0

#### 6.2.12. cuFFT: 12.0 版本

## New Features

## 新特性

- PTX JIT kernel compilation allowed the addition of many new accelerated cases for Maxwell, Pascal, Volta and Turing architectures.

- PTX JIT 内核编译允许为 Maxwell、Pascal、Volta 和 Turing 架构添加许多新的加速案例。

## $>$ Known Issues

## $>$ 已知问题

S cuFFT plan generation time increases due to PTX JIT compiling. Refer to Plan Initialization TIme.

由于 PTX JIT 编译，S cuFFT 计划生成时间增加。请参阅计划初始化时间。

## Resolved Issues

## 已解决问题

- cuFFT plans had an unintentional small memory overhead (of a few kB) per plan. This is resolved.

- cuFFT 计划每个计划有意外的小内存开销（几 kB）。此问题已解决。

### 6.3. cuSOLVER Library

### 6.3. cuSOLVER 库

#### 6.3.1. cuSOLVER: Release 12.6 Update 2

#### 6.3.1. cuSOLVER: 12.6 更新 2

## New Features

## 新特性

- New API cusolverDnXgeev to solve non-Hermitian eigenvalue problems.

- 新增 API cusolverDnXgeev 用于解决非厄米特特征值问题。

- New API cusolverDnXsyevBatched to solve uniform batched Hermitian eigenvalue problems.

- 新增 API cusolverDnXsyevBatched 用于解决统一批处理厄米特特征值问题。

## Known Issues

## 已知问题

cusolverDnXsyevBatched can compute an incorrect result when the batch size is at least 2 and cuComplex or cuDoubleComplex are used. The workaround is to initialize the workspace to zero before calling cusolverDnXsyevBatched. [4899543]

当批量大小至少为2且使用cuComplex或cuDoubleComplex时，cusolverDnXsyevBatched可能会计算出错误的结果。解决方法是在调用cusolverDnXsyevBatched之前将工作空间初始化为零。[4899543]

#### 6.3.2. cuSOLVER: Release 12.6

#### 6.3.2. cuSOLVER: 版本 12.6

## New Features

## 新特性

Performance improvements of cusolverDnXgesvdp(   ).

cusolverDnXgesvdp()的性能改进。

#### 6.3.3. cuSOLVER: Release 12.5 Update 1

#### 6.3.3. cuSOLVER: 版本 12.5 更新 1

## Resolved Issues

## 已解决的问题

- The potential out-of-bound accesses on bufferOnDevice by calls of cusolverDnXlarft have been resolved.

- 通过调用 cusolverDnXlarft 对 bufferOnDevice 的潜在越界访问问题已得到解决。

#### 6.3.4. cuSOLVER: Release 12.5

#### 6.3.4. cuSOLVER: 版本 12.5

## New Features

## 新特性

- Performance improvements of cusolverDnXgesvd and cusolverDn<t>gesvd if jobu != 'N' or jobvt != 'N'.

- 如果 jobu != 'N' 或 jobvt != 'N'，cusolverDnXgesvd 和 cusolverDn<t>gesvd 的性能有所提升。

Performance improvements of cusolverDnXgesvdp if jobz = CU-SOLVER_EIG_MODE_NOVECTOR.

如果 jobz = CU-SOLVER_EIG_MODE_NOVECTOR，cusolverDnXgesvdp 的性能有所提升。

- Lower workspace requirement of cusolverDnXgesvdp for tall-and-skinny-matrices.

- 对于高瘦矩阵，cusolverDnXgesvdp 的工作空间需求降低。

## Known Issues

## 已知问题

- With CUDA Toolkit 12.4 Update 1, values 1dt > k in calls of cusolverDnXlarft can result in out-of-bound memory accesses on bufferOnDevice. As a workaround it is possible to allocate a larger device workspace buffer of size workspaceInBytesOnDe-vice=ALIGN_32((ldt*k + n*k)*sizeofCudaDataType(dataTypeT)), with

- 使用 CUDA Toolkit 12.4 Update 1 时，在调用 cusolverDnXlarft 时，值 1dt > k 可能导致 bufferOnDevice 上的内存访问越界。作为解决方法，可以分配一个更大的设备工作空间缓冲区，大小为 workspaceInBytesOnDevice=ALIGN_32((ldt*k + n*k)*sizeofCudaDataType(dataTypeT))。

---

auto ALIGN_32=[](int64_t val) \{

	return ((val + 31)/32)*32;

\};

and

auto sizeofCudaDataType=[](cudaDataType dt) \{

	if (dt == CUDA_R_32F) return sizeof(float);

	if (dt == CUDA_R_64F) return sizeof(double);

	if (dt == CUDA_C_32F) return sizeof(cuComplex);

	if (dt == CUDA_C_64F) return sizeof(cuDoubleComplex);

\};

---

#### 6.3.5. cuSOLVER: Release 12.4 Update 1

#### 6.3.5. cuSOLVER: 版本 12.4 更新 1

## New Features

## 新特性

- The performance of cusolverDnXlarft has been improved. For large matrices, the speedup might exceed 100x. The performance on H100 is now consistently better than on A100. The change in cusolverDnXlarft also results in a modest speedup in cu-solverDn<t>ormqr, cusolverDn<t>ormtr, and cusolverDnXsyevd.

- cusolverDnXlarft 的性能已得到提升。对于大型矩阵，加速可能超过 100 倍。在 H100 上的性能现在始终优于 A100。cusolverDnXlarft 的更改还导致 cusolverDn<t>ormqr、cusolverDn<t>ormtr 和 cusolverDnXsyevd 的性能略有提升。

- The performance of cusolverDnXgesvd when singular vectors are sought has been improved. The job configuration that computes both left and right singular vectors is up to 1.5x faster.

- 当需要计算奇异向量时，cusolverDnXgesvd 的性能已得到提升。计算左右奇异向量的作业配置速度提高了 1.5 倍。

## Resolved Issues

## 已解决的问题

- cusolverDnXtrtri_bufferSize now returns the correct workspace size in bytes.

- cusolverDnXtrtri_bufferSize 现在返回正确的工作空间大小（以字节为单位）。

## Deprecations

## 弃用

- Using long-deprecated cusolverDnPotrf, cusolverDnPotrs, cusolverDnGeqrf, cusolverDnGetrf, cusolverDnGetrs, cusolverDnSyevd, cusolverDnSyevdx, cu-solverDnGesvd, and their accompanying bufferSize functions will result in a deprecation warning. The warning can be turned off by using the -DDISABLE_CUSOLVER_DEPRECATED flag while compiling; however, users should use cusolverDnXpotrf, cusolverDnXpotrs, cusolverDnXgeqrf, cusolverDnXgetrf, cusolverDnXgetrs, cusolverDnXsyevd, cusolverDnX syevdx, cusolverDnX gesvd, and the corresponding bufferSize functions instead.

- 使用长期弃用的 cusolverDnPotrf、cusolverDnPotrs、cusolverDnGeqrf、cusolverDnGetrf、cusolverDnGetrs、cusolverDnSyevd、cusolverDnSyevdx、cu-solverDnGesvd 及其伴随的 bufferSize 函数将导致弃用警告。在编译时使用 -DDISABLE_CUSOLVER_DEPRECATED 标志可以关闭警告；然而，用户应改用 cusolverDnXpotrf、cusolverDnXpotrs、cusolverDnXgeqrf、cusolverDnXgetrf、cusolverDnXgetrs、cusolverDnXsyevd、cusolverDnXsyevdx、cusolverDnXgesvd 及相应的 bufferSize 函数。

#### 6.3.6. cuSOLVER: Release 12.4

#### 6.3.6. cuSOLVER: 版本 12.4

## New Features

## 新特性

- cusolverDnXlarft and cusolverDnXlarft_bufferSize APIs were introduced. cu-solverDnXlarft forms the triangular factor of a real block reflector, while cusolverD-nXlarft_bufferSize returns its required workspace sizes in bytes.

- 引入了 cusolverDnXlarft 和 cusolverDnXlarft_bufferSize API。cu-solverDnXlarft 形成实块反射器的三角因子，而 cusolverD-nXlarft_bufferSize 返回其所需的工作空间大小（以字节为单位）。

## Known Issues

## 已知问题

- cusolverDnXtrtri_bufferSize returns an incorrect required device workspace size. As a workaround the returned size can be multiplied by the size of the data type (for example, 8 bytes if matrix A is of type double) to obtain the correct workspace size.

- cusolverDnXtrtri_bufferSize 返回的设备工作空间大小不正确。作为解决方法，可以将返回的大小乘以数据类型的大小（例如，如果矩阵 A 是 double 类型，则为 8 字节）以获得正确的工作空间大小。

#### 6.3.7. cuSOLVER: Release 12.2 Update 2

#### 6.3.7. cuSOLVER: 版本 12.2 更新 2

## Resolved Issues

## 已解决问题

- Fixed an issue with cusolverDn<t>gesvd(   ), cusolverDnGesvd(   ), and cusolverD-nXgesvd(   ), which could cause wrong results for matrices larger than 18918 if jobu or jobvt was unequal to ’ $N$ ’.

- 修复了 cusolverDn<t>gesvd(   )、cusolverDnGesvd(   ) 和 cusolverD-nXgesvd(   ) 的问题，如果 jobu 或 jobvt 不等于 ’ $N$ ’，可能会导致大于 18918 的矩阵结果错误。

#### 6.3.8. cuSOLVER: Release 12.2

#### 6.3.8. cuSOLVER: 版本 12.2

## New Features

## 新特性

- A new API to ensure deterministic results or allow non-deterministic results for improved performance. See cusolverDnSetDeterministicMode(   ) and cusolverD-nGetDeterministicMode(   ). Affected functions are: cusolverDn<t>geqrf(   ), cusolverDn<t>syevd(   ), cusolverDn<t>syevdx(   ), cusolverDn<t>gesvdj(   ), cu-solverDnXgeqrf(   ), cusolverDnXsyevd(   ), cusolverDnXsyevdx(   ), cusolverD-nXgesvdr(   ), and cusolverDnXgesvdp(   ).

- 一个新的 API 用于确保确定性结果或允许非确定性结果以提高性能。参见 cusolverDnSetDeterministicMode(   ) 和 cusolverDnGetDeterministicMode(   )。受影响的函数有：cusolverDn<t>geqrf(   ), cusolverDn<t>syevd(   ), cusolverDn<t>syevdx(   ), cusolverDn<t>gesvdj(   ), cusolverDnXgeqrf(   ), cusolverDnXsyevd(   ), cusolverDnXsyevdx(   ), cusolverDnXgesvdr(   ), 和 cusolverDnXgesvdp(   )。

## Known Issues

## 已知问题

- Concurrent executions of cusolverDn<t>getrf(   ) or cusolverDnXgetrf(   ) in different non-blocking CUDA streams on the same device might result in a deadlock.

- 在同一设备的不同非阻塞 CUDA 流中并发执行 cusolverDn<t>getrf(   ) 或 cusolverDnXgetrf(   ) 可能会导致死锁。

### 6.4. cuSPARSE Library

### 6.4. cuSPARSE 库

#### 6.4.1. cuSPARSE: Release 12.6 Update 2

#### 6.4.1. cuSPARSE: 版本 12.6 更新 2

## Resolved Issues

## 已解决的问题

- Re-wrote the documentation for cusparseSpMV_preprocess(   ), cuspars-eSpMM_preprocess(   ), and cusparseSDDMM_preprocess(   ). The documentation now explains the additional constraints that code must satisfy when using these functions. [CUSPARSE-1962]

- 重写了 cusparseSpMV_preprocess(   )、cusparseSpMM_preprocess(   ) 和 cusparseSDDMM_preprocess(   ) 的文档。文档现在解释了使用这些函数时代码必须满足的额外约束条件。[CUSPARSE-1962]

- cusparseSpMV(   ) would expect the values in the external buffer to be maintained from one call to the next. If this was not true, it could compute the incorrect result or crash. [CUSPARSE-1897]

- cusparseSpMV(   ) 期望外部缓冲区中的值在一次调用到下一次调用之间保持不变。如果不满足此条件，它可能会计算出错误的结果或崩溃。[CUSPARSE-1897]

- cusparseSpMV_preprocess(   ) wouldn't run correctly if cusparseSpMM_preprocess(   ) was executed on the same matrix, and vice versa. [CUSPARSE-1897]

- 如果在同一矩阵上执行了 cusparseSpMM_preprocess(   )，则 cusparseSpMV_preprocess(   ) 无法正确运行，反之亦然。[CUSPARSE-1897]

- cusparseSpMV_preprocess(   ) runs SpMV computation if it’s called two or more times on the same matrix. [CUSPARSE-1897]

- 如果在同一矩阵上调用两次或更多次，cusparseSpMV_preprocess(   ) 会运行 SpMV 计算。[CUSPARSE-1897]

- cusparseSpMV(   ) could cause subsequent calls to cusparseSpMM(   ) with the same matrix to produce incorrect results or crash. [CUSPARSE-1897]

- cusparseSpMV(   ) 可能会导致后续使用相同矩阵调用 cusparseSpMM(   ) 时产生错误结果或崩溃。[CUSPARSE-1897]

- With a single sparse matrix A and a dense matrix X that has only a single column, calling both cusparseSpMM_preprocess $\left( {A,X,\ldots }\right)$ could cause subsequent calls to cuspars-eSpMV(   ) to crash or produce incorrect results. The same is true with the roles of SpMV and SpMM swapped. [CUSPARSE-1921]

- 对于单个稀疏矩阵 A 和仅有一列的稠密矩阵 X，调用 cusparseSpMM_preprocess $\left( {A,X,\ldots }\right)$ 可能会导致后续调用 cusparseSpMV(   ) 时崩溃或产生错误结果。同样，如果 SpMV 和 SpMM 的角色互换，也会出现相同的问题。[CUSPARSE-1921]

#### 6.4.2. cuSPARSE: Release 12.6

#### 6.4.2. cuSPARSE: 版本 12.6

## Known Issues

## 已知问题

- cusparseSpMV_preprocess(   ) runs SpMV computation if it is called two or more times on the same matrix. [CUSPARSE-1897]

- cusparseSpMV_preprocess(   ) 如果在同一矩阵上调用两次或更多次，则会运行 SpMV 计算。[CUSPARSE-1897]

- cusparseSpMV_preprocess(   ) will not run if cusparseSpMM_preprocess(   ) was executed on the same matrix, and vice versa. [CUSPARSE-1897]

- 如果在同一矩阵上执行了 cusparseSpMM_preprocess(   )，则 cusparseSpMV_preprocess(   ) 不会运行，反之亦然。[CUSPARSE-1897]

The same external_buffer must be used for all cusparseSpMV calls. [CUSPARSE-1897]

所有 cusparseSpMV 调用必须使用相同的外部缓冲区。[CUSPARSE-1897]

#### 6.4.3. cuSPARSE: Release 12.5 Update 1

#### 6.4.3. cuSPARSE: 12.5 版本更新 1

## New Features

## 新特性

- Added support for BSR format in cusparseSpMM.

- 在 cusparseSpMM 中添加了对 BSR 格式的支持。

## Resolved Issues

## 已解决的问题

> cusparseSpMM(   ) would sometimes get incorrect results when alpha=0, num_batches>1, batch_stride indicates that there is padding between batches.

> 当 alpha=0, num_batches>1, batch_stride 表示批次之间有填充时，cusparseSpMM(   ) 有时会得到不正确的结果。

- cusparseSpMM_bufferSize(   ) would return the wrong size when the sparse matrix is Blocked Ellpack and the dense matrices have only a single column (n=1).

- 当稀疏矩阵为 Blocked Ellpack 且密集矩阵只有单列（n=1）时，cusparseSpMM_bufferSize(   ) 会返回错误的大小。

- cusparseSpMM returned the wrong result when k=0 (for example when A has zero columns). The correct behavior is doing $C \smallsetminus   *  =$ beta. The bug behavior was not modifying $C$ at all.

- 当 k=0 时（例如当 A 有零列时），cusparseSpMM 返回了错误的结果。正确的行为是执行 $C \smallsetminus   *  =$ beta。错误行为是完全没有修改 $C$。

- cusparseCreateSlicedEll would return an error when the slice size is greater than the matrix number of rows.

- 当切片大小大于矩阵行数时，cusparseCreateSlicedEll 会返回错误。

Sliced-ELLPACK cusparseSpSV produced wrong results for diagonal matrices.

Sliced-ELLPACK cusparseSpSV 对对角矩阵产生了错误的结果。

- Sliced-ELLPACK cusparseSpSV_analysis(   ) failed due to insufficient resources for some matrices and some slice sizes.

- 对于某些矩阵和某些切片大小，Sliced-ELLPACK cusparseSpSV_analysis(   ) 由于资源不足而失败。

#### 6.4.4. cuSPARSE: Release 12.5

#### 6.4.4. cuSPARSE: 版本 12.5

## New Features

## 新特性

- Added support for mixed input types in SpMV: single precision input matrix, double precision input vector, double precision output vector.

- 在 SpMV 中添加了对混合输入类型的支持：单精度输入矩阵，双精度输入向量，双精度输出向量。

## Resolved Issues

## 已解决的问题

- cusparseSpMV(   ) introduces invalid memory accesses when the output vector is not aligned to 16 bytes.

- cusparseSpMV(   ) 在输出向量未对齐到 16 字节时引入了无效的内存访问。

#### 6.4.5. cuSPARSE: Release 12.4

#### 6.4.5. cuSPARSE: 版本 12.4

## New Features

## 新特性

Added the preprocessing step for sparse matrix-vector multiplication cuspars-eSpMV_preprocess(   ).

添加了稀疏矩阵-向量乘法的预处理步骤 cuspars-eSpMV_preprocess(   )。

- Added support for mixed real and complex types for cusparseSpMM(   ).

- 添加了对 cusparseSpMM(   ) 的混合实数和复数类型的支持。

- Added a new API cusparseSpSM_updateMatrix(   ) to update the sparse matrix between the analysis and solving phase of cusparseSpSM(   ).

- 添加了一个新的 API cusparseSpSM_updateMatrix(   )，用于在 cusparseSpSM(   ) 的分析和求解阶段之间更新稀疏矩阵。

## Known Issues

## 已知问题

- cusparseSpMV(   ) introduces invalid memory accesses when the output vector is not aligned to 16 bytes.

- 当输出向量未对齐到 16 字节时，cusparseSpMV(   ) 会引入无效的内存访问。

## Resolved Issues

## 已解决问题

- cusparseSpVV(   ) provided incorrect results when the sparse vector has many non-zeros.

- 当稀疏向量有许多非零元素时，cusparseSpVV(   ) 提供了不正确的结果。

#### 6.4.6. cuSPARSE: Release 12.3 Update 1

#### 6.4.6. cuSPARSE: 版本 12.3 更新 1

## New Features

## 新特性

- Added support for block sizes of 64 and 128 in cusparseSDDMM(   ).

- 在 cusparseSDDMM(   ) 中添加了对 64 和 128 块大小的支持。

- Added a preprocessing step cusparseSDDMM_preprocess(   ) for BSR cusparseSDDMM(   ) that helps improve performance of the main computing stage.

- 为 BSR cusparseSDDMM(   ) 添加了一个预处理步骤 cusparseSDDMM_preprocess(   )，有助于提高主计算阶段的性能。

#### 6.4.7. cuSPARSE: Release 12.3

#### 6.4.7. cuSPARSE: 版本 12.3

## New Features

## 新特性

- The cusparseSpSV_bufferSize(   ) and cusparseSpSV_analysis(   ) routines now accept NULL pointers for the dense vector.

- cusparseSpSV_bufferSize(   ) 和 cusparseSpSV_analysis(   ) 例程现在接受密集向量的 NULL 指针。

- The cusparseSpSM_bufferSize(   ) and cusparseSpSM_analysis(   ) routines now accept dense matrix descriptors with NULL pointer for values.

- cusparseSpSM_bufferSize(   ) 和 cusparseSpSM_analysis(   ) 例程现在接受值字段为 NULL 指针的密集矩阵描述符。

## Known Issues

## 已知问题

- The cusparseSpSV_analysis(   ) and cusparseSpSM_analysis(   ) routines are blocking calls/not asynchronous.

- cusparseSpSV_analysis(   ) 和 cusparseSpSM_analysis(   ) 例程是阻塞调用/非异步的。

- Wrong results can occur for cusparseSpSV(   ) using sliced ELLPACK format and transpose/transpose conjugate operation on matrix A.

- 使用切片 ELLPACK 格式和矩阵 A 的转置/转置共轭操作时，cusparseSpSV(   ) 可能会出现错误结果。

## Resolved Issues

## 已解决的问题

- cusparseSpSV(   ) provided indeterministic results in some cases.

- cusparseSpSV(   ) 在某些情况下提供了不确定的结果。

- Fixed an issue that caused cusparseSpSV_analysis(   ) to hang sometimes in a multithread environment.

- 修复了导致 cusparseSpSV_analysis(   ) 在多线程环境中有时挂起的问题。

- Fixed an issue with cusparseSpSV(   ) and cusparseSpSV(   ) that sometimes yielded wrong output when the output vector/matrix or input matrix contained NaN.

- 修复了 cusparseSpSV(   ) 和 cusparseSpSV(   ) 的问题，当输出向量/矩阵或输入矩阵包含 NaN 时，有时会产生错误的输出。

#### 6.4.8. cuSPARSE: Release 12.2 Update 1

#### 6.4.8. cuSPARSE: 版本 12.2 更新 1

## New Features

## 新功能

- The library now provides the opportunity to dump sparse matrices to files during the creation of the descriptor for debugging purposes. See logging API https://docs.nvidia.com/ cuda/cusparse/index.html#cusparse-logging-api.

- 该库现在提供了在创建描述符时将稀疏矩阵转储到文件中的功能，以便进行调试。请参阅日志记录 API https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-logging-api。

## Resolved Issues

## 已解决的问题

- Removed CUSPARSE_SPMM_CSR_ALG3 fallback to avoid confusion in the algorithm selection process.

- 移除了 CUSPARSE_SPMM_CSR_ALG3 的回退，以避免在算法选择过程中产生混淆。

- Clarified the supported operations for cusparseSDDMM(   ).

- 明确了 cusparseSDDMM(   ) 支持的操作。

- cusparseCreateConstSlicedEll(   ) now uses const pointers.

- cusparseCreateConstSlicedEll(   ) 现在使用常量指针。

Fixed wrong results in rare edge cases of cusparseCsr2CscEx2(   ) with base 1 indexing.

修复了在 cusparseCsr2CscEx2(   ) 中使用基址 1 索引时在罕见边缘情况下出现的错误结果。

cusparseSpSM_bufferSize(   ) could ask slightly less memory than needed.

cusparseSpSM_bufferSize(   ) 可能会请求比实际需要稍少的内存。

- cusparseSpMV(   ) now checks the validity of the buffer pointer only when it is strictly needed.

- cusparseSpMV(   ) 现在仅在严格需要时检查缓冲区指针的有效性。

## Deprecations

## 弃用

- Several legacy APIs have been officially deprecated. A compile-time warning has been added to all of them.

- 几个遗留的 API 已被正式弃用。已为所有这些 API 添加了编译时警告。

#### 6.4.9. cuSPARSE: Release 12.1 Update 1

#### 6.4.9. cuSPARSE: 12.1 更新 1

## New Features

## 新特性

- Introduced Block Sparse Row (BSR) sparse matrix storage for the Generic APIs with support for SDDMM routine (cusparseSDDMM).

- 为通用 API 引入了块稀疏行 (BSR) 稀疏矩阵存储，并支持 SDDMM 例程 (cusparseSDDMM)。

- Introduced Sliced Ellpack (SELL) sparse matrix storage format for the Generic APIs with support for sparse matrix-vector multiplication (cusparseSpMV) and triangular solver with a single right-hand side (cusparseSpSV).

- 为通用 API 引入了切片 Ellpack (SELL) 稀疏矩阵存储格式，并支持稀疏矩阵-向量乘法 (cusparseSpMV) 和单右侧三角求解器 (cusparseSpSV)。

- Added a new API call (cusparseSpSV_updateMatrix) to update matrix values and/or the matrix diagonal in the sparse triangular solver with a single right-hand side after the analysis step.

- 添加了一个新的 API 调用 (cusparseSpSV_updateMatrix)，用于在分析步骤后更新稀疏三角求解器中的矩阵值和/或矩阵对角线。

#### 6.4.10. cuSPARSE: Release 12.0 Update 1

#### 6.4.10. cuSPARSE: 12.0 更新 1

## New Features

## 新特性

- cusparseSDDMM(   ) now supports mixed precision computation.

- cusparseSDDMM(   ) 现在支持混合精度计算。

- Improved cusparseSpMM(   ) alg2 mixed-precision performance on some matrices on NVIDIA Ampere architecture GPUs.

- 在 NVIDIA Ampere 架构 GPU 上，改进了 cusparseSpMM(   ) alg2 在某些矩阵上的混合精度性能。

- Improved cusparseSpMV(   ) performance with a new load balancing algorithm.

- 使用新的负载均衡算法改进了 cusparseSpMV(   ) 的性能。

- cusparseSpSV(   ) and cusparseSpSM(   ) now support in-place computation, namely the output and input vectors/matrices have the same memory address.

- cusparseSpSV(   ) 和 cusparseSpSM(   ) 现在支持原地计算，即输出和输入向量/矩阵具有相同的内存地址。

## Resolved Issues

## 已解决的问题

- cusparseSpSM(   ) could produce wrong results if the leading dimension (Id) of the RHS matrix is greater than the number of columns/rows.

- 如果 RHS 矩阵的前导维度 (Id) 大于列数/行数，cusparseSpSM(   ) 可能会产生错误的结果。

#### 6.4.11. cuSPARSE: Release 12.0

#### 6.4.11. cuSPARSE: 版本 12.0

## New Features

## 新特性

- JIT LTO functionalities (cusparseSpMMOp(   )) switched from driver to nvJitLto library. Starting from CUDA 12.0 the user needs to link to libnvJitLto.so, see cuSPARSE documentation. JIT LTO performance has also been improved for cusparseSpMMOpPlan(   ).

- JIT LTO 功能（cusparseSpMMOp(   )）从驱动程序切换到 nvJitLto 库。从 CUDA 12.0 开始，用户需要链接到 libnvJitLto.so，请参阅 cuSPARSE 文档。JIT LTO 性能在 cusparseSpMMOpPlan(   ) 中也得到了改进。

- Introduced const descriptors for the Generic APIs, for example, cusparseConst-SpVecGet (   ). Now the Generic APIs interface clearly declares when a descriptor and its data are modified by the cuSPARSE functions.

- 为通用 API 引入了常量描述符，例如 cusparseConst-SpVecGet(   )。现在，通用 API 接口明确声明了 cuSPARSE 函数何时修改描述符及其数据。

- Added two new algorithms to cusparseSpGEMM(   ) with lower memory utilization. The first algorithm computes a strict bound on the number of intermediate product, while the second one allows partitioning the computation in chunks.

- 在 cusparseSpGEMM(   ) 中添加了两个新算法，内存利用率更低。第一个算法计算中间产品数量的严格界限，而第二个算法允许将计算分块进行。

Added int8_t support to cusparseGather(   ), cusparseScatter(   ), and cuspar-seCsr2cscEx2(   ).

在 cusparseGather(   )、cusparseScatter(   ) 和 cuspar-seCsr2cscEx2(   ) 中添加了 int8_t 支持。

- Improved cusparseSpSV(   ) performance for both the analysis and the solving phases.

- 改进了 cusparseSpSV(   ) 在分析和求解阶段的性能。

Improved cusparseSpSM(   ) performance for both the analysis and the solving phases.

改进了 cusparseSpSM(   ) 在分析和求解阶段的性能。

- Improved cusparseSDDMM(   ) performance and added support for batch computation.

- 改进了 cusparseSDDMM(   ) 的性能，并增加了对批量计算的支持。

- Improved cusparseCsr2cscEx2(   ) performance.

- 改进了 cusparseCsr2cscEx2(   ) 的性能。

## Resolved Issues

## 已解决的问题

- cusparseSpSV(   ) and cusparseSpSM(   ) could produce wrong results.

- cusparseSpSV(   ) 和 cusparseSpSM(   ) 可能会产生错误的结果。

susparseDnMatGetStridedBatch(   ) did not accept batchStride == 0.

susparseDnMatGetStridedBatch(   ) 不接受 batchStride == 0。

## Deprecations

## 弃用

- Removed deprecated CUDA 11.x APIs, enumerators, and descriptors.

- 移除了已弃用的 CUDA 11.x API、枚举器和描述符。

### 6.5. Math Library

### 6.5. 数学库

#### 6.5.1. CUDA Math: Release 12.6 Update 1

#### 6.5.1. CUDA 数学：12.6 更新 1 版本

## Resolved Issues

## 已解决的问题

- Issue 4731352 from release 12.6 is resolved.

- 12.6 版本中的问题 4731352 已解决。

#### 6.5.2. CUDA Math: Release 12.6

#### 6.5.2. CUDA 数学：12.6 版本

## $>$ Known Issues

## $>$ 已知问题

As a result of ongoing compatibility testing NVIDIA identified that a number of CUDA Math Integer SIMD APIs silently produced wrong results if used on the CPU in programs compiled with MSVC 17.10. The root cause is found to be the coding error in the header-based implementation of the APIs exposed to the undefined behavior during narrowing integer conversion when doing a host-based emulation of the GPU functionality. The issue will be fixed in a future release of CUDA. Applications affected are those calling _____vimax3_s16x2, _____vimin3_s16x2, _____vibmax_s16x2, and _____vibmin_s16x2 on the CPU and not in CUDA kernels. [4731352]

由于持续的兼容性测试，NVIDIA 发现如果在使用 MSVC 17.10 编译的程序中在 CPU 上使用某些 CUDA 数学整数 SIMD API，这些 API 会静默地产生错误结果。根本原因是在基于头文件的 API 实现中发现了编码错误，这些 API 在进行基于主机的 GPU 功能模拟时，在整数缩窄转换期间暴露了未定义行为。该问题将在未来的 CUDA 版本中修复。受影响的应用程序是那些在 CPU 上调用 _____vimax3_s16x2、_____vimin3_s16x2、_____vibmax_s16x2 和 _____vibmin_s16x2 而不是在 CUDA 内核中调用的应用程序。[4731352]

#### 6.5.3. CUDA Math: Release 12.5

#### 6.5.3. CUDA 数学：12.5 版本

## Known Issues

## 已知问题

- As a result of ongoing testing we updated the interval bounds in which double precision lgamma(   ) function may experience greater than the documented 4 ulp accuracy loss. New interval shall read (-23.0001; -2.2637). This finding is applicable to CUDA 12.5 and all previous versions. [4662420]

- 由于持续的测试，我们更新了双精度 lgamma() 函数可能经历大于文档记录的 4 ulp 精度损失的区间范围。新的区间应为 (-23.0001; -2.2637)。这一发现适用于 CUDA 12.5 及所有先前版本。[4662420]

#### 6.5.4. CUDA Math: Release 12.4

#### 6.5.4. CUDA 数学：12.4 版本

## Resolved Issues

## 已解决的问题

- Host-specific code in cuda_fp16/bf16 headers is now free from type-punning and shall work correctly in the presence of optimizations based on strict-aliasing rules. [4311216]

- cuda_fp16/bf16 头文件中的主机特定代码现在不再使用类型双关，并且在使用基于严格别名规则的优化时应能正确工作。[4311216]

#### 6.5.5. CUDA Math: Release 12.3

#### 6.5.5. CUDA 数学：12.3 版本

## New Features

## 新特性

Performance of SIMD Integer CUDA Math APIs was improved.

SIMD 整数 CUDA 数学 API 的性能得到了提升。

## Resolved Issues

## 已解决的问题

- The _____hisinf(   ) Math APIs from cuda_fp16.h and cuda_bf16.h headers were silently producing wrong results if compiled with the $- {std} = c +  + {20}$ compiler option because of an underlying nvcc compiler issue, resolved in version 12.3.

- 由于底层 nvcc 编译器问题，如果使用 $- {std} = c +  + {20}$ 编译器选项编译，cuda_fp16.h 和 cuda_bf16.h 头文件中的 _____hisinf(   ) 数学 API 会静默产生错误结果，该问题已在 12.3 版本中解决。

## Known Issues

## 已知问题

- Users of cuda_fp16.h and cuda_bf16.h headers are advised to disable host compilers strict aliasing rules based optimizations (e.g. pass -fno-strict-aliasing to host GCC compiler) as these may interfere with the type-punning idioms used in the _____half, _____half2, _____nv_bfloat16, _____nv_bfloat162 types implementations and expose the user program to undefined behavior. Note, the headers suppress GCC diagnostics through: `#pragma` GCC diagnostic ignored -Wstrict-aliasing. This behavior may improve in future versions of the headers.

- 建议 cuda_fp16.h 和 cuda_bf16.h 头文件的用户禁用主机编译器的严格别名规则优化（例如，向主机 GCC 编译器传递 -fno-strict-aliasing），因为这些优化可能会干扰 _____half、_____half2、_____nv_bfloat16、_____nv_bfloat162 类型实现中使用的类型双关惯用法，并导致用户程序出现未定义行为。请注意，头文件通过 `#pragma` GCC diagnostic ignored -Wstrict-aliasing 抑制了 GCC 诊断。这种行为可能会在未来的头文件版本中有所改进。

#### 6.5.6. CUDA Math: Release 12.2

#### 6.5.6. CUDA 数学：12.2 版本

## New Features

## 新功能

- CUDA Math APIs for _____half and _____nv_bfloat16 types received usability improvements, including host side <emulated> support for many of the arithmetic operations and conversions.

- CUDA 数学 API 对 _____half 和 _____nv_bfloat16 类型的可用性进行了改进，包括对许多算术操作和转换的主机端 <emulated> 支持。

- _____half and _____nv_bfloat16 types have implicit conversions to/from integral types, which are now available with host compilers by default. These may cause build issues due to ambiguous overloads resolution. Users are advised to update their code to select proper overloads. To opt-out user may want to define the following macros (these macros will be removed in the future CUDA release):

- _____half 和 _____nv_bfloat16 类型与整数类型之间的隐式转换现在默认在主机编译器中可用。这些转换可能会由于重载解析不明确而导致构建问题。建议用户更新代码以选择适当的重载。用户可以选择定义以下宏来禁用这些转换（这些宏将在未来的 CUDA 版本中移除）：

- _____CUDA_FP16_DISABLE_IMPLICIT_INTEGER_CONVERTS_FOR_HOST_COMPILERS_____

- _____CUDA_FP16_DISABLE_IMPLICIT_INTEGER_CONVERTS_FOR_HOST_COMPILERS_____

- _____CUDA_BF16_DISABLE_IMPLICIT_INTEGER_CONVERTS_FOR_HOST_COMPILERS_____

- _____CUDA_BF16_DISABLE_IMPLICIT_INTEGER_CONVERTS_FOR_HOST_COMPILERS_____

## Resolved Issues

## 已解决的问题

- During ongoing testing, NVIDIA identified that due to an algorithm error the results of 64-bit floating-point division in default round-to-nearest-even mode could produce spurious overflow to infinity. NVIDIA recommends that all developers requiring strict IEEE754 compliance update to CUDA Toolkit 12.2 or newer. The affected algorithm was present in both offline compilation as well as just-in-time (JIT) compilation. As JIT compilation is handled by the driver, NVIDIA recommends updating to driver version greater than or equal to R535 (R536 on Windows) when IEEE754 compliance is required and when using JIT. This is a software algorithm fix and is not tied to specific hardware.

- 在持续测试过程中，NVIDIA 发现由于算法错误，64 位浮点除法在默认的舍入到最近偶数模式下可能会产生虚假的无穷大溢出。NVIDIA 建议所有需要严格 IEEE754 合规性的开发者更新到 CUDA Toolkit 12.2 或更高版本。受影响的算法存在于离线编译和即时（JIT）编译中。由于 JIT 编译由驱动程序处理，NVIDIA 建议在需要 IEEE754 合规性并使用 JIT 时更新到大于或等于 R535（Windows 上为 R536）的驱动程序版本。这是一个软件算法修复，与特定硬件无关。

- Updated the observed worst case error bounds for single precision intrinsic functions _____expf(   ),_____exp10f(   ) and double precision functions asinh(   ), acosh(   ).

- 更新了单精度内置函数 _____expf(   )、_____exp10f(   ) 和双精度函数 asinh(   )、acosh(   ) 的观测最坏情况误差界限。

#### 6.5.7. CUDA Math: Release 12.1

#### 6.5.7. CUDA 数学：12.1 版本

## New Features

## 新特性

- Performance and accuracy improvements in atanf, acosf, asinf, sinpif, cospif, powf, erff, and tgammaf.

- 在 atanf、acosf、asinf、sinpif、cospif、powf、erff 和 tgammaf 中进行了性能和精度改进。

#### 6.5.8. CUDA Math: Release 12.0

#### 6.5.8. CUDA 数学：12.0 版本

## New Features

## 新特性

- Introduced new integer/fp16/bf16 CUDA Math APIs to help expose performance benefits of new DPX instructions. Refer to https://docs.nvidia.com/cuda/cuda-math-api/index.html.

- 引入了新的整数/fp16/bf16 CUDA 数学 API，以帮助展示新 DPX 指令的性能优势。请参阅 https://docs.nvidia.com/cuda/cuda-math-api/index.html。

## Known Issues

## 已知问题

- Double precision inputs that cause the double precision division algorithm in the default 'round to nearest even mode' produce spurious overflow: an infinite result is delivered where DBL_MAX 0x7FEF_FFFF_FFFF_FFFF is expected. Affected CUDA Math APIs: _____ddiv_rn(   ). Affected CUDA language operation: double precision / operation in the device code.

- 双精度输入导致默认“舍入到最近偶数模式”中的双精度除法算法产生虚假溢出：在预期 DBL_MAX 0x7FEF_FFFF_FFFF_FFFF 的情况下，返回了无限结果。受影响的 CUDA 数学 API：_____ddiv_rn(   )。受影响的 CUDA 语言操作：设备代码中的双精度 / 操作。

Deprecations

弃用

- All previously deprecated undocumented APIs are removed from CUDA 12.0.

- 所有之前弃用的未记录 API 已从 CUDA 12.0 中移除。

### 6.6. NVIDIA Performance Primitives (NPP)

### 6.6. NVIDIA 性能原语 (NPP)

#### 6.6.1. NPP: Release 12.4

#### 6.6.1. NPP: 版本 12.4

## New Features

## 新特性

- Enhanced large file support with size_t.

- 使用 size_t 增强了大文件支持。

#### 6.6.2. NPP: Release 12.0

#### 6.6.2. NPP: 版本 12.0

Deprecations - Deprecating non-CTX API support from next release. Resolved Issues

弃用 - 从下一个版本开始弃用非 CTX API 支持。已解决的问题

- A performance issue with the NPP ResizeSqrPixel API is now fixed and shows improved performance.

- NPP ResizeSqrPixel API 的性能问题现已修复，并显示出改进的性能。

### 6.7. nvJPEG Library

### 6.7. nvJPEG 库

#### 6.7.1. nvJPEG: Release 12.4

#### 6.7.1. nvJPEG：12.4 版本

## New Features

## 新特性

- IDCT performance optimizations for single image CUDA decode.

- 单图像 CUDA 解码的 IDCT 性能优化。

- Zero Copy behavior has been changed: Setting NVJPEG_FLAGS_REDUCED_MEMORY_DECODE_ZERO_COPY flag will no longer enable NVJPEG_FLAGS_REDUCED_MEMORY_DECODE.

- 零拷贝行为已更改：设置 NVJPEG_FLAGS_REDUCED_MEMORY_DECODE_ZERO_COPY 标志将不再启用 NVJPEG_FLAGS_REDUCED_MEMORY_DECODE。

#### 6.7.2. nvJPEG: Release 12.3 Update 1

#### 6.7.2. nvJPEG：12.3 更新 1

## New Features

## 新特性

- New APIs: nvjpegBufferPinnedResize and nvjpegBufferDeviceResize which can be used to resize pinned and device buffers before using them.

- 新 API：nvjpegBufferPinnedResize 和 nvjpegBufferDeviceResize，可用于在使用之前调整固定和设备缓冲区的大小。

#### 6.7.3. nvJPEG: Release 12.2

#### 6.7.3. nvJPEG：版本 12.2

## New Features

## 新特性

- Added support for JPEG Lossless decode (process 14, FO prediction).

- 增加了对 JPEG 无损解码（过程 14，FO 预测）的支持。

- nvJPEG is now supported on L4T.

- nvJPEG 现在支持 L4T。

#### 6.7.4. nvJPEG: Release 12.0

#### 6.7.4. nvJPEG: 版本 12.0

## New Features

## 新特性

- Immproved the GPU Memory optimisation for the nvJPEG codec.

- 改进了 nvJPEG 编解码器的 GPU 内存优化。

## Resolved Issues

## 已解决的问题

- An issue that causes runtime failures when nvJPEGDecMultipleInstances was tested with a large number of threads is resolved.

- 当 nvJPEGDecMultipleInstances 使用大量线程进行测试时导致运行时失败的问题已解决。

- An issue with CMYK four component color conversion is now resolved.

- CMYK 四分量颜色转换的问题现已解决。

## Known Issues

## 已知问题

- Backend NVJPEG_BACKEND_GPU_HYBRID - Unable to handle bistreams with extra scans lengths. Deprecations

- 后端 NVJPEG_BACKEND_GPU_HYBRID - 无法处理具有额外扫描长度的比特流。弃用

- The reuse of Huffman table in Encoder (nvjpegEncoderParamsCopyHuffmanTables). Release Notes, Release 12.6

- 在编码器中重用霍夫曼表 (nvjpegEncoderParamsCopyHuffmanTables)。发布说明，版本 12.6

## Chapter 7. Notices

## 第7章 声明

### 7.1. Notice

### 7.1. 声明

This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation ("NVIDIA") makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.

本文档仅供参考，不应被视为对产品功能、状况或质量的保证。NVIDIA Corporation（“NVIDIA”）不对本文档中包含的信息的准确性或完整性作出任何明示或暗示的声明或保证，并且对本文档中的任何错误不承担任何责任。NVIDIA 对因使用此类信息而产生的后果或使用不承担任何责任，也不对因使用此类信息而可能导致的专利或其他第三方权利的侵犯承担任何责任。本文档不承诺开发、发布或交付任何材料（定义见下文）、代码或功能。

NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.

NVIDIA保留随时对本文件进行更正、修改、增强、改进和任何其他更改的权利，恕不另行通知。

Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.

客户在下订单前应获取最新的相关信息，并应验证这些信息是最新且完整的。

NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document.

除非在由NVIDIA和客户授权代表签署的个别销售协议中另有约定，否则NVIDIA产品的销售受NVIDIA在订单确认时提供的标准销售条款和条件的约束（“销售条款”）。NVIDIA特此明确反对将任何客户的一般条款和条件应用于本文件中引用的NVIDIA产品的购买。本文件不直接或间接形成任何合同义务。

NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer's own risk.

NVIDIA产品并非设计、授权或保证适用于医疗、军事、航空、航天或生命支持设备，也不适用于NVIDIA产品的故障或失灵可能合理预期导致人身伤害、死亡或财产或环境损害的应用。NVIDIA对此类设备或应用中包含和/或使用NVIDIA产品不承担任何责任，因此此类包含和/或使用由客户自行承担风险。

NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer's sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer's product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.

NVIDIA不声明或保证基于本文件的产品适用于任何特定用途。NVIDIA不一定对每个产品的所有参数进行测试。客户有责任评估和确定本文件中包含的任何信息的适用性，确保产品适合并符合客户计划的应用，并为应用执行必要的测试，以避免应用或产品的故障。客户产品设计中的弱点可能影响NVIDIA产品的质量和可靠性，并可能导致超出本文件包含的额外或不同的条件和/或要求。NVIDIA对任何可能基于或归因于以下情况的故障、损坏、成本或问题不承担任何责任：(i) 以与本文件相反的方式使用NVIDIA产品或(ii) 客户产品设计。

No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.

本文件不授予任何NVIDIA专利权、版权或其他NVIDIA知识产权的明示或暗示许可。NVIDIA发布的关于第三方产品或服务的信息不构成NVIDIA对此类产品或服务的许可、保证或认可。使用此类信息可能需要获得第三方在专利或其他知识产权下的许可，或获得NVIDIA在专利或其他知识产权下的许可。

Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.

本文件中的信息复制仅在前述获得NVIDIA书面批准、未经更改且完全遵守所有适用的出口法律和法规、并附有所有相关条件、限制和通知的情况下才被允许。

THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, "MATERIALS") ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA's aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product.

本文件及所有NVIDIA设计规范、参考板、文件、图纸、诊断、列表和其他文件（统称及分别称为“材料”）均按“原样”提供。NVIDIA不对材料作出任何明示、暗示、法定或其他形式的保证，并明确否认所有关于不侵权、适销性和特定用途适用性的暗示保证。在法律允许的范围内，NVIDIA对因使用本文件引起的任何损害（包括但不限于任何直接、间接、特殊、附带、惩罚性或后果性损害）概不负责，无论损害是如何造成的，也无论责任理论如何，即使NVIDIA已被告知可能发生此类损害。尽管客户可能因任何原因遭受任何损害，NVIDIA对客户因本文所述产品而产生的总责任和累计责任应按照产品销售条款中的规定进行限制。

### 7.2. OpenCL

### 7.2. OpenCL

OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.

OpenCL是Apple Inc.的商标，经Khronos Group Inc.许可使用。

### 7.3. Trademarks

### 7.3. 商标

NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated.

NVIDIA和NVIDIA标志是NVIDIA Corporation在美国和其他国家的商标或注册商标。其他公司和产品名称可能是其各自关联公司的商标。

## Copyright

## 版权

©2007-2024, NVIDIA Corporation & affiliates. All rights reserved

©2007-2024, NVIDIA Corporation & affiliates. 保留所有权利