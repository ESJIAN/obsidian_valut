爬虫的初学者们，只看这一篇就够了，看到就是赚到！

**目录**

[1.爬虫简介](https://blog.csdn.net/m0_61903191/article/details/138548164#1.%E7%88%AC%E8%99%AB%E7%AE%80%E4%BB%8B)

[2.版本及库的要求](https://blog.csdn.net/m0_61903191/article/details/138548164#t0)

[3.爬虫的框架](https://blog.csdn.net/m0_61903191/article/details/138548164#t1)

[4.HTML简介](https://blog.csdn.net/m0_61903191/article/details/138548164#t2)

[5.爬虫库及演示](https://blog.csdn.net/m0_61903191/article/details/138548164#t3)

  [（1）requests库（网页下载器）](https://blog.csdn.net/m0_61903191/article/details/138548164#5.%E7%88%AC%E8%99%AB%E6%89%80%E9%9C%80%E5%BA%93)

  [（2）BeautifulSoup库（网页解析器）](https://blog.csdn.net/m0_61903191/article/details/138548164#%EF%BC%882%EF%BC%89BeautifulSoup%E5%BA%93)

[6.爬虫框架补充](https://blog.csdn.net/m0_61903191/article/details/138548164#t4)

  [（1）URL管理模块](https://blog.csdn.net/m0_61903191/article/details/138548164#5.%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6)

[7.对目标网站进行解析](https://blog.csdn.net/m0_61903191/article/details/138548164#t5)

[8.举个栗子](https://blog.csdn.net/m0_61903191/article/details/138548164#t6)

  [（1）所需库](https://blog.csdn.net/m0_61903191/article/details/138548164#8.%E4%B8%BE%E4%B8%AA%E6%A0%97%E5%AD%90)

  [（2）爬取目标](https://blog.csdn.net/m0_61903191/article/details/138548164#%EF%BC%882%EF%BC%89%E7%88%AC%E5%8F%96%E7%9B%AE%E6%A0%87)

  [（3）网页分析](https://blog.csdn.net/m0_61903191/article/details/138548164#%EF%BC%883%EF%BC%89%E7%BD%91%E9%A1%B5%E5%88%86%E6%9E%90)

  [（4）编写爬虫前的准备](https://blog.csdn.net/m0_61903191/article/details/138548164#%EF%BC%884%EF%BC%89%E7%BC%96%E5%86%99%E7%88%AC%E8%99%AB%E5%89%8D%E7%9A%84%E5%87%86%E5%A4%87)

  [（5）编写代码](https://blog.csdn.net/m0_61903191/article/details/138548164#%EF%BC%885%EF%BC%89%E7%BC%96%E5%86%99%E4%BB%A3%E7%A0%81)

[9.参考资料](https://blog.csdn.net/m0_61903191/article/details/138548164#t7)

---

## 1.爬虫简介


网络爬虫（又被称为网页蜘蛛，网络机器人）就是模拟浏览器发送网络请求，接收请求响应，一种按照一定的规则，自动地抓取互联网信息的程序。  
原则上,只要是浏览器(客户端)能做的事情，爬虫都能够做。

==为什么我们要使用爬虫==

互联网大数据时代，给予我们的是生活的便利以及海量数据爆炸式的出现在网络中。  
过去，我们通过书籍、报纸、电视、广播或许信息，这些信息数量有限，且是经过一定的筛选，信息相对而言比较有效，但是缺点则是信息面太过于狭窄了。不对称的信息传导，以致于我们视野受限，无法了解到更多的信息和知识。  
互联网大数据时代，我们突然间，信息获取自由了，我们得到了海量的信息，但是大多数都是无效的垃圾信息。  
例如新浪微博，一天产生数亿条的状态更新，而在百度搜索引擎中，随意搜一条——减肥100,000,000条信息。  
在如此海量的信息碎片中，我们如何获取对自己有用的信息呢？  
答案是筛选！  
通过某项技术将相关的内容收集起来，在分析删选才能得到我们真正需要的信息。  
这个信息收集分析整合的工作，可应用的范畴非常的广泛，无论是生活服务、出行旅行、金融投资、各类制造业的产品市场需求等等……都能够借助这个技术获取更精准有效的信息加以利用。  
网络爬虫技术，虽说有个诡异的名字，让能第一反应是那种软软的蠕动的生物，但它却是一个可以在虚拟世界里，无往不前的利器。

## 2.版本及库的要求

**2.1**python使用版本：3.7.9

**2.2**爬虫所需库

        [1]resquests

        [2]Beautifulsoup4

        [3]selenium

ps：上述库可通过

> pip install requests

直接安装，安装完在pycharm新建test.py文件，输入一下代码进行验证，查看库是否安装成功

```haskell
import requestsfrom bs4 import BeautifulSoupimport selenium print("ok")
```

若安装成功则输出如下图所示：

![](https://i-blog.csdnimg.cn/blog_migrate/06a7add5433273dc5e82924033b4bcef.png)

## 3.爬虫的框架

![](https://i-blog.csdnimg.cn/blog_migrate/ce8ae925f6c44e6964cd9e57f1206681.png)

(1)   URL管理器：相当于一个数据容器，其中包含待爬取及爬取过的url。

        模块作用：防止爬虫重复爬取页面，或循环爬取陷入死循环中。

（2）网页下载器：用python获取网页的信息。

        模块作用：爬取页面信息。复杂页面的爬取，动态页面，需要登录验证码的页面，或者有一些防御手段禁止爬取的页面，都需要在此模块中解决。（一般网页使用动态加载数据，需要使用到selenium框架解决。）

（3）网页解析器：获取网页下载器下载下来的HTML，提取其中有价值的信息和待爬取的新URL，传输给URL管理器，循环爬取数据。

（4）并发爬取，在爬虫调度端实现。

（5）数据存储，像将数据保存在数据库中，统一在价值数据模块中实现。


## 4.HTML简介

这部分内容简单看一下

想深入了解看可以看看[HTML 教程](http://www.w3school.com.cn/html/index.asp "HTML 教程")

![](https://i-blog.csdnimg.cn/blog_migrate/607153724334ef6eadfd2df743bef7e5.png)

![](https://i-blog.csdnimg.cn/blog_migrate/58cfcaba5b9bcbc2e6259f574f0f4cea.png)

## 5.爬虫库及演示

#### （1）[requests库]（网页下载器）

一个优雅的、简单的Python、HTTP库，常常用于爬虫对网页内容的下载。

![](https://i-blog.csdnimg.cn/blog_migrate/56a92a3f2062b7b0c1a1c1835fbc3e19.png)

**requests库用法：**

![](https://i-blog.csdnimg.cn/blog_migrate/b695a02fc45d239fbe615c1a37d8e61a.png)


![](https://i-blog.csdnimg.cn/blog_migrate/3417eafc98ae510bfba924efbfa48e7f.png)

以下代码可以直接在pycharm中运行，看看返回结果

代码段1：

```python
import requests
 
url = "http://www.crazyant.net"
 
r = requests.get(url)
 
print(r.status_code)
print(r.headers)
print(r.encoding)
print(r.text)
print(r.cookies)
```

 代码段2：

```python
import requests
 
#url = “http://www.crazyant.net”
#url = "http://www.baidu.com"
url = "http://www.httpcn.com"
 
r = requests.get(url)
#r.encoding = "UTF-8"
 
print(r.status_code)
print(r.headers)
print(r.encoding)
print(r.text)
print(r.cookies)
```

当编码不规范时，爬取的中文信息中可能会出现乱码，此时需要我们手动更改编码信息

```python
import requests
 
#url = “http://www.crazyant.net”
#url = "http://www.baidu.com"
url = "http://www.httpcn.com"
 
r = requests.get(url)
r.encoding = "UTF-8"
 
print(r.status_code)
print(r.headers)
print(r.encoding)
print(r.text)
print(r.cookies)
```

更改前：  
![](https://i-blog.csdnimg.cn/blog_migrate/25ecf3ac70d01a1bfadda7ea23a5db16.png)

更改后：  
![](https://i-blog.csdnimg.cn/blog_migrate/87565397dd8fda97e8f65278f4160de3.png)

#### （2）BeautifulSoup库（网页解析器）

**BeautifulSoup库官网**：[Beautiful Soup: We called him Tortoise because he taught us.](https://www.crummy.com/software/BeautifulSoup/ "Beautiful Soup: We called him Tortoise because he taught us.")

![](https://i-blog.csdnimg.cn/blog_migrate/7c0407aaf4720979c7236b226dcd5a59.png)

![](https://i-blog.csdnimg.cn/blog_migrate/2ad72330de15c462f8543a65e98337c6.png)

![](https://i-blog.csdnimg.cn/blog_migrate/197836e36586034d54b0c36496549e75.png)

![](https://i-blog.csdnimg.cn/blog_migrate/f702f357950fd11a6ab8a81f3c998fe1.png)

 方便练习，我们先自己写一个简单的网页

```html
<html>
<head>
    <meta http-equiv=Content-Type content="text/html;charset=utf-8">
    <title>网页标题</title>
</head>
<body>
    <h1>标题1</h1>
    <h2>标题2</h2>
    <h3>标题3</h3>
    <h4>标题4</h4>
 
    <div id="content" class="default">
        <p>段落</p>
        <a href="http://www.baidu.com">百度</a> </br>
        <a href="http://www.crazyyant.net">疯狂的蚂蚁</a> </br>
        <a href="http://www.iqiyi.com">爱奇艺</a> </br>
        <img src="http://www.python.org/static/img/python-logo.png"/>
    </div>
</body>
</html>
```

运行之后长这样：

![](https://i-blog.csdnimg.cn/blog_migrate/3cfb421edf77bd4c3b28036e33ff6d12.png)

**爬取上述简单网页：**

创建一个Python Package，命名为bs4_test

把上述网页放在Package下，命名为test.html

再在Package中创建一个test.py文件

结构如下图：

![](https://i-blog.csdnimg.cn/blog_migrate/7eb63c3af8af3daab3375437bce1b39b.png)

test.py文件中的代码：

```python
from bs4 import BeautifulSoup
 
with open("./test.html", encoding="utf-8") as fin:
#打开当前目录下的HTML
    html_doc = fin.read()
    #读取HTML中的内容作为一个字符串
 
soup = BeautifulSoup(html_doc, "html.parser")
#创建一个BeautifulSoup对象
 
div_node = soup.find("div", id="content")
#指定查找模块的id为content
print(div_node)
print("#" * 30)
 
links = soup.find_all("a")
#查找所有用的a节点
for link in links:
    print(link.name, link["href"], link.get_text())
#循环遍历a节点，并将遍历的a节点名称、href属性、文字内容打印出来
 
img = soup.find("img")
print(img["src"])
#打印图片地址
```

运行结果：

![](https://i-blog.csdnimg.cn/blog_migrate/0ed04210a899ae6780fcda66b9dfa050.png)

## 6.[爬虫框架](https://so.csdn.net/so/search?q=%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6&spm=1001.2101.3001.7020)补充

#### （1）URL管理模块

![](https://i-blog.csdnimg.cn/blog_migrate/7dc2c200864435a04d656b353b169b46.png)

url管理器的实现代码：

```python
class UrlManager():
    '''
    url管理器
    '''
 
    def __init__(self):
    #定义一个初始化函数
        self.new_urls = set()
        #新的待爬取url的集合
        self.old_urls = set()
        #已爬取url的集合
 
    def add_new_url(self, url):
    #定义新增单个url的方法一,传一个参数url
        if url is None or len(url) == 0:
        #判断url是否为空或长度为0
            return
            #符合上述条件就停止增加
        if url in self.new_urls or url in self.old_urls:
        #判断url是否已经被记录在集合里
            return
            #已经载集合里的url不新增
        self.new_urls.add(url)
        #上述干扰条件排除后，url就可以加入待爬取的集合中
 
    def add_new_urls(self,urls):
    #定义新增url的方法二，传一个参数urls
        if urls is None or len(urls) == 0:
        #判断参数urls是否为空
            return
        for url in urls:
        #不为空就将单个url循环传入单个判断url方法中经行判断存储
            self.add_new_url(url)
 
    def get_url(self):
    #定义获取新url的函数
        if self.has_new_url():
        #如果存在待爬取的url
            url = self.new_urls.pop()
            #就将待爬取的url从集合中移除并返回
            self.old_urls.add(url)
            #将移除的url加入已爬取的集合中
            return url
            #并将其返回
        else:
            return None
 
    def has_new_url(self):
    #定义一个判断url是否存在等待爬取的url
        return len(self.new_urls) > 0
        #如果待爬取集合中有元素就分返回这个集合
 
'''测试代码'''
if __name__ == "__main__":
#文件内置变量，仅在执行当前文件时可用。当此文件被调用时，此出变量不会被执行。因此测试代码时一般加上这句话
    url_manger = UrlManager()
    #调用整个类
 
    url_manger.add_new_url("url1")
    url_manger.add_new_urls(["url1", "url2"])
    #故意增加一个重复的url
    print(url_manger.new_urls, url_manger.old_urls)
 
    print("#" * 30)
    new_url = url_manger.get_url()
    print(url_manger.new_urls, url_manger.old_urls)
 
    print("#" * 30)
    new_url = url_manger.get_url()
    print(url_manger.new_urls, url_manger.old_urls)
 
    print("#" * 30)
    print(url_manger.has_new_url())
```

## 7.对目标网站进行解析

这点需要听课（作者用的不是Windows系统，有点不一样，不影响）

[[2.8]--爬虫前提之对目标网站进行分析_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1CY411f7yh?p=12&vd_source=0daf667a7e8233ed99128f48da309d57 "[2.8]--爬虫前提之对目标网站进行分析_哔哩哔哩_bilibili")

## 8.举个栗子

###### （1）所需库

        [1]requests

        [2]BeautifulSoup

###### （2）爬取目标

        网页的所有标题的文本和链接

###### （3）网页分析

        打开检查，鼠标点击到标题，定位到它的源码

![](https://i-blog.csdnimg.cn/blog_migrate/ea93d598973973ef372b7d92d0517694.png)

 可以看到标提包裹在h2标签中，class="entry-title"

![](https://i-blog.csdnimg.cn/blog_migrate/1f52f26b72b8215b9ef8302fb1b21e59.png)

往下多看几个，发现都是这样的

![](https://i-blog.csdnimg.cn/blog_migrate/e33d330e15cf11e86864497360dd4f06.png)

![](https://i-blog.csdnimg.cn/blog_migrate/01de3559f1d6e057a6200012c55098fe.png)

我们就可以指定查找模块h2，class为class="entry-title"

###### （4）编写爬虫前的准备

        [1]新建一个python package，命名为blog_test

        [2]新建一个main.py文件

###### （5）编写代码

```python
import requests
from bs4 import BeautifulSoup
 
url = "http://www.crazyant.net/"
 
r = requests.get(url)
if r.status_code != 200:
    raise Exception()
    #程序出现问题，退出程序
 
html_doc = r.text
#获取文本
soup = BeautifulSoup(html_doc, "html.parser")
#定义变量
 
h2_nodes = soup.find_all("h2", class_="entry-title")
#找到h2，class_="enter-title"
 
for h2_node in h2_nodes:
     #h2_node是h2_nodes中的一条信息
    link = h2_node.find("a")
    #在查找的信息内，找到a标签的内容，传递给参数link
    print(link["href"], link.get_text())
    #输出a标签中herf属性，和文本内容
```

## 9.参考资料

B站视频：

[python爬虫入门实战案例教程-入门到精通（收藏版）_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1CY411f7yh/?spm_id_from=333.337.search-card.all.click&vd_source=0daf667a7e8233ed99128f48da309d57 "python爬虫入门实战案例教程-入门到精通（收藏版）_哔哩哔哩_bilibili")

爬虫简介参考：

[搜索智能精选](https://answer.baidu.com/answer/land?params=sAtsiFV7gpUJt01JF6tlkIncfQF%2BpjcnQJ4hWXv6LTBqATXD6%2BIYHIyrAhXKCpCbHkUIO6WJCDU6WwaPh36WCvE382pwpfYWDhcSrTlebJC3ohXCGsDAtdHl1ySFjAjdQwTrE%2Fty2L4aXBiFmzC1K6FDNcOiJgfaz9tLaEvVsc12wmN8imi%2B7DIDpiv7nscmP8kRqEKTEs0SigkeMWsniw%3D%3D&from=dqa&lid=8253bced007c735d&word=%E7%88%AC%E8%99%AB%E7%9A%84%E5%AE%9A%E4%B9%89 "搜索智能精选")

Python部分语法：

[Python 中的 if __name=='__main__'作用浅析 - 简书](https://www.jianshu.com/p/e9cbca23f67f "Python 中的 if __name=='__main__'作用浅析 - 简书")