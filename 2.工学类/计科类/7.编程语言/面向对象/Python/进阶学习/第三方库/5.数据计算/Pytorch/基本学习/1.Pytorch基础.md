![](https://i-blog.csdnimg.cn/blog_migrate/3e8bd010e1f9a4e39ad36406029fd3cf.jpeg)


- # 1.  本库犯错



# 一、Pytorch

## 1.1 简介

Pytorch是torch的python版本，是由Facebook开源的神经网络框架，专门针对 GPU 加速的深度神经网络（DNN）编程。Torch 是一个经典的对多维矩阵数据进行操作的张量（tensor ）库，在机器学习和其他数学密集型应用有广泛应用。与Tensorflow的静态计算图不同，pytorch的计算图是动态的，可以根据计算需要实时改变计算图。但由于Torch语言采用 Lua，导致在国内一直很小众，并逐渐被支持 Python 的 Tensorflow 抢走用户。作为经典机器学习库 Torch 的端口，PyTorch 为 Python 语言使用者提供了舒适的写代码选择。

至于为什么推荐使用Pytorch，我想最主要的原因就是它非常的简洁，非常符合Python的风格。

## 1.2 安装

首先确保你已经安装了GPU环境，即Anaconda、CUDA和CUDNN

随后进入Pytorch官网[PyTorch](https://pytorch.org/ "PyTorch")

官网会自动显示符合你电脑配置的Pytorch版本，复制指令到conda环境中运行即可 

![](https://i-blog.csdnimg.cn/blog_migrate/9e82b0daef2eb2290e620d88d43998a0.png)

测试是否安装成功 

```python
import torch print(torch.__version__) # pytorch版本print(torch.version.cuda) # cuda版本print(torch.cuda.is_available()) # 查看cuda是否可用
```

![](https://i-blog.csdnimg.cn/blog_migrate/abbd77dc5a949fd3946055da29d04b1e.png)

若想要安装 CPU 版本，命令类似如下

```bash
pip install torch==2.0.1+cpu torchvision==0.15.2+cpu torchaudio==2.0.2+cpu -f https://download.pytorch.org/whl/cpu/torch_stable.html
```

# 二、Tensor

Tensor张量是Pytorch里最基本的数据结构。直观上来讲，它是一个多维矩阵，支持GPU加速，其基本数据类型如下

|          |                    |                         |
| -------- | ------------------ | ----------------------- |
| 数据类型     | CPU tensor         | GPU tensor              |
| 8位无符号整型  | torch.ByteTensor   | torch.cuda.ByteTensor   |
| 8位有符号整型  | torch.CharTensor   | torch.cuda.CharTensor   |
| 16位有符号整型 | torch.ShortTensor  | torch.cuda.ShortTensor  |
| 32位有符号整型 | torch.IntTensor    | torch.cuda.IntTensor    |
| 64位有符号整型 | torch.LongTensor   | torch.cuda.LonfTensor   |
| 32位浮点型   | torch.FloatTensor  | torch.cuda.FloatTensor  |
| 64位浮点型   | torch.DoubleTensor | torch.cuda.DoubleTensor |
| 布尔类型     | torch.BoolTensor   | torch.cuda.BoolTensor   |

## 2.1 Tensor创建

![](https://i-blog.csdnimg.cn/blog_migrate/21d989ab87a48a2e8a90c82b729f34b5.png)

### 2.1.1 torch.tensor() && torch.tensor([])

二者的主要区别在于创建的对象的size和value不同

![](https://i-blog.csdnimg.cn/blog_migrate/63cd68e0dc04aae54cf0d9a68400a4ee.png)

### 2.1.2 torch.randn && torch.randperm

生成的数据类型为浮点型，与numpy.randn生成随机数的方法类似，生成的浮点数的取值满足均值为0，方差为1的正态分布

![](https://i-blog.csdnimg.cn/blog_migrate/a4b74ba6f1149dafa925423e37e9ec3b.png)

torch.randpern(n)为创建一个n个整数，随机排列的Tensor

![](https://i-blog.csdnimg.cn/blog_migrate/e8fa69c7e69bc90984aa78b384814d24.png)

### 2.1.3 torch.range(begin，end，step)

生成一个一维的Tensor，三个参数分别的起始位置，终止位置和步长

![](https://i-blog.csdnimg.cn/blog_migrate/eee1bc826cb4923463f38c590c41e3f7.png)

### 2.1.4 指定numpy

很多时候我们需要创建指定的Tensor，而numpy就是一个很好的方式

![](https://i-blog.csdnimg.cn/blog_migrate/1d94dcd040a08e702162a4cd6f50352c.png)
## 2.2Tensor文件操作
### 2.2.1存储
1. **保存为`.pt`或`.pth`文件**： 使用PyTorch内置的保存和加载机制是最直接的方式。这允许你保存整个`Tensor`的状态，包括其数据类型、形状和设备（如CPU或GPU）。
    
    ``` python
    import torch
    
    # 假设你有一个名为 x 的 Tensor
    x = torch.tensor([[1., 2.], [3., 4.]])
    
    # 保存 Tensor 到磁盘
    torch.save(x, 'tensor.pth')
    ```
    
2. **保存为`.npy`文件**： 如果你希望使用NumPy的格式，可以将`Tensor`转换为NumPy数组，然后使用NumPy的`save`或`savez`函数保存。
    
    ``` python
    import torch
    import numpy as np
    
    x = torch.tensor([[1., 2.], [3., 4.]])
    np.save('tensor.npy', x.numpy())
    ```
    
3. **保存为CSV文件**： 如果`Tensor`是二维的，可以将其转换为NumPy数组，然后使用`numpy.savetxt`保存为CSV格式，或者使用Pandas的`DataFrame.to_csv`。
    
    
    ``` python
    import torch
    import pandas as pd
    
    x = torch.tensor([[1., 2.], [3., 4.]])
    df = pd.DataFrame(x.numpy())
    df.to_csv('tensor.csv', index=False, header=False)
    ```
    
4. **保存为HDF5文件**： HDF5是一种高性能的数据存储格式，适合存储大型多维数组。可以使用`h5py`库将`Tensor`转换为NumPy数组并保存。
    
    python
    
    深色版本
    
    ``` python 
    import torch
    import h5py
    
    x = torch.tensor([[1., 2.], [3., 4.]])
    with h5py.File('tensor.h5', 'w') as f:
        f.create_dataset('data', data=x.numpy())
    ```
    
5. **保存为JSON文件**： 对于小型和低维的`Tensor`，可以将其转换为Python列表，然后使用`json`模块保存为JSON格式。需要注意的是，`Tensor`可能包含非JSON可序列化的类型，如`float32`，因此可能需要转换为`float`。
    
    python
    
    
    深色版本
    
    ``` python
    import torch
    import json
    
    x = torch.tensor([[1., 2.], [3., 4.]])
    with open('tensor.json', 'w') as f:
        json.dump(x.tolist(), f)
    ```
### 2.2.2读取
1. **从`.pt`或`.pth`文件读取**：
    
     
    
    如果你使用`torch.save`保存了一个张量或模型状态字典，你可以使用`torch.load`来读取它。例如，如果你保存了一个模型的state_dict：
    
    python
    
    深色版本
    
    ```python
    # 保存模型
    torch.save(model.state_dict(), 'model.pth')
    
    # 加载模型
    torch.load('model.pth')
    ```  
    如果保存的是一个单独的张量：
    ```python
    # 保存张量
    torch.save(tensor, 'tensor.pth')
    
    # 加载张量
    loaded_tensor = torch.load('tensor.pth')
    ```
    
2. **从`.npy`文件读取**：
    如果你将张量转换为NumPy数组并保存为`.npy`文件，你可以使用NumPy的`load`函数读取它，然后将其转换回张量：
    ```python
    import numpy as np
    import torch
    
    # 加载NumPy数组
    np_array = np.load('tensor.npy')
    
    # 将NumPy数组转换为张量
    tensor = torch.from_numpy(np_array)
    ```
    
3. **从CSV文件读取**：
    如果你将张量保存为CSV文件，你可以使用Pandas读取数据，然后转换为张量：
    ```python
    import pandas as pd
    import torch
    
    # 读取CSV文件
    df = pd.read_csv('tensor.csv', header=None)
    
    # 将DataFrame转换为张量
    tensor = torch.tensor(df.values)
    ```
    
4. **从HDF5文件读取**：
    
     
    
    如果你使用`h5py`库保存了张量，你可以使用同样的库来读取它：
    
    python
    
    深色版本
    
    ```python
    import h5py
    import torch
    
    # 打开HDF5文件
    with h5py.File('tensor.h5', 'r') as f:
        np_array = f['data'][:]
    
    # 将NumPy数组转换为张量
    tensor = torch.from_numpy(np_array)
    ```
    
5. **从JSON文件读取**：
    
     
    
    如果你将张量转换为列表并保存为JSON文件，你可以读取JSON文件并将其转换回张量：
    
    python
    
    深色版本
    
    ```python
    import json
    import torch
    
    # 读取JSON文件
    with open('tensor.json', 'r') as f:
        list_data = json.load(f)
    
    # 将列表转换为张量
    tensor = torch.tensor(list_data)
    ```



## 2.3 Tensor运算

|                           |                                                           |
| ------------------------- | --------------------------------------------------------- |
| 函数                        | 作用                                                        |
| torch.abs(A)              | 绝对值                                                       |
| torch.add(A，B)            | 相加，A和B既可以是Tensor也可以是标量                                    |
| torch.clamp(A，max，min)    | 裁剪，A中的数据若小于min或大于max，则变成min或max，即保证范围在[min，max]           |
| torch.div(A，B)            | 相除，A%B，A和B既可以是Tensor也可以是标量                                |
| torch.mul(A，B)            | 点乘，A*B，A和B既可以是Tensor也可以是标量                                |
| torch.pow(A,n)            | 求幂，A的n次方                                                  |
| torch.mm(A,B.T)           | 矩阵叉乘，注意与torch.mul之间的区别                                    |
| torch.mv(A,B)             | 矩阵与向量相乘，A是矩阵，B是向量，这里的B需不需要转置都是可以的                         |
|                           |                                                           |
| A.item()                  | 将Tensor转化为基本数据类型，注意Tensor中只有一个元素的时候才可以使用，一般用于在Tensor中取出数值 |
| A.numpy()                 | 将Tensor转化为Numpy类型                                         |
| A.size()                  | 查看尺寸                                                      |
| A.shape                   | 查看尺寸                                                      |
| A.dtype                   | 查看数据类型                                                    |
| A.view()                  | 重构张量尺寸，类似于Numpy中的reshape                                  |
| A.transpose(0，1)          | 行列交换                                                      |
| A[1:]<br><br>A[-1，-1]=100 | 切面，类似Numpy中的切面                                            |
| A.zero_()                 | 归零化                                                       |
| torch.stack((A，B)，sim=-1) | 拼接，升维                                                     |
| torch.diag(A)             | 取A对角线元素形成一个一维向量                                           |
| torch.diag_embed(A)       | 将一维向量放到对角线中，其余数值为0的Tensor                                 |

### 2.2.1 A.add() && A.add_()

所有的带_符号的函数都会对原数据进行修改

![](https://i-blog.csdnimg.cn/blog_migrate/5ef103e66c9d8766b4fb012f3e2f88c1.png)

### 2.2.2 torch.stack

stack为拼接函数，函数的第一个参数为需要拼接的Tensor，第二个参数为细分到哪个维度

```python
A=torch.IntTensor([[1,2,3],[4,5,6]])
B=torch.IntTensor([[7,8,9],[10,11,12]])
C1=torch.stack((A,B),dim=0) # or C1=torch.stack((A,B))
C2=torch.stack((A,B),dim=1)
C3=torch.stack((A,B),dim=2)
C4=torch.stack((A,B),dim=-1)
print(C1,C2,C3,C4)
```

![](https://i-blog.csdnimg.cn/blog_migrate/859d32e9b0a8f587fdd500e697cbf9c3.png)

```
dim=0，C1 = [ A,B ]

dim=1，C2 = [ [ A[0],B[0] ] , [ A[1],B[1] ] ]

dim=2，C3 = [ [ [ A[0][0],B[0][0] ] , [ A[0][1],B[0][1] ] , [ A[0][2],B[0][2] ] ],

                        [ [ A[1][0],B[1][0] ] , [ A[1][1],B[1][1] ] , [ A[1][2],B[1][2] ] ] ]

dim=-1，C4 = C3
```
## 2.5Tensor基本操作

### 2.3.1 Tensor的索引和切片

**汇总：**

| Name                                           | Out                                               |
| ---------------------------------------------- | ------------------------------------------------- |
| a[i, j, k, …] = a[i][j][k][…]                  | 获取张量a的具体数据                                        |
| a[start : end : step, start1 : end1 : step1, ] | 获取张量a第一维[start, end)步长为step的数据，第二维类似，**详情看例子1-3** |
| a[…, start : end : step]                       | 获取张量a前面维度所有维度，只针对最后维度数据进行切片，**详情看例子4**            |
| 布尔索引                                           | **详情看例子5**                                        |
| 整数索引                                           | **详情看例子6**                                        |
| torch.nonzero()                                | **获得数据中非0的位置 详情看例子7**                             |
| torch.where(condition, x, y)                   | **conditon成立是，使用x中的数据，否则使用y中的数据，见例子8**            |

`a为高维张量`

**例子：**  
获取张量中的一个具体数据:

```python
import torch
x = torch.randint(0,24, (2, 3, 4))
print(x)
print(x[1,2,3], x[1][2][3], x[1][2])

out:
tensor([[[ 3, 16, 17, 16],
         [ 0, 20, 15, 19],
         [23, 16,  3, 11]],

        [[20, 20, 18,  0],
         [13,  6, 19, 15],
         [ 3, 20,  0, 18]]])
tensor(18) tensor(18) tensor([ 3, 20,  0, 18])
```
附：[randint方法解释](https://tongyi.aliyun.com/qianwen/share?shareId=348c9c06-a33a-4135-94bc-08b2d3e7ff36)
**切片**  
以前也知道，但是一直没太搞清楚，这次重新学了一下，大概把规律都掌握了：  
一般都是使用<label class="ob-comment" title="" style=""> **start : end : step** <input type="checkbox"> <span style=""> 中括号内有多少个逗号，就指定了哪些索引维度，每次加入一个逗号，就限定了一个索引规则，逗号以内的分号情况又决定了切片的方式 </span></label>来表示获取的数据范围，范围是前闭后开，step表示步长，Pytorch不支持负步长，但是支持负索引，-1指的是该维度下的最后一个索引，结合张量索引的知识点，你把他放在那个维度上，它就对那个维度起作用。  
**例子1：含有步长的切片**

```python
x = torch.arange(0,24).view(2,3,4) 
  创建了一个从 0 到 23 的一维张量，然后将其重塑为一个三维张量，形状为 2x3x4，即两个“深   度切片”（2），每个深度切片有三行（3），每行有四列（4）
print(x)
print(x[:,0:3:2, ])  
  是一个多维索引操作，其中 `:` 表示“所有”，而 `0:3:2` 是一个切片操作。
- 第一个索引 `:` 意味着我们保留所有的第一个维度（深度切片），也就是保留所有2个深度切片。
    
- 第二个索引 `0:3:2`从后往前译：表示我们从第二个维度（行）中获取索引为0开始，步长为2的元素，直到索引3（但不包括3）。这意味着我们将获得第0行和第2行。
    
- 最后一个索引 `:` 同样意味着我们保留第三个维度（列）的所有元素。
```
#### 特性理解：
==关于不完全索引表达式==：
	x[0,0,0]  :锁定0,0,0的元素
	x[1:,0,0]  会被补偿成x[1:0:0,0,0]
   总结：
   - 没有被补齐的会补齐，取修改的和默认值的并集作为最终传参使用值
   - 不允许类似rotate_matrix_4[i, , ]的索引结构，分号不能随便省，rotate_matrix_4[i, :, ]为合法
==关于切片造成的变量维度变化==：
	x[:,0:3:2, ] --补全为-->x[:,0:3:2,:]解读为遍历所有第一维的数组，提取每个第一维中第二维从第0个索引到3个索引步进为2的二维行，每一行的所有元素都保留。然后从上到下对原矩阵按此规则提取元素组成新矩阵
	x[:,0:3:2, 0] ：遍历所有第一维，取第二维度0到3行步进为2的数列，
   **解读总结：**
	   **·无逗号会被补全逗号**
	   **·逗号间无数字会被补分号，表示遍历该维度所有**
   **解读步骤：**
	   ·首先解读有几个逗号
	   ·其次解读有几个:号


out：  
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/dd9501fedd4e5dfbc2972bc3728c5721.png)  
<label class="ob-comment" title="" style=""> **例子2：不含有步长的切片，同时使用两种方式联合的索引** <input type="checkbox"> <span style=""> 归纳:索引维度的上升会改变索引的含义 </span></label>

```python
x = torch.arange(0,24).view(2,3,4)
print(x)
print(x[0][0:2])  # 获取的是第一维下的0索引后第二维下索引为0,1的全部数据 等价与x[0, 0:2]
```

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/043b62ec5f5bdbdedd229af32475c0d2.png)  
**例子3：含有负索引的切片**

```python
x = torch.arange(0,24).view(2,3,4)
print(x)
print(x[0, 0:2, 0:-1:2])  # 重点可以看一下第三维
```

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/1bf0eb16c4659c6c031386960a1e06f1.png)

**例子4：前面维度我都要，针对最后维度的数据进行切片**

```python
x = torch.arange(0,2*3*4*5).view(2,3,4,5)
print(x)
print(x[...,0:2, 0:5:2])
```

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/9c5689e6eededf163cb876efe9c10bc2.png)  
**例子5：布尔索引**

```python
x = torch.arange(0, 9).view(3, 3)
# x4 = torch.ones(3,3)*4
print(x)  # 0-8的二维张量
index = x > 4  # 判断位置对应位置上是否大于数据4, 类似于对4进行广播成和x相同维度的数据，然后逐元素比大小
# index4 = x >x4
# print(index4)
print(index)
print(x[index])  # 针对位置值为True的数据进行获取, 获取后的数据组成一维数据

print("**************")
y = torch.randint(0,9,(3, 3))
dex = y > 4
print(y)
print(y[dex])
```

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/12999ccf469b1f2c9cdbddb8bd3b43d6.png)

**例子6：整数索引**

```python
x = torch.arange(0, 9).view(3, 3)
print(x)
rows = [0,1]
cols = [1,2]
print(x[rows, cols])  # 获取的数据是(0,1)和(1,2)位置上的数据组成的一维张量
```

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/a575689f94da566a6bb68563c9de7972.png)  
**例子7： torch.nonzero()**  
`这个在单目标分割中，计算评价指标和部分损失函数时会遇到`

```python
x = torch.randint(0,2,(1,2,3,4))
print(x)
index = torch.nonzero(x) # 得到x中非0数据的位置信息，同时将每个位置组成一个一维数据，一共是二维张量
print(index)  
print(len(index), index.shape)
```

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/f661e57c5e7791c4be2d7cd492fe9170.png)  
**例子8： torch.where(condition, x, y) 使用**

```python
x = torch.randn(4,3)
y = torch.ones(4,3)
m = torch.where(x>0, x, y)  # 这里的condition可以不用x，只需要和x，y相同维度的数据就可以了，功能类似于将布尔索引的高级使用
print(x)
print(y)
print(m)

print("使用布尔索引完成上述例子")
index = x > 0  # condition
print(index)  # 得到位置信息
for i in range(len(x)):
    for j in range(len(x[i])):
        if index[i, j]:
            m[i, j] = x[i, j]
        else:
            m[i, j] = y[i, j]
print(m)
```

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/61c6838672c3cd468afbdafb5981618d.png)

### 2.3.2 Tensor的转换

**汇总：**

|Name|Out|
|---|---|
|a.nelement()|返回元素个数|
|a.ndimension()|返回张量轴的个数|
|a.size()|返回张量维度信息|
|a.shape|返回张量维度信息|
|a.viw(i, j, …)|对a进行数据维度变换,修改返回值，原始值也改变 ，**实际中这个用的多**|
|a.reshape(i, j, k)|对a进行数据维度变换，修改返回值，原始值不一定改变|
|以上两个中的参数允许存在最多一个 **-1**|用来通过已确定的维度，计算-1位置上的维度，见**例2**|
|torch.squeeze(a)|去掉a中维度为1的轴, 不指定就去掉所有，指定就去掉指定位置的，见**例3**|
|torch.unsqueeze(b, i)|在b中i的位置田间一个维度为1的轴，见**例4**|
|torch.transpose(b, 1, 0)|将b的维度调换位置，类似于二维矩阵的转置操作，见**例5**|
|torch.t(b)|是transpose的简化书写|
|a.permute(i, j, k,…)|将a中数据的位置按照i，j，k的位置进行重现调整，类似于高维矩阵的转置操作，**见例6**|
|||

`a为高维张量， b是二维张量`  
**例子1：**

```python
a = torch.rand(3,4,5,6,7)
print("元素个数：", a.nelement())
print("轴的个数：", a.ndimension())
print("维度信息：", a.size())
print("维度信息：", a.shape)

out：
元素个数： 2520
轴的个数： 5
维度信息： torch.Size([3, 4, 5, 6, 7])
维度信息： torch.Size([3, 4, 5, 6, 7])
```

**例子2：使用-1进行维度推算**

```python
a = torch.randint(0,3,(3, 4))
print(a, a.shape)
b = a.view(-1,2)
print(b, b.shape)

out:
tensor([[0, 2, 0, 1],
        [2, 2, 0, 0],
        [1, 0, 0, 0]]) torch.Size([3, 4])
tensor([[0, 2],
        [0, 1],
        [2, 2],
        [0, 0],
        [1, 0],
        [0, 0]]) torch.Size([6, 2])
```

**例3：去掉张量中维度为1的轴**

```python
x = torch.randint(0,2, (1,1,1,2,4,1,1))
print(x.shape)
print(torch.squeeze(x, 2).shape)
print(torch.squeeze(x).shape)

out:
torch.Size([1, 1, 1, 2, 4, 1, 1])
torch.Size([1, 1, 2, 4, 1, 1])
torch.Size([2, 4])
```

**例4： 给张量指定位置添加一个维度为1的轴**

```python
x = torch.randint(0,2,(3, 4))
print(x.shape)
print(torch.unsqueeze(x, 1).shape)

out:
torch.Size([3, 4])
torch.Size([3, 1, 4])
```

**例5： 二维张量的转置操作**

```python
x = torch.arange(0,12).view(2,6)
print(x)
print("view操作，将数据重新写一下:\n",x.view(6, 2))  #注意二维张量的转置和View的区别
print("转置操作，将数据按照原来维度进行调换:\n",torch.transpose(x, 1, 0))  #注意二维张量的转置和View的区别
print("转置操作，将数据按照原来维度进行调换:\n",torch.t(x))  # torch.t是简化书写

out:
tensor([[ 0,  1,  2,  3,  4,  5],
        [ 6,  7,  8,  9, 10, 11]])
view操作，将数据重新写一下:
 tensor([[ 0,  1],
        [ 2,  3],
        [ 4,  5],
        [ 6,  7],
        [ 8,  9],
        [10, 11]])
转置操作，将数据按照原来维度进行调换:
 tensor([[ 0,  6],
        [ 1,  7],
        [ 2,  8],
        [ 3,  9],
        [ 4, 10],
        [ 5, 11]])
转置操作，将数据按照原来维度进行调换:
 tensor([[ 0,  6],
        [ 1,  7],
        [ 2,  8],
        [ 3,  9],
        [ 4, 10],
        [ 5, 11]])
```

**例6：高维数据的转置**

```python
x = torch.randn(3,4,2,5)
print(x.shape)
y = x.permute(1,0,2,3)  # 一定要注意区分view和转置的区别
print(y.shape)

out:
torch.Size([3, 4, 2, 5])
torch.Size([4, 3, 2, 5])
```

### 2.3.3 Tensor的拼接

**汇总：**

|Name|Out|
|---|---|
|torch.cat((a, b) , dim=)|在**已有的轴上拼接**矩阵，默认轴为0，给定轴的维度可以不同，其余轴的维度必须相同|
|torch.stack((a, b) , dim=)|在**新的轴上拼接**,默认轴为0，要求被拼接的轴的维度都相同|

`a, b 的轴最多只有一个可以不同`

**例子:torch.cat**

```python
import torch
a = torch.arange(0,6).view(2,3)
b = torch.ones(2,4)
print(a)
print(b)
# torch.cat
c = torch.cat([a,b],dim=1)
print("a和b在指定的轴上拼接\n", c)
d = torch.cat((a, a))
print("a和a在默认的轴上拼接\n", d)

out:
tensor([[0, 1, 2],
        [3, 4, 5]])
tensor([[1., 1., 1., 1.],
        [1., 1., 1., 1.]])
a和b在指定的轴上拼接
 tensor([[0., 1., 2., 1., 1., 1., 1.],
        [3., 4., 5., 1., 1., 1., 1.]])
a和a在默认的轴上拼接
 tensor([[0, 1, 2],
        [3, 4, 5],
        [0, 1, 2],
        [3, 4, 5]])
```

**例子：torch.stack()**  
这个用法还是很抽象的，建议从结果倒退结果，最直观的是最后两个维度的呈现：

```python
import torch
a = torch.arange(0,6).view(2,3)
b = torch.ones(2,3)
print(a, a.shape)
print(b, b.shape)
# torch.cat
c = torch.stack([a,b],dim=2)
print("a和b在指定的轴上拼接\n", c, c.shape)
d = torch.stack((a, a))
print("a和a在默认的轴上拼接\n", d, d.shape)

out:
tensor([[0, 1, 2],
        [3, 4, 5]]) torch.Size([2, 3])
tensor([[1., 1., 1.],
        [1., 1., 1.]]) torch.Size([2, 3])
a和b在指定的轴上拼接
 tensor([[[0., 1.],
         [1., 1.],
         [2., 1.]],

        [[3., 1.],
         [4., 1.],
         [5., 1.]]]) torch.Size([2, 3, 2])
a和a在默认的轴上拼接
 tensor([[[0, 1, 2],
         [3, 4, 5]],

        [[0, 1, 2],
         [3, 4, 5]]]) torch.Size([2, 2, 3])
```

### 2.3.4 Tensor的拆分

**汇总：**

|Name|Out|
|---|---|
|torch.split(a, list, dim=)|传入的是拆分后维度的大小，可以传入list，也可以传入整数，不够的放最后|
|torch.chunk(a, x, dim=)|传入的是拆分的矩阵个数|

`a 是高维张量，list为列表，x为整数`  
**例子：torch.split()**

```python
import torch

a = torch.randn(10, 3)
print("传入的是维度大小列表：")
for x in torch.split(a, [1, 2, 3, 4], dim=0):
    print(x.shape)

print("传入的是维度大小整数：")
for x in torch.split(a, 4, dim=0):   # 注意返回值是由tensor组成的元组
    print(x.shape)

out:
传入的是维度大小列表：
torch.Size([1, 3])
torch.Size([2, 3])
torch.Size([3, 3])
torch.Size([4, 3])
传入的是维度大小整数：
torch.Size([4, 3])
torch.Size([4, 3])
torch.Size([2, 3])
```

**例子：torch.chunk()**

```python
import torch

a = torch.randn(15, 6)

print("传入的是拆分后的个数：")
for x in torch.chunk(a, 3, dim=1):  
    print(x.shape)

out:
传入的是拆分的个数：
torch.Size([15, 2])
torch.Size([15, 2])
torch.Size([15, 2])
```

### 2.3.5 Tensor的规约操作

**汇总：**

|Name|Out|
|---|---|
|torch.max(a， dim=)|计算a中指定轴的最大值，**默认是求最大值(不返回地址)**，**指定轴返回(最大值，最大值位置**)|
|torch.cumsum(a, dim=)|沿着指定轴计算累加|
|torch.cumprod(a, dim=)|沿着指定轴计算累乘|
|a.mean()|求均值|
|a.median|求中值|
|a.std()|求协方差|
|a.max()|求最大值|
|torch.unique(a)|寻找张量a中出现了哪些元素|

`a为高维张量`  
#### **例子1：torch.max()**

```python
a = torch.tensor([[3,1],[4,7]])
print(a)
print(torch.max(a))  # 全局最大值
print(torch.max(a, dim=1))   # 在第二维度上找最大值，同时返回其位置(除指定轴外的位置，例返回【0，1】和指定轴1组成了位置信息，[0,1]和[1,1])

out:
tensor([[3, 1],
        [4, 7]])
tensor(7)
torch.return_types.max(
values=tensor([3, 7]),
indices=tensor([0, 1]))
```

#### **例子2：torch.cumsum()**

```python
a = torch.tensor([[3,1,3],[4,7,4],[4,5,2]])
print(a)
print(torch.cumsum(a, dim=1))  # 按照横轴计算每一列的累加，然后放在对应的位置上
```

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/764d669c211ddecf847292aba776a087.png)  
#### **例子3： torch.cumprod()**

```python
a = torch.tensor([[3,1,3],[4,7,4],[4,5,2]])
print(a)
print(torch.cumprod(a, dim=0))  # 按照列轴计算每一行的累乘，然后放在对应的位置上
```

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/cf12b1c2ff2844d25c83fcfee877935b.png)  
#### **例子4：a.mean(), a.std()等**

```python
a = torch.tensor([[3,1,3],[4,7,4],[4,5,2]], dtype=torch.float)
print(a)
print("未指定轴，则计算全局的相关信息：")
print(a.max(), a.std(), a.median(), a.mean())
print("输出指定轴下的信息：")
print("Max_out:", a.max(dim=1),"\n ***")  # 返回最大值 及 位置
print("Str_out:", a.std(dim=1),"\n ***")   # 返回协方差信息
print("Median_out:", a.median(dim=1),"\n ***")  # 返回中间值 及 位置
print("Mean_out:", a.mean(dim=1),"\n ***")  # 返回均值
```

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/ccf5dbfde05b5b5be93a482e3056aec7.png)  
#### **例子5：torch.unique()**

将高维Tensor对象降低至一维

```python
x = torch.randint(0,15,(3,3))
print(x)
print(torch.unique(x)) # 得到x中出现的元素

out：
tensor([[14,  3,  4],
        [ 2,  3,  7],
        [12,  0,  6]])
tensor([ 0,  2,  3,  4,  6,  7, 12, 14])
```

### 2.3.6 Tensor形状的查看

tensor是一个对象，那么我们就有如下方法访问其属性
#### .nidm属性

用于查看tensor对象具体的维数

```python
import torch

# 创建一个示例张量
tensor = torch.randn(3, 4, 5)

# 检查维度数量
dim_count = tensor.ndim
print("Dimension count:", dim_count)  # 输出: Dimension count: 3

```

#### .size属性
```python
import torch

# 创建一个示例张量
tensor = torch.randn(3, 4, 5)

# 使用.size()方法获取形状
tensor_size = tensor.size()
print("Tensor size:", tensor_size)  # 输出: Tensor size: torch.Size([3, 4, 5])
```
#### .shape属性：
==注意这是访问shape属性所以不是用小括号！==
##### 查看所有维度的长度
```python
import torch

# 创建一个示例张量
tensor = torch.randn(3, 4, 5)

# 获取张量的形状
tensor_shape = tensor.shape           # 不传参与size方法一致
print("Tensor shape:", tensor_shape)  # 输出: Tensor shape: torch.Size([3, 4, 5])
```
##### 查看某一维度的长度
```python
import torch

# 创建一个示例张量
tensor = torch.randn(3, 4, 5) 
# 几个逗号，就是逗号数+1的数据维度
# [[[]]],最外侧是对应着第一维度，长度为3

# 获取张量某维具体的形状
tensor_shape = tensor.shape[0] # 传入数字表示具体的维度，返回整型,注意是中括号
# 错误属性访问方法tensor_shape = tensor.shape(0)

print("Tensor shape:", tensor_shape)  # 输出: Tensor shape: 3
# 此外用len也能访问长度len(tensor)访问第1维，len(tensor[0])访问第二维长度
```
### 2.3.7 Tensor对象转化
#### .tolist()--将tensor量转化为python列表类
在 PyTorch 库中，`Tensor` 类型提供了一个名为 `.tolist()` 的方法，用于将 `Tensor` 对象转换成 Python 的原生数据结构，通常是列表（`list`）或嵌套列表。这对于需要将数据传递给不接受 `Tensor` 类型的函数或库时非常有用。

 使用 `.tolist()`

当调用一个 `Tensor` 的 `.tolist()` 方法时，该方法会递归地遍历 `Tensor` 的所有维度，并将每个元素转换为其相应的 Python 数据类型。对于数值类型的数据，如整数和浮点数，它们将被转换为 Python 的 `int` 和 `float` 类型。对于布尔值，它们将被转换为 Python 的 `bool` 类型。

##### 示例

下面是一些使用 `.tolist()` 的示例：
```python
import torch

# 创建一个一维 Tensor
tensor_1d = torch.tensor([1, 2, 3])
# 转换为 list
list_1d = tensor_1d.tolist()
print(list_1d)  # 输出: [1, 2, 3]
				# 直接打印将输出tensor([1, 2, 3])

# 创建一个二维 Tensor
tensor_2d = torch.tensor([[1, 2], [3, 4]])
# 转换为 list
list_2d = tensor_2d.tolist()
print(list_2d)  # 输出: [[1, 2], [3, 4]]
                # 直接打印将输出tensor([[1, 2], [3, 4]])

# 创建一个三维 Tensor
tensor_3d = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])
# 转换为 list
list_3d = tensor_3d.tolist()
print(list_3d)  # 输出: [[[1, 2], [3, 4]], [[5, 6], [7, 8]]]
				# 直接打印将输出tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])

```
##### 注意事项

- 如果你的 `Tensor` 包含大量数据，使用 `.tolist()` 可能会导致性能下降，因为此操作涉及到数据复制和类型转换。
- 当从 GPU 上的 `Tensor` 调用 `.tolist()` 时，数据将首先被复制到 CPU 再进行转换，这可能带来额外的时间开销。


### 2.3.7Tensor的删除

## 2.4Tesor常见方法


# 三、并行计算

CUDA是一种操作GPU的软件架构，Pytorch配合GPU环境这样模型的训练速度会非常的快

## 3.1. 使用 GPU

```python
import torch
 
# 测试GPU环境是否可使用
print(torch.__version__) # pytorch版本
print(torch.version.cuda) # cuda版本
print(torch.cuda.is_available()) # 查看cuda是否可用
 
#使用GPU or CPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
 
# 判断某个对象是在什么环境中运行的
a.device
 
# 将对象的环境设置为device环境
A = A.to(device)
 
# 将对象环境设置为COU
A.cpu().device
 
# 若一个没有环境的对象与另外一个有环境a对象进行交流,则环境全变成环境a
a+b.to(device)
 
# cuda环境下tensor不能直接转化为numpy类型,必须要先转化到cpu环境中
a.cpu().numpy()
 
# 创建CUDA型的tensor
torch.tensor([1,2],device)
```


## 3.2. 使用 HPC

>**注意**：分布式计算调用的是 torch.distribute 包，这个包目前不支持在 Windows 平台上使用。


# 四、其他技巧

## 4.1 自动微分

神经网络依赖反向传播求梯度来更新网络的参数，求梯度是个非常复杂的过程，在Pytorch中，提供了两种求梯度的方式，一个是backward，将求得的结果保存在自变量的grad属性中，另外一种方式是torch.autograd.grad

### 4.1.1 backward求导

使用backward进行求导。这里主要介绍了求导的两种对象，标量Tensor和非标量Tensor的求导。两者的主要区别是非标量Tensor求导的主要区别是加了一个gradient的Tensor，其尺寸与自变量X的尺寸一致。在求完导后，需要与gradient进行点积，所以只是一般的求导的话，设置的参数全部为1。最后还有一种使用标量的求导方式解决非标量求导，了解了解就好了。

```python
import numpy as np
import torch
 
# 标量Tensor求导
# 求 f(x) = a*x**2 + b*x + c 的导数
x = torch.tensor(-2.0, requires_grad=True)
a = torch.tensor(1.0)
b = torch.tensor(2.0)
c = torch.tensor(3.0)
y = a*torch.pow(x,2)+b*x+c
y.backward() # backward求得的梯度会存储在自变量x的grad属性中
dy_dx =x.grad
dy_dx
 
 
# 非标量Tensor求导
# 求 f(x) = a*x**2 + b*x + c 的导数
x = torch.tensor([[-2.0,-1.0],[0.0,1.0]], requires_grad=True)
a = torch.tensor(1.0)
b = torch.tensor(2.0)
c = torch.tensor(3.0)
gradient=torch.tensor([[1.0,1.0],[1.0,1.0]])
y = a*torch.pow(x,2)+b*x+c
y.backward(gradient=gradient) 
dy_dx =x.grad
dy_dx
 
 
# 使用标量求导方式解决非标量求导
# 求 f(x) = a*x**2 + b*x + c 的导数
x = torch.tensor([[-2.0,-1.0],[0.0,1.0]], requires_grad=True)
a = torch.tensor(1.0)
b = torch.tensor(2.0)
c = torch.tensor(3.0)
gradient=torch.tensor([[1.0,1.0],[1.0,1.0]])
y = a*torch.pow(x,2)+b*x+c
z=torch.sum(y*gradient)
z.backward()
dy_dx=x.grad
dy_dx
```

### 4.1.2 autograd.grad求导

```python
 
import torch
 
#单个自变量求导
# 求 f(x) = a*x**4 + b*x + c 的导数
x = torch.tensor(1.0, requires_grad=True)
a = torch.tensor(1.0)
b = torch.tensor(2.0)
c = torch.tensor(3.0)
y = a * torch.pow(x, 4) + b * x + c
#create_graph设置为True,允许创建更高阶级的导数
#求一阶导
dy_dx = torch.autograd.grad(y, x, create_graph=True)[0]
#求二阶导
dy2_dx2 = torch.autograd.grad(dy_dx, x, create_graph=True)[0]
#求三阶导
dy3_dx3 = torch.autograd.grad(dy2_dx2, x)[0]
print(dy_dx.data, dy2_dx2.data, dy3_dx3)
 
 
# 多个自变量求偏导
x1 = torch.tensor(1.0, requires_grad=True)
x2 = torch.tensor(2.0, requires_grad=True)
y1 = x1 * x2
y2 = x1 + x2
#只有一个因变量,正常求偏导
dy1_dx1, dy1_dx2 = torch.autograd.grad(outputs=y1, inputs=[x1, x2], retain_graph=True)
print(dy1_dx1, dy1_dx2)
# 若有多个因变量，则对于每个因变量,会将求偏导的结果加起来
dy1_dx, dy2_dx = torch.autograd.grad(outputs=[y1, y2], inputs=[x1, x2])
dy1_dx, dy2_dx
print(dy1_dx, dy2_dx)
```

![](https://i-blog.csdnimg.cn/blog_migrate/1487719c92f1fd68face44218c5e35d5.png)

### 4.1.3 求最小值

使用自动微分机制配套使用SGD随机梯度下降来求最小值

```python
#例2-1-3 利用自动微分和优化器求最小值
import numpy as np
import torch
 
# f(x) = a*x**2 + b*x + c的最小值
x = torch.tensor(0.0, requires_grad=True)  # x需要被求导
a = torch.tensor(1.0)
b = torch.tensor(-2.0)
c = torch.tensor(1.0)
optimizer = torch.optim.SGD(params=[x], lr=0.01)  #SGD为随机梯度下降
print(optimizer)
 
def f(x):
    result = a * torch.pow(x, 2) + b * x + c
    return (result)
 
for i in range(500):
    optimizer.zero_grad()  #将模型的参数初始化为0
    y = f(x)
    y.backward()  #反向传播计算梯度
    optimizer.step()  #更新所有的参数
print("y=", y.data, ";", "x=", x.data)
```

![](https://i-blog.csdnimg.cn/blog_migrate/4d71282a102356992a858a43a55688c2.png)
## 4.2 Pytorch层次结构

Pytorch中一共有5个不同的层次结构，分别为硬件层、内核层、低阶API、中阶API和高阶API（torchkeras）

|   |   |
|---|---|
|硬件层|底层的计算资源包括CPU和GPU|
|内核层|使用C++来实现|
|低阶API|Python实现的操作符，提供了封装C++内核的低级API指令，主要包括各种张量操作算 子、自动微分、变量管理. 如torch.tensor,torch.cat,torch.autograd.grad,nn.Module.|
|中阶API|Python实现的模型组件，对低级API进行了函数封装，主要包括各种模型层，损失函数，优化器，数据管道等等。 如 torch.nn.Linear,torch.nn.BCE,torch.optim.Adam,torch.utils.data.DataLoader.|
|高阶API|Python实现的模型接口。Pytorch没有官方的高阶API。为了便于训练模型，我们仿照 keras中的模型接口，使用了不到300行代码，封装了Pytorch的高阶模型接口 torchkeras.Model|

# 五、数据

Pytorch主要通过Dataset和DataLoader进行构建数据管道

## 5.1 Dataset and DataLoader

|   |   |
|---|---|
|Dataset|一个数据集抽象类，所有自定义的Dataset都需要继承它，并且重写__getitem__()或__get_sample__()这个类方法|
|DataLoader|一个可迭代的数据装载器。在训练的时候，每一个for循环迭代，就从DataLoader中获取一个batch_sieze大小的数据。|

## 5.2 数据读取与预处理

DataLoader的参数如下

```python
DataLoader(
 dataset,
 batch_size=1,
 shuffle=False,
 sampler=None,
 batch_sampler=None,
 num_workers=0,
 collate_fn=None,
 pin_memory=False,
 drop_last=False,
 timeout=0,
 worker_init_fn=None,
 multiprocessing_context=None,
)
```

在实践中，主要修改的参数以下标为橙色

|   |   |
|---|---|
|dataset|数据集，决定数据从哪里读取，以及如何读取|
|batch_size|批次大小，默认为1|
|shuffle|每个epoch是否乱序|
|sampler|样本采样函数，一般无需设置|
|batch_sampler|批次采样函数，一般无需设置|
|num_workers|使用多进程读取数据，设置的进程数|
|collate_fn|整理一个批次数据的函数|
|pin_memory|是否设置为锁业内存。默认为False，锁业内存不会使用虚拟内存(硬盘)，从锁 业内存拷贝到GPU上速度会更快|
|drop_last|是否丢弃最后一个样本数量不足batch_size批次数据|
|timeout|加载一个数据批次的最长等待时间，一般无需设置|
|worker_init_fn|每个worker中dataset的初始化函数，常用于 IterableDataset。一般不使用|

顺带介绍一下Epoch、Iteration、Batchsize之间的关系

|   |   |
|---|---|
|**Epoch**|所有的样本数据都输入到模型中，称为一个epoch|
|**Iteration**|一个Batch的样本输入到模型中，称为一个Iteration|
|**Batchsize**|一个批次的大小，一个Epoch=Batchsize*Iteration|

先看数据读取的主要流程

![](https://i-blog.csdnimg.cn/blog_migrate/56c869df076a1707c843997747a0ef08.png)1. 从DataLoader开始

2. 进入DataLoaderIter，判断单线程还是多线程

3. 进入Sampler进行采样，获得一批一批的索引，这些索引告诉我们需要读取哪些数据、

4. 进入DatasetFetcher，依据索引读取数据

5. Dataset告诉我们数据的地址

6. 自定义的Dataset中会重写__getietm__方法，针对不同的数据来进行定制化的数据读取

7. 到这里就获取的数据的Text和Label

8. 进入collate_fn将之前获取的个体数据进行组合成batch

9. 一个一个batch组成Batch Data

再来看一个具体的代码

```python
from torch.utils.data import DataLoader
from torch.utils.data.dataset import TensorDataset
 
# 自构建数据集
dataset = TensorDataset(torch.arange(1, 40))
dl = DataLoader(dataset,
                batch_size=10,
                shuffle=True,
                num_workers=1,
                drop_last=True)
# 数据输出
for batch in dl:
    print(batch)
```

![](https://i-blog.csdnimg.cn/blog_migrate/725b5c2df3691bef1c176e75cc488ed5.png)

因为自定义的数据集只有39条，最后一个batch的数据量小于10，被舍弃掉了

而数据预处理主要是重写Dataset和DataLoader中的方法，因此总体代码如下所示

## 5.5 Pytorch工具

基于Pytorch已经产生了一些封装完备的工具，而缺点也很明显，数据处理不是很灵活，对于初学者来说，多写代码比较踏实，因此作者不太推荐使用这些方法

|   |   |
|---|---|
|torchvision|图像视频处理|
|torchaudio|音频处理|
|torchtext|自然语言处理|

# 六、torch.nn

torch.nn是神经网路工具箱，该工具箱建立于Autograd(主要有自动求导和梯度反向传播功能)，提供了网络搭建的模组，优化器等一系列功能。

搭建一个神经网络模型整个流程是怎么样的呢？

> 搭建网络流程
> 
> 1 数据读取
> 
> 2 定义模型
> 
> 3 定义损失函数和优化器
> 
> 4 模型训练
> 
> 5 获取训练结果

我们拿一个最简单的FNN网络来对经典数据集diabetes糖尿病数据集来进行分类预测，模型性能指标直接采用Loss。

数据集：见博客顶端

```python
​
import numpy as np
import torch
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
 
 
# Prepare the dataset
class DiabetesDateset(Dataset):
    # 加载数据集
    def __init__(self, filepath):
        xy = np.loadtxt(filepath, delimiter=',', dtype=np.float32, encoding='utf-8')
        self.len = xy.shape[0]  # shape[0]是矩阵的行数,shape[1]是矩阵的列数
        self.x_data = torch.from_numpy(xy[:, :-1])
        self.y_data = torch.from_numpy(xy[:, [-1]])
 
    # 获取数据索引
    def __getitem__(self, index):
        return self.x_data[index], self.y_data[index]
 
    # 获得数据总量
    def __len__(self):
        return self.len
 
 
dataset = DiabetesDateset('diabetes.csv')
train_loader = DataLoader(dataset=dataset, batch_size=32, shuffle=True, num_workers=2, drop_last=True)  # num_workers为多线程
 
 
# Define the model
class FNNModel(torch.nn.Module):
    def __init__(self):
        super(FNNModel, self).__init__()
        self.linear1 = torch.nn.Linear(8, 6)  # 输入数据的特征有8个,也就是有8个维度,随后将其降维到6维
        self.linear2 = torch.nn.Linear(6, 4)  # 6维降到4维
        self.linear3 = torch.nn.Linear(4, 2)  # 4维降到2维
        self.linear4 = torch.nn.Linear(2, 1)  # 2w维降到1维
        self.sigmoid = torch.nn.Sigmoid()  # 可以视其为网络的一层,而不是简单的函数使用
 
    def forward(self, x):
        x = self.sigmoid(self.linear1(x))
        x = self.sigmoid(self.linear2(x))
        x = self.sigmoid(self.linear3(x))
        x = self.sigmoid(self.linear4(x))
        return x
 
 
model = FNNModel()
 
# Define the criterion and optimizer
criterion = torch.nn.BCELoss(reduction='mean')  # 返回损失的平均值
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
 
epoch_list = []
loss_list = []
 
# Training
if __name__ == '__main__':
    for epoch in range(100):
        # i是一个epoch中第几次迭代,一共756条数据,每个mini_batch为32,所以一个epoch需要迭代23次
        # data获取的数据为(x,y)
        loss_one_epoch = 0
        for i, data in enumerate(train_loader, 0):
            inputs, labels = data
            y_pred = model(inputs)
            loss = criterion(y_pred, labels)
            loss_one_epoch += loss.item()
 
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        loss_list.append(loss_one_epoch / 23)
        epoch_list.append(epoch)
        print('Epoch[{}/{}],loss:{:.6f}'.format(epoch + 1, 100, loss_one_epoch / 23))
 
    # Drawing
    plt.plot(epoch_list, loss_list)
    plt.xlabel('epoch')
    plt.ylabel('loss')
    plt.show()
 
​
```

**Result**

![](https://i-blog.csdnimg.cn/blog_migrate/103ac415bc4a1f6310ab7eb0f4c5ee46.png)

![](https://i-blog.csdnimg.cn/blog_migrate/4256325a3a4c40a755ec7af22d8b44fe.png)

注意点

1 创建模型的参数与__init__中的参数一致

2 训练模型的参数与forward中的参数一致 

参考资料

本文章主要参考Github的各个大牛团队以及Pytorch的官方文档，感谢开源，开源万岁！

1 [Chinese-Text-Classfication-Pytorch](https://github.com/649453932/Chinese-Text-Classification-Pytorch "Chinese-Text-Classfication-Pytorch")

2 [Pytorch official Chinese documents](https://pytorch-cn.readthedocs.io/zh/latest/notes/extending/#c "Pytorch official Chinese documents")

3 [Pytorch official English doccuments](https://pytorch.org/tutorials/ "Pytorch official English doccuments")

4 [B站 PyTorch深度学习快速入门教程（小土堆）](https://www.bilibili.com/video/BV1hE411t7RN?spm_id_from=333.337.search-card.all.click&vd_source=c8e241c7381640b73195e99da6f2222f "B站 PyTorch深度学习快速入门教程（小土堆）")

5 [pytorch-tutorial](https://github.com/yunjey/pytorch-tutorial "pytorch-tutorial")

6 [Youtube--Pytorch](https://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3DSKq-pmkekTk "Youtube--Pytorch")

7 [Pytorch--handbook](https://github.com/zergtant/pytorch-handbook "Pytorch--handbook")

8 [Deep Learning with PyTorch: A 60 Minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html "Deep Learning with PyTorch: A 60 Minute Blitz")

---

以下是作者两个较有诚意的工程，已开源。其中的代码框架结构是我看了较多论文源码后总结出来的一套较为清晰、规范的模板，且我的学术论文都是在此基础上修改的。工程的内容也是热门的CV和NLP领域两个基础性的任务，适合学完Pytorch后的工程实践

[Bert/Roberta+Transformer_BILSTM_TextCNN实现IMDB文本分类](https://blog.csdn.net/ccaoshangfei/article/details/127537953?spm=1001.2014.3001.5502 "Bert/Roberta+Transformer_BILSTM_TextCNN实现IMDB文本分类")

[LeNet、AlexNet、GoogleLeNet、VGG16、ResNet实现COIL20图像分类](https://blog.csdn.net/ccaoshangfei/article/details/128185277?csdn_share_tail=%7B%22type%22%3A%22blog%22%2C%22rType%22%3A%22article%22%2C%22rId%22%3A%22128185277%22%2C%22source%22%3A%22ccaoshangfei%22%7D "LeNet、AlexNet、GoogleLeNet、VGG16、ResNet实现COIL20图像分类")