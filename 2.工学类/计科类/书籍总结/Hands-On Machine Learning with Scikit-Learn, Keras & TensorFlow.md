>**引言**：BOM姬对于导入表格数据的处理需要用到机器学习，故此涉入机器学习，为了更好的达成项目目的，并且感知现在机器学习的发展进程以及未突破的问题，以便更有针对性的对于某方面机器学习性能做突破性研究。


- 阅读背景：[[7.项目类/自创类/计算机/BOM姬/学习路线]]中为了先建立起对机器学习概念用
- 阅读目的：
	- **知识目标：**
	    - 理解机器学习的核心概念，例如监督学习、非监督学习、强化学习等，并能够区分它们的应用场景。
	    - 掌握常用的机器学习算法，例如线性回归、逻辑回归、决策树、支持向量机、神经网络等，并了解它们的原理和优缺点。
	    - 熟悉Scikit-Learn、Keras和TensorFlow等机器学习库的基本用法，能够使用它们构建简单的机器学习模型。
	    - 了解模型评估和选择的方法，例如交叉验证、准确率、精确率、召回率、F1值等，并能够根据实际问题选择合适的评估指标。
	    - 掌握模型调参的基本技巧，例如网格搜索、随机搜索等，并能够优化模型的性能。
	- **技能目标：**
	    - 能够独立完成简单的机器学习项目，例如分类、回归、聚类等。
	    - 能够根据实际问题选择合适的机器学习算法和模型。
	    - 能够使用Scikit-Learn、Keras和TensorFlow等机器学习库进行模型训练和评估。
	    - 能够对模型进行调参，提高模型的性能。
	- **行动目标：**
	    - 阅读完每一章节后，完成相应的练习题和项目。
	    - 将学到的知识应用到实际项目中，例如构建一个图像分类器或一个房价预测模型。
	    - 积极参与机器学习社区的讨论，与其他学习者交流经验
- 阅读方法：
	- **阅读前：**
		- **思考问题：**
		    - 这一小节的主题是什么？它与我设定的阅读目标有什么关系？
		    - 根据小节标题和目录，我能对这一小节的内容做出哪些假设？
		    - 在开始阅读之前，我已经掌握了哪些与本节内容相关的知识？
		    - 我希望通过阅读这一小节解决哪些疑问或学习哪些新技能？
		- **要做的工作：**
		    - 快速浏览本小节的标题、副标题、图表和代码示例，对内容有一个大致的了解。
		    - 明确本小节的学习目标，例如理解某个概念、掌握某个算法或学会使用某个函数。
		    - 记录下你对本小节内容的初步假设和疑问。
	- **阅读中：**
		- **思考问题：**
		    - 作者是如何解释这个概念或算法的？我是否理解了其中的原理？
		    - 代码示例是如何工作的？我是否能够理解每一行代码的含义？
		    - 作者提到了哪些我之前没有了解过的知识点？
		    - 这些知识点与我已有的知识体系有什么联系？
		    - 我是否能够将本小节的内容应用到实际项目中？
		- **要做的工作：**
		    - 仔细阅读本小节的内容，重点关注关键概念、算法原理和代码示例。
		    - 对重要的知识点进行标记、划线或做笔记。
		    - 尝试运行代码示例，并修改参数或输入，观察结果的变化。
		    - 查阅相关的资料，例如官方文档、博客文章或论坛帖子，以加深对知识点的理解。
		    - 记录下你的疑问和思考，以便在阅读后进行回顾和总结。
	- **阅读后：**
		- **思考问题：**
		    - 我是否达到了本小节的学习目标？
		    - 我是否解决了阅读前提出的疑问？
		    - 我是否掌握了本小节的关键概念和算法？
		    - 我是否能够用自己的话解释本小节的内容？
		    - 我是否能够将本小节的内容应用到实际项目中？
		    - 本小节的内容与其他章节有什么联系？
		    - 我还需要学习哪些相关的知识？
		- **要做的工作：**
		    - 回顾本小节的内容，整理笔记，并对知识点进行归纳和总结。
		    - 尝试回答阅读前提出的问题，并记录下你的答案。
		    - 完成本小节的练习题或项目，以检验你的学习效果。
		    - 将本小节的内容与其他章节的内容联系起来，构建完整的知识体系。
		    - 制定下一步的学习计划，例如阅读相关的书籍或文章，或尝试解决更复杂的实际问题。
- 阅读感悟：
	- **把书读厚：**
	    - 线性回归算法有哪些变种？它们分别适用于什么场景？
	    - 决策树算法有哪些剪枝策略？它们如何影响模型的泛化能力？
	    - 神经网络的优化算法有哪些？它们有什么优缺点？
	    - 如何选择合适的模型评估指标？
	    - 如何解决过拟合和欠拟合问题？
	    - 深度学习在自然语言处理、计算机视觉等领域有哪些应用？
	    - 机器学习的伦理问题有哪些？如何避免算法歧视？
	- **联系实际：**
	    - 我可以用线性回归算法预测房价吗？
	    - 我可以用决策树算法进行用户画像吗？
	    - 我可以用神经网络进行图像分类吗？
	    - 我可以用机器学习解决工作中的哪些问题？
	    - 我可以用机器学习创造哪些新的应用？
	- **扩展阅读：**
	    - 阅读Scikit-Learn、Keras和TensorFlow的官方文档。
	    - 阅读相关的学术论文和博客文章。
	    - 参与机器学习社区的讨论和交流。
	    - 学习相关的数学知识，例如线性代数、概率论和优化理论。
	- **把书读薄：**
	    - 机器学习：通过算法让计算机从数据中学习，无需显式编程。
	    - 监督学习：使用带有标签的数据训练模型，例如分类和回归。
	    - 非监督学习：使用没有标签的数据训练模型，例如聚类和降维。
	    - 强化学习：通过与环境交互来训练模型，使其获得最大的奖励。
	    - Scikit-Learn：一个Python机器学习库，提供了常用的算法和工具。
	    - Keras：一个高级神经网络API，可以运行在TensorFlow等后端之上。
	    - TensorFlow：一个强大的开源机器学习框架，可以用于构建各种模型。
		 - 机器学习能够被得以使用的巨大前提是，客观事件的发生不是随机的，是遵循一定机制的，当这个机制被以概率化的手段描述了出来，我们就能够用输入去预测输出。
 -  **AI提示词**：
	 - 请你把上述文段进行分点描述，不要删改原文内容







# 1. 第一部分 机器学习的基础知识
>**引言：**

## 1.1. 第1章 机器学习概览

### 1.1.1. 什么是机器学习

- # 1. 定义

- 简单定义：
- 通用定义：
- 专业定义：

> egg:


- # 2. 概念：
- 训练集：
- 训练样例：
- 模型：
- 精度：

### 1.1.2. 为什么使用机器学习
**问题引入**：考虑一下如何使用传统的编程技术编写垃圾邮件过滤器

- # 1. **传统方案**：

![image.png](https://i0.hdslb.com/bfs/article/c247bdeff6b3590cfa72709cf933a546394687087.png)

**方案步骤**：
1. 首先，你会检查垃圾邮件通常是什么样子。你可能会注意到一些
词或短语（例如“4U”“信用卡”“免费”和“惊人的”）往往
会在主题行中出现很多次。也许你还会注意到发件人姓名、电子邮
件正文和电子邮件其他部分中的一些其他模式。
2. 你会为你注意到的每个模式编写一个检测算法，如果检测到许多
这样的模式，你的程序会将电子邮件标记为垃圾邮件。
3. 你会测试程序并重复步骤1和步骤2，直到它足够好可以发布。

**方案不足**：每次出现一种新的垃圾邮件话术时候你都需要重新研究问题编写规则，时间一长这样就变成了规则的堆砌了，这会给维护带来很大麻烦


- # 2. **机器学习**：

相比之下，基于机器学习技术的垃圾邮件过滤器会自动学习词和短语，这些词和短语是垃圾邮件的预测因素，通过与非垃圾邮件比较，检测垃圾邮件中反复出现的词语模式（见图1-2）。

>**优点**：该程序更短，更易于维护，也更准确。

![image.png](https://i0.hdslb.com/bfs/article/131af572d49a5e4ffa2e0a2dfe5e1c80394687087.png)

**状况新增**：如果垃圾邮件发送者注意到他们所有包含“4U”的电子邮件都被阻止了怎么办？他们可能会转而写“For U”。

- 传统编程技术的垃圾邮件过滤器需要更新来标记“For U”电子邮件。如果垃圾邮件发送者一直绕过你的垃圾邮件过滤器，你就**需要不断编写新规则**。

- 基于机器学习技术的垃圾邮件过滤器会自动注意到“For U”在用户标记的垃圾邮件中变得异常频繁，并且<label class="ob-comment" title="" style=""> 在没有人工干预的情况下自动标记垃圾邮件 <input type="checkbox"> <span style=""> 机器学习的优点一 </span></label>（见图1-3）。

![image.png](https://i0.hdslb.com/bfs/article/45b1685d81caad5874abb4bac4815276394687087.png)

>**总结**：传统方法的局限性在与无法面对新的情况完成“自适应”动作，也就是说**它不会根据用户标记这是一封垃圾邮件的动作而去学习这封垃圾邮件为什么垃圾的原因**。而机器学习在用户标记完这是一封垃圾邮件后会学习这个邮件中的内容，然后下次遇到同样内容类型的垃圾邮件时会把它自动分类到垃圾邮件中。


机器学习的另一个亮点是<label class="ob-comment" title="" style=""> 擅于处理对于传统方法过于复杂或没有已知算法的问题 <input type="checkbox"> <span style=""> 机器学习优点二 </span></label>。

>**举例**：考虑语音识别，假设你想编写一个能够区分单词“one”和“two”的简单程序。你可能会注意到单词“two”以高音（“T”）开头，因此你会对测量高音强度的算法进行硬编码并使用它来区分“one”和“two”，但显然这种技术不能扩展到所有的语音识别（人们所处环境不同、语言不同、使用的词汇不同）。最好的解决方案（至少在今天）是根据给定的大量单词录音示例，编写一个能够自我学习的算法。

<label class="ob-comment" title="" style=""> 最后，机器学习可以帮助人类进行学习 <input type="checkbox"> <span style=""> 机器学习优点三 </span></label>（见图1-4）。可以检查ML模型来查看它们学到了什么（尽管这对于某些模型来说可能很棘手）。

>**举例**：一旦垃圾邮件过滤器接受了足够多的垃圾邮件训练，就可以用它列出垃圾邮件预测器的单词和单词组合。有时候这会揭示意想不到的相关性或新趋势，从而更好地理解问题。挖掘大量数据来发现隐藏的模式称为**数据挖掘**，机器学习擅长于此。（）

![image.png](https://i0.hdslb.com/bfs/article/7c11c99e75ab7e11fc0cefaa912b735e394687087.png)

- # 3. 章节小节

根据前面的描述，我们可以发现机器学习在上述任务的完成过程中对比传传统方案具有以下特点。

- 现有解决方案需要有大量微调或一长串规则来解决的问题（机器学习模型通常可以简化代码并且比传统方法执行得更好）——**高效性**

- 使用传统方法无法解决的复杂问题（最好的机器学习技术可能会找到解决方案）——**通用性**

- 变化的环境（机器学习系统可以很容易地根据新数据重新训练，始终保持最新状态）——**适应性**

- 深入了解复杂问题和大量数据——**分析性**


### 1.1.3. 应用示例

- **分析产品图像以自动分类：** 这是一项图像分类任务，通常使用卷积神经网络（CNN，见第14章）实现，有时也使用Transformer（见第16章）。

![image.png](https://i0.hdslb.com/bfs/article/f7b2ba34541a285d2e4b2db51b691bf0394687087.png)

>**举例**：假设你有一个在线服装商店，你希望自动对上传的产品图片进行分类，以便在网站上更好地组织和展示商品。

1. **数据准备：** 首先，你需要收集大量的服装图片，并为每张图片打上标签，例如“衬衫”、“裤子”、“裙子”、“鞋子”等。这些带标签的图片将作为训练数据。
2. **模型选择：** 选择一个合适的 CNN 模型，例如 ResNet、VGGNet 或 EfficientNet。这些模型已经在大型图像数据集上进行了预训练，可以直接用于你的任务，或者进行微调。
3. **模型训练：** 使用准备好的训练数据，对 CNN 模型进行训练。训练过程中，模型会学习图像的特征，并将其与相应的类别关联起来。
4. **模型评估：** 使用测试数据集评估模型的性能。测试数据集包含模型未见过的新图片，用于检验模型的泛化能力。常用的评估指标包括准确率、精确率、召回率和 F1 值。
5. **模型部署：** 将训练好的模型部署到你的在线商店的服务器上。当用户上传一张新的服装图片时，模型会自动对其进行分类，并将结果返回给网站。
6. **结果展示：** 网站根据模型返回的分类结果，将商品展示在相应的类别下。例如，如果模型将一张图片分类为“衬衫”，则该商品会显示在“衬衫”类别下。



- **通过脑部扫描发现肿瘤：** 这属于语义图像分割，需要对图像中的每个像素进行分类以确定肿瘤的确切位置和形状，通常使用CNN或Transformer完成。

![image.png](https://i0.hdslb.com/bfs/article/f7e3234c1b0789b969555127258be42f394687087.png)


- **自动分类新闻文章：** 这是==自然语言处理==（NLP）中的文本分类任务，可以使用循环神经网络（RNN）和CNN，但Transformer（见第16章）通常效果更优。

![image.png](https://i0.hdslb.com/bfs/article/8b5bdee51be0b8757b69e77c04bb14c3394687087.png)

- **自动标记论坛中的不当评论：** 这同样是==文本分类任务==，采用与新闻分类相同的NLP工具。

![image.png](https://i0.hdslb.com/bfs/article/6f9cb3f5431c9a4f21005f801934d470394687087.png)

- **自动总结长文档：** 这是NLP领域的一个分支，称为==文本摘要任务==，也使用上述NLP工具。

- **创建聊天机器人或个人助理：** 这需要整合多个NLP组件，包括自然语言理解（NLU）和问答模块。

- **基于绩效指标预测公司年收入：** 这是一项==回归任务==（预测数值），可以使用多种回归模型，如线性或多项式回归（见第4章）、支持向量机回归（见第5章）、随机森林回归（见第7章）或人工神经网络（见第10章）。若考虑历史绩效指标，可能需要RNN、CNN或Transformer（见第15章和第16章）。

- **让应用程序响应语音命令：** 这是==语音识别任务==，需要处理长而复杂的音频样本序列，常用RNN、CNN或Transformer（见第15章和第16章）。

- **检测信用卡欺诈：** 这属于==异常检测任务==，可以使用隔离森林、高斯混合模型（见第9章）或自动编码器（见第17章）等技术。

- **基于购买历史进行客户细分：** 这是一项==聚类任务==，旨在将客户分组以便制定不同的营销策略，可以使用k均值、DBSCAN等算法（见第9章）。

- **可视化复杂高维数据集：** 这是数==据可视化任务==，常涉及降维技术（见第8章），以便在清晰的图表中呈现数据。

- **基于购买历史推荐产品：** 这属于==推荐系统任务==。一种方法是使用人工神经网络（见第10章），输入客户历史购买等信息，预测其下次可能购买的商品。这类网络通常基于所有客户的购买记录训练。

- **为游戏构建智能机器人：** 这通常通过强化学习（RL；见第18章）实现。RL训练智能体在给定环境中选择能最大化长期奖励的动作。著名的AlphaGo程序就是用RL构建的。



### 1.1.4. 机器学习系统的类型

现有的机器学习系统类型繁多，为便于理解，我们根据以下标准将它们分为大类（主要有三个TAG类别）

*   **训练方式TAG：** 机器学习系统根据训练期间的监督程度进行分类，主要包括监督学习、无监督学习、半监督学习和自监督学习等。
	
    *   **监督学习：** 在监督学习中，==训练数据包含输入特征和对应的标签（或目标）==。算法通过学习输入和输出之间的映射关系来进行预测。常见的监督学习任务包括分类和回归。
        *   **例子：** 垃圾邮件过滤器（分类），房价预测（回归）。
    *   **无监督学习：** 在无监督学习中，==训练数据只有输入特征，没有标签==。==算法需要自行发现数据中的结构和模式==。常见的无监督学习任务包括聚类、降维和关联规则挖掘。
        *   **例子：** 客户细分（聚类），PCA 降维，购物篮分析。
    *   **半监督学习：** ==半监督学习的训练数据既包含有标签的样本，也包含无标签的样本。这种方法通常在获取标签成本较高时使用==。算法可以利用有标签数据来提高模型性能，同时利用无标签数据来发现数据的潜在结构。
        *   **例子：** 在图像分类中，只有部分图像有标签，其余图像没有标签。
    *   **自监督学习：** 自监督学习是一种特殊的监督学习，==其中标签是从输入数据本身生成的==。算法通过解决一个预定义的任务来学习数据的表示，例如图像着色、图像修复或预测视频中的下一个帧。
        *   **例子：** BERT 模型中的 Masked Language Model 任务。
	
*   **学习方式TAG：** 机器学习系统可以分为**批量学习**和**在线学习**。
    *   **批量学习：** 批量学习（或离线学习）需要在所有数据准备就绪后进行一次性训练。模型在整个训练数据集上进行训练，然后部署到生产环境中。批量学习适用于数据量较小且相对静态的场景。
        *   **优点：** 训练过程稳定，可以充分利用所有数据。
        *   **缺点：** 无法适应新数据，需要重新训练模型。
        *   **例子：** 图像识别，自然语言处理。
    *   **在线学习：** 在线学习系统能够即时地从新数据中进行增量学习。每个新的数据样本都会被用来更新模型，然后该样本可以被丢弃。在线学习适用于数据流式输入或需要快速适应变化的环境。
        *   **优点：** 能够适应新数据，节省存储空间。
        *   **缺点：** 对噪声数据敏感，容易受到灾难性遗忘的影响。
        *   **例子：** 股票价格预测，推荐系统。
	
*   **工作原理TAG：** 机器学习系统的工作方式可以分为基于实例的学习和基于模型的学习。
	
    *   **基于实例的学习：** 基于实例的学习（或基于记忆的学习）通过比较新数据和已知数据进行预测。算法会存储训练数据，并在预测时找到与新数据最相似的样本，然后根据这些样本的标签进行预测。
        *   **优点：** 简单易懂，易于实现。
        *   **缺点：** 需要存储大量数据，预测速度慢，对特征缩放敏感。
        *   **例子：** K 近邻算法。
    *   **基于模型的学习：** 基于模型的学习则通过在训练数据中发现模式并构建预测模型来实现。算法会学习一个函数，该函数可以将输入特征映射到输出标签。
        *   **优点：** 预测速度快，泛化能力强。
        *   **缺点：** 模型复杂，需要大量的训练数据。
        *   **例子：** 线性回归，逻辑回归，神经网络。


>**提示：** 这些分类标准可以灵活组合使用，==例如一个垃圾邮件过滤器可以同时是一个在线的、基于模型的监督学习系统==。
#### （1）训练监督

| 特征       | 监督学习 (Supervised Learning)                | 无监督学习 (Unsupervised Learning)                               | 半监督学习 (Semi-supervised Learning) | 自监督学习 (Self-supervised Learning)        | 强化学习 (Reinforcement Learning)         |
| -------- | ----------------------------------------- | ----------------------------------------------------------- | -------------------------------- | --------------------------------------- | ------------------------------------- |
| **训练数据** | **已标记**的数据集 (输入特征 + 正确的输出/标签)             | **未标记**的数据集 (只有输入特征)                                        | **部分标记，部分未标记**的数据集               | **未标记**的数据集 (但会生成伪标签)                   | **与环境交互产生的经验** (状态、动作、奖励)             |
| **学习方式** | 从标记数据中学习输入到输出的映射函数，目标是预测新数据的输出。           | 从未标记数据中发现隐藏的结构、模式和关系。                                       | 利用少量标记数据指导对大量未标记数据的学习。           | 通过人为设计的预任务，从数据自身生成标签进行学习。               | 通过与环境的交互，学习最大化累积奖励的策略。                |
| **主要任务** | **分类** (将数据分到预定义的类别)，**回归** (预测连续数值)      | **聚类** (发现数据中的分组)，**降维** (简化数据)，**异常检测**，**关联规则学习**，**可视化** | **分类**和**回归** (利用少量标签提升性能)       | **特征学习** (学习有用的数据表示，常用于后续监督任务的预训练)      | **决策制定** (学习在不同状态下采取何种动作以获得最大回报)      |
| **标签需求** | **高** (需要大量准确标记的数据)                       | **无** (不需要任何标签)                                             | **中等** (需要少量标记数据)                | **无** (但需要设计合适的预任务)                     | **间接** (通过奖励信号来指导学习)                  |
| **模型评估** | 通常使用准确率、精确率、召回率、F1分数 (分类)；均方误差、R方 (回归)等指标 | 通常使用内部评估指标 (如轮廓系数、DB指数) 或结合下游任务的性能进行评估                      | 通常结合监督和无监督的评估方法                  | 通常通过下游任务的性能来评估学习到的表示                    | 通常使用累积奖励、平均奖励等指标                      |
| **典型应用** | 垃圾邮件过滤，图像识别，疾病诊断，股票价格预测，信用评分等。            | 客户细分，市场篮子分析，社交网络分析，基因表达分析，异常交易检测等。                          | 文本分类，图像识别，语音识别 (在标记数据稀疏时)        | 图像修复，视频着色，自然语言处理中的掩码语言模型 (BERT) 等预训练任务。 | 游戏AI (AlphaGo)，机器人控制，自动驾驶，推荐系统，资源管理等。 |
| **学习难度** | 相对直接 (基于已有的正确答案学习)                        | 更具挑战性 (需要算法自行发现结构)                                          | 介于监督和无监督之间                       | 依赖于预任务的设计，可能需要大量计算资源                    | 复杂 (涉及探索与利用的平衡，以及信用分配问题)              |
<center><b>表：五种机器学习方式的对比表</b></center>

##### 1）监督学习

```ad-note
title:本节概念
- **相关概念：**
    - **“目标 (target)”** 和 **“标签 (label)”** 这两个词在监督学习中通常被视为同义词。
	    - “目标”在回归任务中更常见。
	    - “标签”在分类任务中更常见。
    - **“特征 (feature)”** 有时也被称为“预测变量 (predictor)” 或 “属性 (attribute)”。
- **概念范围：** 这些术语（标签、目标、特征、预测变量、属性）==可以指代单个样本的属性==（例如，“这辆车的里程特征是等于15000”），==也可以指代所有样本的整体特性==（例如，“里程特征与价格密切相关”）。
```

```ad-question
title:**标签和特征不是同一个概念吗？**
你说得很对！在监督学习中，“目标 (target)” 和 “标签 (label)” 这**两个词确实经常被当作同义词使用，尤其是在非正式的语境中**。你观察到的用法倾向也是准确的：

- 目标 (target) 更常用于回归 (regression) 任务中，因为我们预测的是一个连续的数值。这个数值就是我们想要达成的“目标”。
- 标签 (label) 更常用于分类 (classification) 任务中，因为我们预测的是一个离散的类别。这个类别就是我们给数据点贴上的“标签”。

至于你提到的“特征 (features)”、“预测变量 (predictors)” 和 “属性 (attributes)”，它们也常常指代同一个概念，即用来描述数据的输入变量，模型会利用这些变量来预测目标或标签。

现在来回答你的核心问题：“标签和特征不说的是同一个东西么？”


绝对不是！标签（或目标）和特征是监督学习中两个完全不同的概念：

- 特征 (Features)：
	- 描述输入数据的属性或特性。
	- 是模型用来进行预测的依据。
	- 通常是数据集的列（在表格数据中）。
	- 对于一个给定的样本（例如，一封邮件），特征可以是邮件的长度、包含特定关键词的次数、发件人域名等等。

- 标签 (Labels) 或 目标 (Targets)：
	- 是我们想要预测的输出变量。
	- 是模型在训练过程中学习的“正确答案”。
	- 对于一个给定的样本，标签是该样本的真实类别（在分类中，例如“垃圾邮件”或“非垃圾邮件”）或真实数值（在回归中，例如房价）。



用一个简单的例子来说明：假设我们想要构建一个模型来预测房价（回归任务）。

- 特征 (Features) 可能包括：房屋的面积、卧室的数量、地理位置、建造年份等等。
- 目标 (Target) 就是我们想要预测的房价（一个连续的数值）。

现在，假设我们想要构建一个模型来识别图片中的动物是猫还是狗（分类任务）。

- 特征 (Features) 可能包括：图片的像素值、提取的边缘信息、纹理特征等等。
- 标签 (Label) 就是图片中真实的动物类别（“猫”或“狗”）。
   



**总结一下**：

- 特征是输入，是模型学习的原材料。
- 标签/目标是输出，是模型需要预测的答案。

它们在监督学习中扮演着截然不同的角色，模型通过学习特征和标签之间的关系来进行预测。你对这些术语的理解非常到位！
```

- # 1. 监督学习的定义
在监督学习中，算法接收到的训练集包含所需的解决方案，==这些解决方案被称为标签==。 图1-5展示了一个用于垃圾邮件分类的已标记训练集，这是监督学习的一个典型例子。

![image.png](https://i0.hdslb.com/bfs/article/3de964ff67017fc9dc6d4c4e52a1110e394687087.png)

- # 2. 监督学习的特点
	
	- **有标签数据：** 监督学习使用带有标签的数据集进行训练，其中标签是正确的输出或目标值。
	- **目标明确：** 监督学习的目标是学习一个将输入映射到输出的函数，以便能够对新的、未见过的数据进行准确的预测。
	- **反馈机制：** 在训练过程中，模型会根据其预测结果与实际标签之间的差异获得反馈，并据此调整自身参数。
	- **分类和回归：** 监督学习主要解决两类问题：分类（将数据分到不同的类别）和回归（预测连续的数值）。
	- **泛化能力：** 监督学习的目标是训练出一个具有良好泛化能力的模型，即在未见过的数据上也能表现良好。


- # 3. 监督学习应用：分类 (Classification)
	
    - **目标：** 学习如何对新的、未见过的电子邮件进行分类。
    - **方法：** 算法通过学习邮件的特征（例如，关键词、发件人、主题）与对应类别之间的关系来进行分类。
    - **示例：** 垃圾邮件过滤器是一个典型的监督学习应用。
    - **应用场景：** 自动识别和过滤垃圾邮件，提高用户体验。
    - **优点：** 能够有效地区分不同类别的邮件，减少用户手动筛选的工作量。
        - 能够自动识别垃圾邮件，减少用户手动筛选的工作量。
        - 可以根据用户反馈不断学习，提高过滤准确率。
        - 节省用户的时间和精力，提高工作效率。

![image.png](https://i0.hdslb.com/bfs/article/3de964ff67017fc9dc6d4c4e52a1110e394687087.png)

- # 3. 监督学习应用：回归 (Regression)
	
    - **目标：** 在给定一组特征（例如，里程、年龄、品牌等）的情况下，预测汽车的价格。
    - **方法：** 算法通过学习汽车的特征与价格之间的关系来进行预测。
    - **示例：** 汽车价格预测。
    - **图示参考：** 图1-6展示了一个回归问题，即在给定输入特征的情况下预测数值（通常有多个输入特征，有时也有多个输出值）。
    - **应用场景：** 帮助消费者和销售商评估汽车的合理价格，提高交易效率。
    - **优点：**
        - 能够根据市场行情和车辆状况，给出较为准确的价格评估。
        - 可以帮助消费者更好地了解汽车的价值，避免被不合理报价所欺骗。
        - 提高交易效率，减少交易双方的信息不对称。

![image.png](https://i0.hdslb.com/bfs/article/ebe39d9131aa957ce44e81bff5dbc0f3394687087.png)

```ad-note
title:**“回归”一词的有趣来源：**

- “回归”这个术语由Francis Galton引入，最初用于描述高个子孩子往往比父母矮的统计现象。
- Galton将这种“孩子身高回归到人群平均身高”的现象称为“回归到均值”。
- 后来，他将这个术语应用于分析变量之间相关性的方法上。
```

>**注意**：某些回归模型可以用于分类任务，反之亦然（将标签与具体的身份数值相联系）。例如，逻辑回归通常被用于分类，因为它能够预测属于特定类别的概率（例如，20%的概率是垃圾邮件）。

```ad-question
title:**怎么理解某些些回归模型可以用于分类任务，反之亦然？**
**1. 回归模型用于分类任务：**

回归模型通常输出的是**连续的数值**，而分类任务的目标是预测**离散的类别标签**。那么，如何将一个输出连续值的回归模型应用于分类呢？主要有以下几种方式：

- **设定阈值 (Thresholding):**
    
    - 回归模型预测出一个连续值。
    - 我们人为地设定一个或多个阈值。
    - 根据预测值落在哪个阈值区间内，将其映射到不同的类别。
    - **例子：** 预测学生考试分数的回归模型，输出范围可能是0-100。我们可以设定一个阈值60分。如果模型预测分数大于等于60，则分类为“及格”；小于60，则分类为“不及格”。
- **概率输出 (Probability Interpretation):**
    
    - 某些回归模型，如**逻辑回归 (Logistic Regression)**，虽然名字里有“回归”，但其核心机制是通过一个Sigmoid函数将线性模型的输出转化为0到1之间的概率值。
    - 这个概率值可以直接解释为属于某个类别的可能性。
    - 通过设定一个概率阈值（通常是0.5），可以将概率值转化为二分类的预测结果。例如，概率大于0.5预测为类别A，小于0.5预测为类别B。
    - **注意：** 逻辑回归虽然在数学形式上与线性回归有关，但其设计目标和输出解释使其成为一个标准的分类模型。这里提到它是为了说明“回归模型”这个名称有时会让人产生误解。
- **多输出回归 (Multi-output Regression):**
    
    - 一些回归模型可以同时预测多个连续值。
    - 在分类任务中，每个输出值可以代表属于某个类别的置信度或得分。
    - 通过比较这些输出值，选择具有最高置信度/得分的类别作为预测结果。
    - **例子：** 一个预测图像属于猫、狗、鸟的回归模型，可能会输出三个值，分别代表属于这三个类别的“可能性得分”。我们可以选择得分最高的类别作为最终预测。

**2. 分类模型用于回归任务：**

分类模型通常输出的是**离散的类别标签**或属于每个类别的**概率**。如何将一个输出离散值的分类模型应用于回归呢？这通常不如回归模型用于分类常见，但也有一些方法：

- **将类别标签映射为数值 (Label Encoding and Treating as Continuous):**
    
    - 如果分类的类别是有序的（例如，低、中、高），我们可以将这些类别标签映射为数值（例如，0、1、2）。
    - 然后，我们可以将分类模型（例如，决策树分类器）的输出视为这些数值，从而进行“回归”。
    - **缺点：** 这种方法假设类别之间存在均匀的数值间隔，这在实际情况中可能并不成立，会损失很多信息。
- **概率作为预测值 (Probability as Regression Target):**
    
    - 某些分类模型可以输出属于每个类别的概率。
    - 在某些特定的场景下，我们可以将这些概率值本身作为回归的预测目标。
    - **例子：** 预测用户对某个商品的购买意愿，分类模型可能输出“购买”和“不购买”的概率。我们可以将“购买”的概率值（例如，0到1之间）作为回归的预测值。
- **集成方法 (Ensemble Methods):**
    
    - 通过特定的集成方法，例如将多个分类器的输出进行某种形式的平均或加权，可以得到一个连续的预测值。
    - 这种方法相对复杂，并且通常更侧重于提升分类性能，而不是直接将分类器作为回归器使用。

**总结：**

模型类型的划分并非绝对的，模型的底层机制和输出形式决定了其主要的应用领域。然而，通过一些巧妙的方法和解释，某些回归模型可以产生可用于分类的输出（例如，通过阈值或概率），而将分类模型应用于回归任务则相对更具挑战性，并且通常会引入一些假设和局限性。逻辑回归是一个很好的例子，它模糊了回归和分类之间的界限，专注于预测概率以进行分类。
```

- # 4. 基于监督学习的算法
	**分类算法：**
	
	1. **逻辑回归 (Logistic Regression):**
	    
	    - **模型：** 使用 sigmoid 函数将线性模型的输出转换为概率值，即 , 其中 z 是线性模型的输出，p 是属于某个类别的概率。
	    - **适用场景：** 适用于二分类问题，例如垃圾邮件过滤、疾病诊断等。
	2. **支持向量机 (Support Vector Machine, SVM):**
	    
	    - **模型：** 通过找到一个最优的超平面来分隔不同类别的数据，或者通过核函数将数据映射到高维空间，使其线性可分。
	    - **适用场景：** 适用于高维数据、非线性可分数据等复杂问题。
	3. **决策树 (Decision Tree):**
	    
	    - **模型：** 基于树结构的分类算法，通过一系列的 if-then-else 规则来进行决策。
	    - **适用场景：** 适用于数据量较小、特征维度较低的问题。
	4. **随机森林 (Random Forest):**
	    
	    - **模型：** 集成学习算法，通过构建多个决策树并进行投票来进行预测。
	    - **适用场景：** 适用于各种分类问题，具有较高的精度和鲁棒性。
	5. **神经网络 (Neural Network):**
	    
	    - **模型：** 由多个神经元相互连接而成的复杂网络结构，可以学习复杂的非线性关系。
	    - **适用场景：** 适用于各种分类问题，尤其在图像识别、自然语言处理等领域表现出色。
	
	**回归算法：**
	
	6. **线性回归 (Linear Regression):**
	    
	    - **模型：** 假设输入特征和输出之间存在线性关系，即 , 其中 y 是预测值，x 是输入特征，θ 是模型参数。
	    - **适用场景：** 适用于回归问题，例如房价预测、销售额预测等。
	7. **多项式回归 (Polynomial Regression):**
	    
	    - **模型：** 扩展了线性回归，允许输入特征的非线性组合，例如 
	    - **适用场景：** 适用于数据之间存在非线性关系的情况。
	8. **支持向量回归 (Support Vector Regression, SVR):**
	    
	    - **模型：** 与 SVM 类似，但用于回归问题，通过找到一个函数，使得尽可能多的数据点落在该函数周围的一个管道内。
	    - **适用场景：** 适用于高维数据、非线性回归问题。
	9. **决策树回归 (Decision Tree Regression):**
	    
	    - **模型：** 与决策树类似，但用于回归问题，通过一系列的 if-then-else 规则来预测连续值。
	    - **适用场景：** 适用于数据量较小、特征维度较低的回归问题。
	10. **随机森林回归 (Random Forest Regression):**
	    
	    - **模型：** 与随机森林类似，但用于回归问题，通过构建多个决策树并进行平均来进行预测。
	    - **适用场景：** 适用于各种回归问题，具有较高的精度和鲁棒性。
	11. **神经网络 (Neural Network):**
	    
		- **模型：** 由多个神经元相互连接而成的复杂网络结构，可以学习复杂的非线性关系。
		- **适用场景：** 适用于各种回归问题，尤其在时间序列预测等领域表现出色。


```ad-abstract
title:**其他说明**
- 监督学习算法需要使用带有标签的数据集进行训练。
- 监督学习算法的目标是学习一个将输入特征映射到输出标签的函数。
- 监督学习算法可以用于解决分类和回归两类问题。
```




##### 2）无监督学习

- **无监督学习的定义：** 在无监督学习中，训练数据是未标记的，打比方就像系统试图在没有任何“老师”提供正确答案的情况下自行学习。

![image.png](https://i0.hdslb.com/bfs/article/5ab36667af756b5cd816a78c0867ec07394687087.png)

- **无监督学习的典型特点：**
    - **数据驱动的模式发现：**
        - 算法能够自主地从数据中发现隐藏的结构和模式，无需人工干预。
        - 适用于探索性数据分析和知识发现。
    - **对数据质量要求较低：**
        - 不需要标记数据，可以处理大量未标记的数据。
        - 对数据中的噪声和异常值具有一定的鲁棒性。
    - **结果解释性较差：**
        - 算法发现的模式可能难以解释或理解。
        - 需要结合领域知识和可视化技术进行分析。

- **典型无监督学习应用：聚类 (Clustering)**
    - **示例场景：** 分析大量博客访客数据。
    - **目标：** 使用聚类算法检测相似访客的分组（如图1-8所示）。
    - **学习方式：** 算法无需人为告知访客属于哪个组，而是自行发现数据中的内在关联。
    - **聚类结果示例：**
        - 可能发现40%的访客是喜欢漫画书的青少年，通常在放学后阅读博客。
        - 可能发现20%的访客是喜欢科幻小说的成年人，通常在周末访问。
    - **层次聚类：** 可以将每个大的分组进一步细分为更小的子组。
    - **应用价值：** 帮助根据不同的访客群体定制博客内容。

![image.png](https://i0.hdslb.com/bfs/article/9c1949fcc31eab3699005de35fef3c0d394687087.png)



- **典型无监督学习应用：可视化算法 (Visualization Algorithms)**
    - **输入：** 大量复杂且未标记的数据。
    - **输出：** 算法生成数据的2D或3D表示（如图1-9所示）。
    - **学习目标：** 在低维空间中尽可能保留原始数据中的结构，例如避免不同集群的重叠。
    - **应用价值：** 便于理解数据的组织方式，并可能发现未知的模式。
    - **图1-9的示例说明：** 动物与车辆在可视化图中可能距离较远，而马与鹿距离较近，但都与鸟的距离较远。
    - **图的来源许可：** 图1-9的使用获得了Richard Socher等人的许可，出自他们的论文。


![image.png](https://i0.hdslb.com/bfs/article/a34bee4fc6dce7a8a76aff366ef44731394687087.png)
![image.png](https://i0.hdslb.com/bfs/article/e2880bb390be58519c097df01cb08a5e394687087.png)

- **相关无监督学习应用：降维 (Dimensionality Reduction)**
    - **目标：** 在不丢失过多信息的前提下简化数据。
    - **方法：** 将几个相关的特征合并为一个。
    - **示例：** 合并行驶里程和车龄这两个强相关的特征，得到一个代表汽车磨损程度的新特征。
    - **术语：** 这种合并过程称为特征提取 (Feature Extraction)。
    - **应用场景：** 在将训练数据提供给其他机器学习算法（如监督学习算法）之前，进行降维通常是有益的。
    - **优点：**
        - 加快算法运行速度。
        - 减少数据占用的磁盘和内存空间。
        - 在某些情况下，可以提升模型性能。


- **相关无监督学习应用：异常检测 (Anomaly Detection)**
    - **目标：** 识别数据中不寻常或异常的实例。
    - **应用示例：**
        - 检测欺诈性的信用卡交易。
        - 发现制造过程中的缺陷。
        - 在预处理阶段自动移除数据集中的异常值。
    - **学习方式：** 系统通过学习正常实例的模式来识别异常实例（如图1-10所示）。
    - **检测过程：** 当遇到新实例时，判断其是否符合已学习到的正常模式。

![image.png](https://i0.hdslb.com/bfs/article/438a030ef9c1b95c7713a66c71e1d6fd394687087.png)



- **相关无监督学习应用：新颖性检测 (Novelty Detection)**
    - **目标：** 检测与训练集中所有实例都显著不同的新实例。
    - **关键要求：** 训练集需要非常“干净”，不包含任何希望算法检测到的异常实例。
    - **与异常检测的区别：**
        - **示例：** 如果训练集中包含少量吉娃娃犬的照片（占1%），新颖性检测算法不应将新的吉娃娃犬照片视为新颖。
        - **示例：** 异常检测算法可能会将这些稀有的、与其他狗差异较大的吉娃娃犬标记为异常。


- **相关无监督学习应用：关联规则学习 (Association Rule Learning)**
    - **目标：** 在大量数据中挖掘属性之间有趣的关联关系。
    - **示例场景：** 分析超市的销售日志。
    - **发现的规则示例：** 购买烧烤酱和薯片的顾客也倾向于购买牛排。
    - **应用价值：** 可以根据发现的关联性调整商品摆放策略。

##### 3）半监督学习

- **半监督学习的定义：** 由于标记数据通常既耗时又昂贵，因此你通常会有很多未标记的实例而很少有已标记的实例。一些算法可以处理一部分已标记的数据。这称为半监督学习（见图1-11）。打比方就像系统在少量老师的指导下，利用大量的自学材料进行学习。
- **核心思想**：
- **学习过程**：

![image.png](https://i0.hdslb.com/bfs/article/546661d7d3fb7e485b3690cb5deaefff394687087.png)

- **半监督学习的典型特点：** 具有两个类别（三角形和正方形）的半监督学习——未标记的样例（圆形）有助于将新实例（十字）分类到三角形而不是正方形，即使它更接近标记的正方形。==这就像虽然十字离标记的正方形更近，但周围大量未标记的圆形样本都靠近三角形，暗示十字也可能属于三角形==。
    
    
- **半监督学习应用：照片托管服务（例如Google相册）**
    
    - **无监督的初步识别：** 一旦你将所有的家庭照片上传到该服务，它会自动识别出同一个人A出现在照片1、5和11中，而另一个人B出现在照片2、5和7中。这是算法（聚类）的无监督部分。这就像算法自己先将照片按照相似度分组，发现哪些照片里的人脸比较像。
    - **监督的人工干预：** 现在系统只需要你告诉它这些人是谁。你只需为每个人添加一个标签，系统就可以为每张照片中的每个人命名，这对于搜索照片非常有用。这就像你告诉系统“这个组里的人是爷爷”，系统就能自动标记所有包含爷爷的照片。

- **算法构成：** 大多数半监督学习算法是无监督和监督算法的组合。例如，可以使用聚类算法将相似的实例分组在一起，然后每个未标记的实例都可以用其集群中最常见的标签进行标记。一旦标记了整个数据集，就可以使用任何监督学习算法。这好比先用无监督学习将数据大致分类，然后用少量的标签信息来指导这些类别的具体含义，最终就可以像监督学习一样进行更精确的预测。

- **实际挑战：** 这就是系统运行良好的情况。在实践中，它通常会为每个人创建几个集群，有时会将两个看起来相似的人混合在一起，因此你可能需要为每个人提供一些标签并手动清理一些集群。这好比算法初步将照片分组，但有时会将长得很像的亲戚分到不同组，需要人工稍微纠正一下。

##### 4）自监督学习

- **自监督学习的定义：** 这种==通过自身数据生成标签进行学习的方法称为自监督学习==。
- **核心思想：** 从完全未标记的数据集中生成完全标记的数据集。
- **学习过程：** 一旦生成了标记数据集，就可以使用任何监督学习算法进行训练。

- **自监督学习应用：图像修复**
    - **数据准备：** 拥有一个很大的未标记图像数据集。
    - **标签生成：** 随机屏蔽每个图像的一小部分，将屏蔽的图像作为输入，原始完整图像作为标签（如图1-12所示）。
    - **模型训练：** 训练一个模型来恢复被屏蔽掉的部分，使其尽可能接近原始图像。
- **自监督学习的潜在应用：**
    - 生成的模型本身可能很有用，例如用于修复损坏的图像或从图片中删除不想要的对象。
- **自监督学习的主要目标：**
    - 通常情况下，使用自监督学习训练的模型本身并不是最终目标。
    - 更常见的是，针对稍微不同的、用户真正关心的任务来调整和微调模型。

![image.png](https://i0.hdslb.com/bfs/article/a909792f20651a47a66287053505a041394687087.png)

- **自监督学习应用：宠物分类**
    - **目标任务：** 构建一个宠物分类模型，能够识别照片中的宠物物种。
    - **自监督预训练：** 利用大量的未标记宠物照片数据集，先训练一个图像修复模型。
    - **预训练的意义：** 如果修复模型表现良好，它应该已经学习到区分不同宠物种类的能力（例如，修复猫脸时不会生成狗脸）。
    - **迁移学习：** 假设模型架构允许（大多数神经网络架构都允许），可以将预训练模型调整用于宠物种类预测任务。
    - **微调：** 最后一步是在已标记的宠物照片数据集上微调模型。由于模型已经具备了识别不同宠物特征的基础知识，微调过程只需要学习这些特征与物种标签之间的映射。

```ad-question
title:**自监督学习与与前面三种监督学习的关联**
- 有些人认为自监督学习是无监督学习的一部分，因为它处理的是未标记的数据集。
- 然而，自监督学习在训练期间使用（生成的）标签，因此在这一点上更接近于监督学习。
- “无监督学习”通常用于指代聚类、降维或异常检测等任务。
- 自监督学习主要关注与监督学习相同的任务，即分类和回归。

> **结论：** 最好将自监督学习视为一个独立的机器学习类别。
```

- **迁移学习的重要性：**
    - 将从一项任务中学到的知识转移到另一项相关任务称为迁移学习。
    - 迁移学习是当今机器学习中最重要的技术之一，尤其是在使用深度神经网络时。
    - 将在第二部分详细讨论迁移学习。

##### 5）强化学习

```ad-note
title:**相关概念**
- **核心概念 - 智能体 (Agent)：** 强化学习系统在上下文中被称为智能体。
- **与环境的交互：** 智能体通过观察环境，选择并执行动作来与环境互动。
- **奖励与惩罚 (Reward and Punishment)：** 智能体在执行动作后会获得环境的反馈，这种反馈以奖励（正回报）或惩罚（负回报）的形式出现（见图1-13）。
- **学习目标 - 策略 (Policy)：** 智能体的目标是自行学习最优的行为方式，即策略。策略定义了在给定情况下智能体应该选择哪个动作，以最大化其随时间累积的总回报。
```

- **强化学习的定义：** 强化学习是一种与监督学习、无监督学习和半监督学习截然不同的机器学习方法。这个学习系统（在此上下文中称为智能体）可以观察环境，选择和执行动作，并获得回报（或负回报形式的惩罚，见图1-13）。然后它必须自行学习什么是最好的方法，称为策略，以便随着时间的推移获得最大的回报。策略定义了智能体在给定情况下应该选择的动作。


![image.png](https://i0.hdslb.com/bfs/article/1909bd73f8e733d90542f9f3b9f1de42394687087.png)

- **强化学习应用：机器人行走：** 许多机器人利用强化学习算法来学习如何有效地行走。


- **强化学习应用： DeepMind的AlphaGo：**
    - AlphaGo程序是强化学习的一个杰出例子。
    - 它在2017年5月的围棋比赛中击败了当时世界排名第一的围棋选手柯洁，引起了广泛关注。
    - AlphaGo的学习方式：通过分析数百万场围棋比赛的棋谱，然后让程序自己与自己进行无数次对弈，从而学习到了高超的围棋策略。
- **学习阶段与应用阶段的区别：** 需要注意的是，在AlphaGo与人类冠军比赛时，其学习过程是关闭的。在比赛中，AlphaGo仅仅是在应用它在训练阶段学到的策略。
- **离线学习 (Offline Learning)：** 正如后续章节（1.4.2节）将介绍的那样，AlphaGo这种先学习策略再进行应用的模式被称为离线学习。


#### （2）离线、在线学习
>**引言**：对机器学习系统进行分类的另一个标准是系统能否从输入数据流中进行增量学习，而增量学习主要有离线（批量）学习和在线（增量）学习两种方式。
##### 1）离线学习
```ad-note
title:**相关概念**
- **资源消耗与时间成本：** 批量学习通常需要大量的时间和计算资源。
- **训练方式：** 系统通常离线完成训练，即先训练好整个模型，然后部署到生产环境运行，之后不再进行学习。
```

- **批量学习的定义：** 在批量学习中，系统无法进行增量学习，必须使用所有可用的数据进行训练，因此也称之为**离线学习**

- **批量学习的特点：** 训练完成后，模型只是应用它所学到的知识，不再根据新数据进行调整。

- **模型性能衰退及原因**
	- **模型性能衰退 (模型腐烂/数据漂移)：** 模型的性能往往会随着时间的推移而逐渐下降，因为现实世界不断变化，而模型却保持不变。
	- **模型性能衰退的原因：**
	    - 世界的演进发展导致数据分布发生变化。
	    - 对于猫狗分类模型，性能衰减可能较慢。
	    - 对于预测金融市场等快速变化的系统，性能衰减可能较快。
	    - 即使是猫狗分类模型也可能需要定期重新训练，原因包括：
	        - 相机技术的变化（图像格式、清晰度、亮度、大小比例）。
	        - 用户偏好的变化（喜欢的宠物种类）。
	        - 出现新的数据模式（例如，给宠物戴帽子）。

- **应对新数据与自动化流程**
	- **应对新数据：** 如果要让批量学习系统理解新数据（例如新型垃圾邮件），需要在完整数据集（包括新旧数据）上从头开始训练新版本的系统，然后替换旧模型。
	- **自动化流程：** 训练、评估和启动机器学习系统的整个过程可以相对容易地自动化（见图1-3），使得批量学习系统也能适应变化。

- **更新频率的权衡**
    - 解决方案是经常更新数据并从头开始训练新版本的系统。
    - 更新频率取决于用例，例如可能每天或每周训练一次。
    - 对于需要适应快速变化数据的系统（如预测股票价格），这种更新频率可能不够。

- **资源限制与替代方案**
	- **计算资源需求：** 在完整的数据集上进行训练需要大量的计算资源（CPU、内存空间、磁盘空间、磁盘I/O、网络I/O等）。
	- **经济成本：** 如果数据量巨大且系统每天都从头开始训练，最终会产生高昂的成本。
	- **大数据集的限制：** 如果数据量非常庞大，可能无法使用批量学习算法。
	- **资源受限场景的局限性：** 对于需要自动学习且资源有限的系统（例如智能手机应用程序或火星上的漫游机器人），携带大量训练数据并占用大量资源进行长时间训练是不切实际的。
	
	- **替代方案：** 在上述情况下，更好的选择是使用能够进行增量学习的算法。


- **基于离线学习的算法**
	1. **线性回归 (Linear Regression):**
	    
	    - **原理：** 线性回归是一种经典的回归算法，它假设输入特征和输出之间存在线性关系。
	    - **训练方式：** 线性回归通常使用最小二乘法或梯度下降法进行训练，需要在所有训练数据上进行一次性训练。
	    - **适用场景：** 适用于数据量较小且相对静态的回归问题。
	2. **逻辑回归 (Logistic Regression):**
	    
	    - **原理：** 逻辑回归是一种经典的分类算法，它使用 sigmoid 函数将线性模型的输出转换为概率值。
	    - **训练方式：** 逻辑回归通常使用梯度下降法或拟牛顿法进行训练，需要在所有训练数据上进行一次性训练。
	    - **适用场景：** 适用于二分类问题，如垃圾邮件过滤、疾病诊断等。
	3. **支持向量机 (Support Vector Machine, SVM):**
	    
	    - **原理：** SVM 是一种强大的分类和回归算法，它通过找到一个最优的超平面来分隔不同类别的数据。
	    - **训练方式：** SVM 的训练过程通常需要求解一个凸优化问题，需要在所有训练数据上进行一次性训练。
	    - **适用场景：** 适用于高维数据、非线性可分数据等复杂问题。
	4. **决策树 (Decision Tree):**
	    
	    - **原理：** 决策树是一种基于树结构的分类和回归算法，它通过一系列的 if-then-else 规则来进行决策。
	    - **训练方式：** 决策树的训练过程通常采用贪心算法，如 CART 算法，需要在所有训练数据上进行一次性训练。
	    - **适用场景：** 适用于数据量较小、特征维度较低的问题。
	5. **随机森林 (Random Forest):**
	    
	    - **原理：** 随机森林是一种集成学习算法，它通过构建多个决策树并进行投票来进行预测。
	    - **训练方式：** 随机森林的训练过程需要在所有训练数据上进行多次抽样和训练，属于离线学习。
	    - **适用场景：** 适用于各种分类和回归问题，具有较高的精度和鲁棒性。
	6. **神经网络 (Neural Network):**
	    
	    - **原理：** 神经网络是一种复杂的机器学习模型，它由多个神经元相互连接而成，可以学习复杂的非线性关系。
	    - **训练方式：** 神经网络的训练过程通常使用反向传播算法，需要在所有训练数据上进行多次迭代，属于离线学习。
	    - **适用场景：** 适用于各种分类、回归和模式识别问题，尤其在图像识别、自然语言处理等领域表现出色。

```ad-abstract
title:**其他说明**
- 离线学习算法通常需要在所有数据准备就绪后进行一次性训练。
- 离线学习算法的训练过程可能需要大量的计算资源和时间。
- 离线学习算法无法适应新数据的变化，需要定期重新训练模型。
```




##### 2）在线学习

```ad-note
title:**相关概念**
- **核外学习：** “核外学习”通常是离线完成的，因此“在线学习”这个名字可能具有误导性。将其视为“增量学习”可能更恰当。
- **学习率：** 适应不断变化的数据的速度，如果设置的学习率很高，那么系统会快速适应新数据，但它也会很快忘记旧数据（并且你也不希望垃圾邮件过滤器只标记最新类型的垃圾邮件）。反之，如果设置的学习率很低，那么系统会有更多的惰性，也就是说，它会学习得更慢，但它对新数据中的噪声或非典型数据点（异常值）序列的敏感度也会降低。
```

- **在线学习的定义：** 通过以单独的数据实例或小批量的数据组的方式，循序渐进地向系统提供数据来进行增量训练，因此也称之为**增量学习**


- **在线学习的特点：** 
	- **学习效率高**：每个学习步骤都快速且成本低廉，因此系统可以近乎实时地学习新数据（见图1-14）。
	- **易受不良数据的影响：**
	    - 如果将不良数据输入在线学习系统，系统的性能可能会迅速下降（==取决于数据质量和学习率==）。
	    - 对于实时系统，这种性能下降会被用户直接感知。
	    - 不良数据的来源可能包括机器人传感器故障或恶意刷屏等。

![image.png](https://i0.hdslb.com/bfs/article/167bb9301a4aca82e9c2bcdc5115b1b8394687087.png)

- **在线学习应用 ：快速变化的数据：** 在线学习对于需要快速适应数据变化的系统非常有用，例如检测股票市场的新模式。

- **在线学习应用 ： 资源有限的环境**： 如果计算资源有限（例如在移动设备上训练模型），在线学习是一个合适的选择。

- **在线学习应用 ：超大数据集 (核外学习)：**
    - 对于无法一次性加载到计算机主内存中的超大数据集，在线学习算法同样适用（称为核外学习）。
    - 算法加载部分数据，在该部分数据上执行一个训练步骤，然后重复此过程，直到处理完所有数据（见图1-15）。

![image.png](https://i0.hdslb.com/bfs/article/0e4cbc43f9d7a18514bb16be4706434f394687087.png)

- **关键参数 - 学习率：**
    - 学习率是在线学习系统中一个重要的参数，决定了系统适应不断变化的数据的速度。
    - **高学习率：** 系统会快速适应新数据，但也可能很快忘记旧数据（例如，垃圾邮件过滤器可能只标记最新类型的垃圾邮件）。
    - **低学习率：** 系统学习速度较慢，但对旧数据的遗忘也较慢，具有更高的惰性。
- **风险缓解措施：**
    - 需要密切监控系统性能，并在检测到性能下降时立即停止学习（并尝试恢复到之前的良好状态）。
    - 可能还需要监控输入数据，并对异常数据做出反应，例如使用异常检测算法（见第9章）。

- **基于在线学习的算法**
	1. **随机梯度下降 (SGD):**
	    
	    - 虽然梯度下降本身是一种优化算法，但随机梯度下降 (SGD) 及其变体（如小批量梯度下降）常用于在线学习。
	    - SGD 每次只使用一个训练样本来更新模型参数，使其能够快速适应新数据。
	    - 适用于线性回归、逻辑回归、神经网络等多种模型。
	2. **Passive-Aggressive 算法:**
	    
	    - Passive-Aggressive (PA) 算法是一类简单有效的在线学习算法，尤其适用于分类问题。
	    - PA 算法对每个新样本进行预测，如果预测正确（即“passive”），则不更新模型；如果预测错误（即“aggressive”），则以最小的幅度调整模型参数，以纠正错误。
	    - PA 算法有多个变体，如 PA-I、PA-II 等，它们在更新规则上略有不同。
	3. **Perceptron:**
	    
	    - 感知机是一种简单的线性分类算法，可以用于在线学习。
	    - 感知机对每个新样本进行预测，如果预测错误，则根据样本的特征向量更新模型参数。
	    - 感知机算法简单易懂，但只能处理线性可分的数据。
	4. **Hinge Loss 的线性模型:**
	    
	    - 可以使用 Hinge Loss 作为损失函数的线性模型进行在线学习。
	    - Hinge Loss 常用于支持向量机 (SVM)，但也可以与其他优化算法（如 SGD）结合使用，实现在线学习。
	5. **Online k-Means:**
	    
	    - k-Means 是一种常用的聚类算法，但也可以通过在线方式进行更新。
	    - Online k-Means 算法每次只使用一个样本来更新聚类中心，使其能够适应数据分布的变化。

```ad-abstract
title:**其他说明**
- 许多机器学习算法都可以通过在线方式进行修改和更新。
- 在线学习的关键在于如何有效地利用每个新样本来更新模型参数，同时避免过拟合和灾难性遗忘。
- 学习率是在线学习中一个重要的参数，需要仔细调整，以平衡学习速度和稳定性。
```

#### （3）实例、模型学习
>**引言**：对机器学习系统进行分类的另一种方法是根据它们的泛化方式。大多数机器学习任务都与做出预测有关。==这意味着在给定大量训练样例的情况下，系统需要能够对它以前未见到过的样例做出良好的预测（泛化）==。在训练数据上有很好的性能是好的，但还不够，真正的目标是在新实例上表现良好。

**泛化**方法主要有两种：基于实例的学习和基于模型的学习。

##### 1）实例学习
```ad-note
title:**相关概念**
```

- **实例学习的定义：** 也称为基于记忆的学习（memory-based learning）或懒惰学习（lazy learning）

- **实例学习的特点**：
	1. **存储训练数据：** 算法显式地存储训练数据集中的所有实例（或其子集）。
	2. **预测时进行比较：** 当需要对新的、未见过的实例进行预测时，算法会将该实例与存储的训练实例进行比较，通过某种相似性度量（例如，欧氏距离、余弦相似度等）找到最相似的实例。
	3. **基于相似实例进行决策：** 新实例的预测结果是基于与其最相似的训练实例的标签或值的。例如，在分类问题中，新实例可能被分配到与其最相似的训练实例所属的类别；在回归问题中，新实例的值可能是与其最相似的训练实例的值的平均或加权平均。
	4. **没有显式的训练阶段：** 与其他机器学习算法不同，基于实例的学习算法在训练阶段只是简单地存储数据，真正的“学习”发生在预测阶段。
	5. **局部近似：** 基于实例的学习可以被认为是对目标函数的局部近似，因为算法只关注新实例周围的训练数据。


- **实例学习应用：垃圾邮件分类器**
	- **最简单的学习方式 - 背诵：** 最常见的学习方式可能是简单地背诵。
	- **背诵式垃圾邮件过滤器的局限性：** 如果使用背诵的方式创建垃圾邮件过滤器，它只会标记与用户已标记的电子邮件完全相同的邮件，这种方法虽然可行但不是最优的。
	- **更智能的垃圾邮件过滤：** 更好的垃圾邮件过滤器不仅能标记相同的邮件，还能标记与已知垃圾邮件非常相似的邮件。
	- **相似性度量的必要性：** 这需要一种衡量两封电子邮件之间相似性的方法。
	- **基本的相似性度量示例：** 一个非常基本的相似性度量可以是计算两封邮件共有的单词数。
	- **基于相似性的分类：** 如果一封电子邮件与已知的垃圾邮件有很多相同的单词，那么系统会将其标记为垃圾邮件。
	- **泛化过程示例：** 如图1-16所示，一个新的实例被归类为三角形，因为在学习到的样例中，大多数与该新实例最相似的实例都属于三角形这个类别。
	
	![image.png](https://i0.hdslb.com/bfs/article/d3725c183207211ce8a7eeab54f84eb0394687087.png)


- **实例学习的核心机制：** 系统用心学习训练样例，然后通过使用相似性度量将新实例与学习到的样例（或它们的子集）进行比较，从而进行泛化。

- **基于实例的学习算法：**
	1. **K近邻算法 (K-Nearest Neighbors, KNN):**
	    
	    - **原理：** KNN 算法是最经典的基于实例的学习算法之一。对于一个新的输入实例，KNN 算法会在训练集中找到与其最相似的 K 个实例（即 K 个“近邻”），然后根据这 K 个近邻的标签或值来预测新实例的输出。
	    - **相似性度量：** 常用的相似性度量包括欧氏距离、曼哈顿距离、余弦相似度等。
	    - **预测方式：** 在分类问题中，KNN 算法通常采用“多数表决”的方式，即将新实例分配到 K 个近邻中最常见的类别；在回归问题中，KNN 算法通常计算 K 个近邻的平均值或加权平均值，作为新实例的预测值。
	    - **K 值的选择：** K 值的选择对 KNN 算法的性能有很大影响。较小的 K 值可能导致过拟合，而较大的 K 值可能导致欠拟合。通常需要通过交叉验证等方法来选择合适的 K 值。
	2. **局部加权学习 (Locally Weighted Learning, LWL):**
	    
	    - **原理：** LWL 算法与 KNN 算法类似，也是一种基于近邻的算法。不同之处在于，LWL 算法会对每个近邻赋予不同的权重，距离新实例越近的近邻，权重越大。
	    - **权重函数：** LWL 算法使用一个权重函数来计算每个近邻的权重。常用的权重函数包括高斯核函数、Epanechnikov 核函数等。
	    - **预测方式：** LWL 算法使用加权平均或加权多数表决的方式来预测新实例的输出。
	    - **优点：** LWL 算法能够更好地适应数据的局部结构，通常比 KNN 算法具有更高的精度。
	3. **基于案例的推理 (Case-Based Reasoning, CBR):**
	    
	    - **原理：** CBR 是一种更高级的基于实例的学习方法，常用于解决复杂的问题，如医疗诊断、法律推理等。
	    - **案例库：** CBR 系统维护一个案例库，其中每个案例包含一个问题描述、解决方案和结果。
	    - **推理过程：** 当 CBR 系统遇到一个新的问题时，它会首先在案例库中找到与该问题最相似的案例，然后根据相似案例的解决方案来推导出新问题的解决方案。
	    - **知识表示：** CBR 系统通常需要使用复杂的知识表示方法来描述问题和解决方案，如语义网络、本体等。
	    - **案例适应：** 在某些情况下，CBR 系统可能需要对相似案例的解决方案进行调整，以适应新问题的具体情况。

```ad-abstract
title:**其他说明**
- 基于实例的学习算法的性能高度依赖于相似性度量的选择。
- 基于实例的学习算法通常需要大量的存储空间来存储训练数据。
- 基于实例的学习算法的预测速度可能较慢，因为需要将新实例与所有（或大量）训练实例进行比较。
- 基于实例的学习算法对数据的预处理（如特征缩放、缺失值处理等）比较敏感。
```

##### 2）模型学习
```ad-note
title:**相关概念**
- **“模型”一词的多重含义：** “模型”可以指代模型类型（如线性回归）、特定的模型架构（如单输入单输出的线性回归），或已训练好的模型（带有特定参数值的线性回归）。
- **模型选择与模型训练：** 模型选择包括选择模型类型和完全指定其架构。训练模型是运行算法找到最拟合训练数据的模型参数，并期望模型能很好地泛化到新数据。
```

- **基于模型的学习的定义：** 对一组样例进行泛化的另一种方法是为这些样例构建一个模型，然后使用该模型进行预测（见图1-17）。

![image.png](https://i0.hdslb.com/bfs/article/18fa1233bf0af88356910918ef4114cb394687087.png)

- **模型学习应用：金钱与幸福感的关系研究**
    - **数据获取：** 从经合组织（OECD）网站下载生活幸福指数数据，从世界银行找到人均GDP统计数据。
    - **数据整合：** 将数据并入表格并按人均GDP排序（见表1-1的摘录）。
	![image.png|451](https://i0.hdslb.com/bfs/article/cd713376d4c847744dc1242d28dd2962394687087.png)
    - **数据可视化：** 绘制这些国家的数据（见图1-18）。
	![image.png](https://i0.hdslb.com/bfs/article/8aa1e5e2394504e349ed1f16527041ed394687087.png)
    - **趋势观察：** 观察到生活满意度随着国家人均GDP的增长或多或少呈线性增长的趋势。
    - **模型选择：** 决定将生活满意度建模为人均GDP的线性函数（见公式1-1）。
	    - **公式1-1：一个简单的线性模型**
	        - 生活满意度 = θ₀ + θ₁ × 人均GDP
	    - **模型参数：** 该模型有两个模型参数：θ₀ 和 θ₁。
	    - **参数调整：** 通过调整这些参数，可以用该模型表示任何线性函数（如图1-19所示）。
			![image.png](https://i0.hdslb.com/bfs/article/070cda2808d4b8fee873e18de6c86129394687087.png)
	    - **参数表示：** 希腊字母 θ（theta）通常用于表示模型参数。
	    - **性能指标：** 在使用模型之前，需要定义参数值 θ₀ 和 θ₁。为了找到最佳参数值，需要指定一个性能指标，例如效用函数（衡量模型有多好）或代价函数（衡量模型有多差）。
	    - **代价函数：** 对于线性回归问题，通常使用代价函数来衡量线性模型的预测与训练样例之间的差距，目标是最小化这个差距。
		- **模型训练：** 线性回归算法通过提供的训练样例，找到使线性模型最拟合数据的参数。这称为训练模型。
		- **训练结果：** 在示例中，算法发现最佳参数值为 θ₀ = 3.75 和 θ₁ = 6.78 × 10⁻⁵。
	    ![image.png](https://i0.hdslb.com/bfs/article/ab3471accc792c24b11da453ed86f86d394687087.png)
	    - **模型预测 (推断)：** 现在可以运行模型进行预测。例如，预测塞浦路斯的生活满意度，先获取其人均GDP（37655美元），然后应用模型得到预测值约为 6.30。
	    - **代码示例：** 示例1-1展示了使用Scikit-Learn加载数据、分离特征和标签、创建散点图、训练线性模型并进行预测的Python代码。
		![image.png](https://i0.hdslb.com/bfs/article/16b479bfd9a8c572e6c2f3d9d37c41ff394687087.png)
		
		 - **模型对比：** 如果使用基于实例的学习（k近邻回归），以以色列（人均GDP最接近塞浦路斯）的生活满意度（7.2）作为预测值，或取立陶宛、斯洛文尼亚和以色列生活满意度的平均值（6.33）作为预测值，结果与基于模型的预测接近。
	    - **模型改进：** 如果模型预测不佳，可能需要使用更多属性、获取更多或更高质量的数据，或者选择更强大的模型（如多项式回归模型）。

```ad-abstract
title:**总结 - 典型的机器学习项目工作流程：**
- 研究数据。
- 选择一个模型。
- 使用训练数据进行训练（学习算法搜索最小化代价函数的模型参数值）。
- 最后，应用该模型对新实例进行预测（推断），并期望模型具有良好的泛化
```

- **基于模型的学习算法**
	**监督学习 (Supervised Learning):**
	
	1. **线性回归 (Linear Regression):**
	    
	    - **模型：** 假设输入特征和输出之间存在线性关系，即 ，其中 y 是预测值，x 是输入特征，θ 是模型参数。
	    - **训练目标：** 找到使预测值与实际值之间的误差最小的参数 θ。
	    - **适用场景：** 适用于回归问题，例如房价预测、销售额预测等。
	2. **多项式回归 (Polynomial Regression):**
	    
	    - **模型：** 扩展了线性回归，允许输入特征的非线性组合，例如 。
	    - **训练目标：** 找到使预测值与实际值之间的误差最小的参数 θ。
	    - **适用场景：** 适用于数据之间存在非线性关系的情况。
	3. **逻辑回归 (Logistic Regression):**
	    
	    - **模型：** 使用 sigmoid 函数将线性模型的输出转换为概率值，即 ，其中 z 是线性模型的输出，p 是属于某个类别的概率。
	    - **训练目标：** 找到使预测概率与实际类别之间的误差最小的参数。
	    - **适用场景：** 适用于二分类问题，例如垃圾邮件过滤、疾病诊断等。
	4. **支持向量机 (Support Vector Machine, SVM):**
	    
	    - **模型：** 通过找到一个最优的超平面来分隔不同类别的数据，或者通过核函数将数据映射到高维空间，使其线性可分。
	    - **训练目标：** 找到使分类间隔最大化的超平面参数。
	    - **适用场景：** 适用于高维数据、非线性可分数据等复杂问题。
	5. **决策树 (Decision Tree):**
	    
	    - **模型：** 基于树结构的分类和回归算法，通过一系列的 if-then-else 规则来进行决策。
	    - **训练目标：** 构建一棵能够最好地划分数据的树结构。
	    - **适用场景：** 适用于数据量较小、特征维度较低的问题。
	6. **随机森林 (Random Forest):**
	    
	    - **模型：** 集成学习算法，通过构建多个决策树并进行投票来进行预测。
	    - **训练目标：** 训练多个具有差异性的决策树。
	    - **适用场景：** 适用于各种分类和回归问题，具有较高的精度和鲁棒性。
	7. **神经网络 (Neural Network):**
	    
	    - **模型：** 由多个神经元相互连接而成的复杂网络结构，可以学习复杂的非线性关系。
	    - **训练目标：** 找到使预测输出与实际输出之间的误差最小的连接权重。
	    - **适用场景：** 适用于各种分类、回归和模式识别问题，尤其在图像识别、自然语言处理等领域表现出色。
	
	**无监督学习 (Unsupervised Learning):**
	
	8. **K-均值聚类 (K-Means Clustering):**
	    
	    - **模型：** 将数据划分为 K 个簇，每个簇由其中心点表示。
	    - **训练目标：** 找到使每个数据点与其所属簇中心点之间的距离最小的簇划分方案。
	    - **适用场景：** 适用于客户细分、图像分割等聚类问题。
	9. **高斯混合模型 (Gaussian Mixture Model, GMM):**
	    
	    - **模型：** 假设数据由多个高斯分布混合而成，每个高斯分布代表一个簇。
	    - **训练目标：** 找到每个高斯分布的参数（均值、方差、权重），使数据在该模型下的似然函数最大化。
	    - **适用场景：** 适用于数据分布复杂、簇形状不规则的聚类问题。

```ad-abstract
title:**其他说明**
- 基于模型的学习算法通过在训练数据中发现模式并构建预测模型来实现。
- 模型学习算法需要大量的训练数据来学习模型参数。
- 模型学习算法的预测速度通常较快，因为只需要运行模型即可。
```



### 1.1.5. 机器学习的主要挑战
>**引言**：由于你的主要任务是选择一个模型并在一些数据上对其进行训练，因此可能出现问题的不外乎是“不良模型”和“不良数据”。让我们先从不良数据开始。

#### （1）不良数据
##### 1) 训练数据量不足



##### 2) 训练数据不具代表性
##### 3) 低质量数据
##### 4) 无关特征
##### 5) 过拟合训练数据
##### 6) 欠拟合训练数据

#### （2）不良模型


### 1.1.6. 测试和验证


#### (1) 超参数调整和模型选择
#### (2) 数据不匹配


### 1.1.7. 练习题

1. 如何定义机器学习？
2. 机器学习在哪些问题上表现突出，你能说出四类应用吗？
3. 什么是被标记的训练集？
4. 最常见的两种监督学习任务是什么？
5. 你能说出四种常见的无监督学习任务吗？
6. 你会使用什么类型的算法让机器人在各种未知地形中行走？
7. 你会使用什么类型的算法将客户分成多个组？
8. 你会把垃圾邮件检测问题定义为监督学习问题还是无监督学习问
题？
9. 什么是在线学习系统？
10. 什么是核外学习？
11. 什么类型的算法依赖于相似性度量来进行预测？
12. 模型参数和模型超参数有什么区别？
13. 基于模型的算法搜索什么？它们最常用的成功策略是什么？它
们如何做出预测？
14. 你能说出机器学习中的四个主要挑战吗？
15. 如果你的模型在训练数据上表现很好，但对新实例的泛化能力
很差，这是怎么回事？你能说出三种可能的解决方案吗？
16. 什么是测试集？为什么要使用它？
17. 验证集的目的是什么？
18. 什么是train-dev集？什么时候需要它？如何使用？
19. 如果使用测试集来调整超参数会出现什么问题？
这些练习题的答案可在本章notebook的末尾找到，网址为
https://homl.info/colab3。


## 1.2. 第2章 端到端机器学习项目

### 1.2.1. 使用真实数据

### 1.2.2. 放眼大局

#### (1) 框定问题
#### (2) 选择性能指标
#### (3) 检查假设


### 1.2.3. 获取数据

```
#### (1) 使用Google Colab运行代码示例
#### (2) 保存你的代码更改和数据
#### (3) 交互性的力量和危险
#### (4) 本书代码与notebook代码
#### (5) 下载数据
#### (6) 快速浏览数据结构
#### (7) 创建测试集
```

### 1.2.4. 探索和可视化数据以获得见解


#### (1) 可视化地理数据
#### (2) 寻找相关性
#### (3) 实验不同属性组合

### 1.2.5. 为机器学习算法准备数据


#### (1) 清洗数据
#### (2) 处理文本和类别属性
#### (3) 特征缩放和转换
#### (4) 定制转换器
#### (5) 转换流水线


### 1.2.6. 选择和训练模型

```
#### (1) 在训练集上训练和评估
#### (2) 使用交叉验证进行更好的评估
```

### 1.2.7. 微调模型


#### (1) 网格搜索
#### (2) 随机搜索
#### (3) 集成方法
#### (4) 分析最佳模型及其错误
#### (5) 在测试集上评估系统


### 1.2.8. 启动、监控和维护系统

### 1.2.9. 试试看

### 1.2.10. 练习题

## 1.3. 第3章 分类

### 1.3.1. MNIST

### 1.3.2. 训练二元分类器

### 1.3.3. 性能测量


#### (1) 使用交叉验证测量精度
#### (2) 混淆矩阵
#### (3) 准确率和召回率
#### (4) 准确率/召回率权衡
#### (5) ROC曲线



### 1.3.4. 多类分类

### 1.3.5. 错误分析

### 1.3.6. 多标签分类

### 1.3.7. 多输出分类

### 1.3.8. 练习题

## 1.4. 第4章 训练模型

### 1.4.1. 线性回归



#### (1) 标准方程
#### (2) 计算复杂度


### 1.4.2. 梯度下降



#### (1) 批量梯度下降
#### (2) 随机梯度下降
#### (3) 小批量梯度下降


### 1.4.3. 多项式回归

### 1.4.4. 学习曲线

### 1.4.5. 正则化线性模型



#### (1) 岭回归
#### (2) Lasso回归
#### (3) 弹性网络回归
#### (4) 早停


### 1.4.6. 逻辑回归

```
#### (1) 估计概率
#### (2) 训练和代价函数
#### (3) 决策边界
#### (4) softmax回归
```

### 1.4.7. 练习题

## 1.5. 第5章 支持向量机

### 1.5.1. 线性SVM分类

### 1.5.2. 非线性SVM分类

```
#### (1) 多项式核
#### (2) 相似性特征
#### (3) 高斯RBF核
#### (4) SVM类和计算复杂度
```

### 1.5.3. SVM回归

### 1.5.4. 线性SVM分类器的工作原理

### 1.5.5. 对偶问题

### 1.5.6. 练习题

## 1.6. 第6章 决策树

### 1.6.1. 训练和可视化决策树

### 1.6.2. 做出预测

### 1.6.3. 估计类概率

### 1.6.4. CART训练算法

### 1.6.5. 计算复杂度

### 1.6.6. 基尼杂质或熵

### 1.6.7. 正则化超参数

### 1.6.8. 回归

### 1.6.9. 对轴方向的敏感性

### 1.6.10. 决策树具有高方差

### 1.6.11. 练习题

## 1.7. 第7章 集成学习和随机森林

### 1.7.1. 投票分类器

### 1.7.2. bagging和pasting

```
#### (1) Scikit-Learn中的bagging和pasting
#### (2) 包外评估
#### (3) 随机补丁和随机子空间
```

### 1.7.3. 随机森林

```
#### (1) 极端随机树
#### (2) 特征重要性
```

### 1.7.4. 提升法

```
#### (1) AdaBoost
#### (2) 梯度提升
#### (3) 基于直方图的梯度提升
```

### 1.7.5. 堆叠法

### 1.7.6. 练习题

## 1.8. 第8章 降维

### 1.8.1. 维度的诅咒

### 1.8.2. 降维的主要方法

```
#### (1) 投影
#### (2) 流形学习
```

### 1.8.3. PCA

```
#### (1) 保留方差
#### (2) 主成分
#### (3) 向下投影到d维
#### (4) 使用Scikit-Learn
#### (5) 可解释方差比
#### (6) 选择正确的维度
#### (7) PCA压缩
#### (8) 随机PCA
#### (9) 增量PCA
```

### 1.8.4. 随机投影

### 1.8.5. LLE

### 1.8.6. 其他降维技术

### 1.8.7. 练习题

## 1.9. 第9章 无监督学习技术

### 1.9.1. 聚类算法
>**引入**：你在山中徒步旅行时，偶然发现了从未见过的植物。你环顾四周，你会发现还有很多。它们并不完全相同，但是它们足够相似，你可能会知道它们有可能属于同一物种（或至少属于同一属）。你可能需要植物学家告诉你什么是物种，但你当然不需要专家来识别外观相似的物体组。这称为聚类：识别相似实例并将其分配给相似实例的集群或组。

>**理解**：即便你不知道这是什么，你也可以通过外观特征判断他们是否是一类，这就是无监督中聚类的核心思想——通过前浅显的可供计算的某个特征来将某个事物中包含的对象进行归类。

#### (1) k均值

它的目标是将一组数据点划分为 K 个簇，使得同一簇内的数据点尽可能相似，而不同簇之间的数据点尽可能不同。

![图片](https://mmbiz.qpic.cn/sz_mmbiz_gif/EyvIAGiaMq6ew8V3K1UCVolhrsh9RyK9p5xeBb1stIG7UfvvZ16K1YplU8ew2WpZ9SwUchXUqfGRsojSt7ILSng/640?wx_fmt=gif&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1)

- # 1.基本原理

K-Means 算法通过迭代优化簇内平方误差和（WCSS）来将数据划分为 K 个簇。它的目标是最小化每个样本点到其所属簇中心的距离平方和。

其基本流程如下：

1. 初始化：选择一个预设的簇数 K，并随机选择 K个数据点作为初始的簇中心。
    
2. 分配步骤：将==每个数据点分配给距离其最近的簇中心==，通常使用欧氏距离来衡量点与中心之间的距离。
    
3. 更新步骤：计算每个簇中所有数据点的均值（即簇的质心），并将这个均值作为新的簇中心。
    
4. 重复步骤：重复分配和更新步骤，直到簇中心不再发生变化，或者变化非常小，表示算法收敛。
    

- # 2. 距离度量

![image.png](https://i0.hdslb.com/bfs/openplatform/e792db7ef5a3608054207ad47082f54154f2a146.png)

- $d_{(x_i,c_i)}$ 表示数据点 $x_i$ 到 $c_i$ 的欧式距离（也就是点间距） 

- # 3. 目标函数

![image.png](https://i0.hdslb.com/bfs/openplatform/ce98a61445779771e0293cab5057c29ac1c0a0b4.png)


- # 4. 数学公式

![image.png](https://i0.hdslb.com/bfs/openplatform/9fe8256a0a1424ba2c0009c20756befe5e903886.png)


- # 5.算法优缺点

优点

1. 计算简单且易于实现，适用于大规模数据集。
    
2. 在许多实际问题中表现良好，特别是数据集呈现明显的球状簇时。
    

 缺点

1. 需要预先指定簇的数量 ，如果  选择不当，可能导致聚类效果不好。
    
2. 对初始簇中心的选择非常敏感，可能导致局部最优解。为了减少这种影响，常常使用 K-Means++ 来初始化簇中心。
    
3. 对异常值和噪声较为敏感。
    

- # 6. 算法的改进

为了克服 K-means 算法的缺点，提出了一些改进方法，比如：

1. **K-means++聚类:**（改进初始质心生成）
	
	- 数学原理:K-means++ 是对经典K-means算法的一个改进，主要创新在于初始质心的选择方法。在传统的K-means中，**初始质心通常是随机选择的，这可能导致不理想的聚类结果**。K-means++通过一种更精细的策略选择初始质心，以增加产生更优聚类结果的可能性。具体来说，==它首先随机选择一个数据点作为第一个质心，然后根据数据点到已选择质心的距离的概率分布来选择接下来的质心==。
    
	- 创新点:通过优化初始质心的选择过程，K-means++能够提高聚类的质量和算法的稳定性。它减少了陷入局部最优的可能性，**并且通常能够加快算法的收敛速度**。
    
    ![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/qkEiaC9We0AUNkiagVXxnE8SZB8XR8XgAyLD5RDhZmRJ8ksShEmoxpcGUf83pPzI42ic6Q3SAr3wnTDDHfUZ2AkRA/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1)
    
2. Mini-Batch K-means
    
    使用小批量数据来更新簇中心，提高算法的计算效率，适用于大规模数据集。



**轮廓系数优化K-means聚类:**

- 数学原理:轮廓系数是评估聚类效果的一种度量方法，它结合了聚类的紧密性和分离性。对于每个数据点，计算其与所在簇内其他点的平均距离（紧密性）和与最近的非所在簇的所有点的平均距离（分离性）。轮廓系数是分离性和紧密性之差与它们之间的最大值的比率。在优化K-means聚类时，可以使用轮廓系数作为一个指标来确定最佳的簇数量或评估不同的聚类配置。
    
- 创新点:使用轮廓系数优化K-means聚类使得簇的选择不仅依赖于算法的内在迭代过程，还结合了一个明确的聚类效果评估指标。提高了聚类结果的有效性，特别是在簇数量不是预先确定的情况下。
    
    ![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/qkEiaC9We0AUNkiagVXxnE8SZB8XR8XgAy1QXhrKficibh3TQJqHXPvZRPkOWRkMpwYic3YTGjqqu3iacKfZJmedsbIA/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1)


- # 7. 案例分享

下面是一个使用 K-means 算法对数据进行聚类的完整示例代码。

该代码包括了如何使用肘部法来选择最佳的聚类数 K 值，并最终应用 K-means 算法进行聚类。

```python
import numpy as np  
import matplotlib.pyplot as plt  
from sklearn.cluster import KMeans  
from sklearn.datasets import make_blobs  
  
# 生成示例数据集  
X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)  
  
# 使用肘部法来选择最佳的k值  
wcss = []  # 存储每个k值的WCSS（簇内误差平方和）  
for k in range(1, 11):  
    kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=300, n_init=10, random_state=0)  
    kmeans.fit(X)  
    wcss.append(kmeans.inertia_)  # inertia_是WCSS的值  
  
# 绘制肘部法的图形  
plt.figure(figsize=(8,6))  
plt.plot(range(1, 11), wcss, marker='o')  
plt.title('肘部法选择最佳K值')  
plt.xlabel('簇的数量 (K)')  
plt.ylabel('WCSS (簇内误差平方和)')  
plt.show()
```

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/EyvIAGiaMq6ew8V3K1UCVolhrsh9RyK9pAgHCKEM72ZudZiazxlDptyz2GGoz7GLv3MmA3gsquupZsWvSP1zy8xQ/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1)

```python
# 根据肘部法的图形，选择一个合适的K值。  
best_k = 4  
kmeans = KMeans(n_clusters=best_k, init='k-means++', max_iter=300, n_init=10, random_state=0)  
y_kmeans = kmeans.fit_predict(X)  
  
# 可视化聚类结果  
plt.figure(figsize=(8,6))  
plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')  
  
# 绘制簇中心  
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', s=200, alpha=0.75, marker='X')  
plt.title(f'K-means 聚类结果 (K={best_k})')  
plt.xlabel('特征1')  
plt.ylabel('特征2')  
plt.show()
```

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/EyvIAGiaMq6ew8V3K1UCVolhrsh9RyK9puncUI4iacsN9oT1V25pvFZHwUuwe3TR92Sfiaqy3jjMbTicjGBdcuzMGQ/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1)




- ## 7.2. 使用聚类进行图像分割

- ## 7.3. 使用聚类进行半监督学习

#### (2) DBSCAN
#### (3) 其他聚类算法

**模糊C均值聚类（FCM, Fuzzy C-Means）:**

- 数学原理: FCM 是一种模糊聚类算法，允许单个数据点以不同的隶属度属于多个簇。它通过迭代优化隶属度和簇中心来减少目标函数，该函数是簇中心和数据点之间距离的加权和。
    
- 创新点: 引入模糊概念，与 K-Means 的硬划分不同，FCM 通过隶属度提供了更多灵活性，适用于数据结构不清晰或重叠的情况。
    
    ![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/qkEiaC9We0AUNkiagVXxnE8SZB8XR8XgAyNbmQvGo5SvicXvM5oTv0k0Xticlt2weAia0FQ0vDUMXJqEYicewibCrBzQA/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1)

**ISODATA 聚类（迭代自组织数据分析技术）:**

- 数学原理: 类似于 K-Means，但更加灵活。ISODATA 考虑簇的合并和分裂，允许簇数量在迭代过程中动态变化。
    
- 创新点: 能够自适应调整簇的数量，适用于对簇数量不确定的情况。
    

**![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/qkEiaC9We0AUNkiagVXxnE8SZB8XR8XgAyI3ZOWwHalibBBq3cshq2oQupqtLvtAC3tvtu14micSfVXtjyiaNWg5hDQ/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1)**

**Spectral谱聚类:**

- 数学原理: 基于图理论，将数据点看作图中的节点。算法首先构建一个相似度图，然后基于图的拉普拉斯矩阵的特征向量进行聚类。
    
- 创新点: 能够捕捉数据的复杂结构，尤其是非球形的簇。谱聚类对于特定类型的数据非常有效。
    

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/qkEiaC9We0AUNkiagVXxnE8SZB8XR8XgAyYIGSMIbP5qzXicFrUD7M9uGkxIUWCUL0shH3DDlqfvmRYibsRoHOicKzA/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1)

**高斯混合模型（GMM）聚类:**

- 数学原理: GMM 是一种基于概率的聚类方法，它假设数据是从若干个高斯分布的混合生成的。每个簇由一个高斯分布表示，参数通过最大化似然估计或期望最大化（EM）算法进行优化。
    
- 创新点: 提供了软聚类方法，每个数据点属于每个簇的概率可以计算出来。适用于估计簇的形状和大小。
    

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/qkEiaC9We0AUNkiagVXxnE8SZB8XR8XgAyibAyjicAOgKMLTxlyAgqxFRnM8NawERZS4E20HicEt80g1qMzEvibMOyaw/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1)

**AHC层次聚类（Agglomerative Hierarchical Clustering）:**

- 数学原理:AHC 是一种自底向上的层次聚类方法。它开始于将数据集中的每个点视为一个单独的簇，然后迭代地合并最相似的簇。这种相似性通常是通过一种距离度量（如欧氏距离）来定义的。合并过程持续进行，直到所有点都归并到一个单一的簇中，或者达到某个停止标准（如预定的簇数量）。
    
- 创新点:AHC 提供了一种直观的方式来理解和可视化数据的层次结构。通过树状图（dendrogram）的形式，它揭示了数据的嵌套结构，使得解释和理解聚类结果更加直观。
    

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/qkEiaC9We0AUNkiagVXxnE8SZB8XR8XgAyhRoibicvQBgApbHwKYibT4M1z7xWSibr3srZJR5QbiaVujnE4VicEGfEgGrA/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1)

### 1.9.2. 高斯混合模型


#### (1) 使用高斯混合进行异常检测
#### (2) 选择集群数
#### (3) 贝叶斯高斯混合模型
#### (4) 其他用于异常和新颖性检测的算法

### 1.9.3. 练习题

# 2. 第二部分 神经网络与深度学习

## 2.1. 第10章 Keras人工神经网络简介

### 2.1.1. 从生物神经元到人工神经元

```
#### (1) 生物神经元
#### (2) 神经元的逻辑计算
#### (3) 感知机
#### (4) 多层感知机和反向传播
#### (5) 回归MLP
#### (6) 分类MLP
```

### 2.1.2. 使用Keras实现MLP

```
#### (1) 使用顺序API构建图像分类器
#### (2) 使用顺序API构建回归MLP
#### (3) 使用函数式API构建复杂模型
#### (4) 使用子类化API构建动态模型
#### (5) 保存和还原模型
#### (6) 使用回调函数
#### (7) 使用TensorBoard进行可视化
```

### 2.1.3. 微调神经网络超参数

```
#### (1) 隐藏层数量
#### (2) 每个隐藏层的神经元数量
#### (3) 学习率、批量大小和其他超参数
```

### 2.1.4. 练习题

## 2.2. 第11章 训练深度神经网络

### 2.2.1. 梯度消失和梯度爆炸问题

```
#### (1) Glorot初始化和He初始化
#### (2) 更好的激活函数
#### (3) 批量归一化
#### (4) 梯度裁剪
```

### 2.2.2. 重用预训练层

```
#### (1) 用Keras进行迁移学习
#### (2) 无监督预训练
#### (3) 基于辅助任务的预训练
```

### 2.2.3. 更快的优化器

```
#### (1) 动量优化
#### (2) Nesterov加速梯度
#### (3) AdaGrad
#### (4) RMSProp
#### (5) Adam
#### (6) AdaMax
#### (7) Nadam
#### (8) AdamW
```

### 2.2.4. 学习率调度

### 2.2.5. 通过正则化避免过拟合

```
#### (1) ℓ1正则化和ℓ2正则化
#### (2) dropout
#### (3) 蒙特卡罗（MC）dropout
#### (4) 最大范数正则化
```

### 2.2.6. 总结和实用指南

### 2.2.7. 练习题

## 2.3. 第12章 使用TensorFlow自定义模型和训练

### 2.3.1. TensorFlow快速浏览

### 2.3.2. 像使用NumPy一样使用TensorFlow

```
#### (1) 张量和操作
#### (2) 张量和NumPy
#### (3) 类型转换
#### (4) 变量
#### (5) 其他数据结构
```

### 2.3.3. 自定义模型和训练算法

```
#### (1) 自定义损失函数
#### (2) 保存和加载包含自定义组件的模型
#### (3) 自定义激活函数、初始化、正则化和约束
#### (4) 自定义指标
#### (5) 自定义层
#### (6) 自定义模型
#### (7) 基于模型内部数据的损失和指标
#### (8) 使用自动微分计算梯度
#### (9) 自定义训练循环
```

### 2.3.4. TensorFlow函数和图

```
#### (1) AutoGraph和跟踪
#### (2) TF函数规则
```

### 2.3.5. 练习题

## 2.4. 第13章 使用TensorFlow加载和预处理数据

### 2.4.1. tf.data API

```
#### (1) 链式转换
#### (2) 乱序数据
#### (3) 来自多个文件的行交织
#### (4) 预处理数据
#### (5) 合并在一起
#### (6) 预取
#### (7) 在Keras中使用数据集
```

### 2.4.2. TFRecord格式

```
#### (1) 压缩的TFRecord文件
#### (2) 协议缓冲区
#### (3) TensorFlow protobuf
#### (4) 加载和解析Example
#### (5) 使用SequenceExample protobuf处理列表的列表
```

### 2.4.3. Keras预处理层

```
#### (1) 归一化层
#### (2) 离散化层
#### (3) 类别编码层
#### (4) StringLookup层
#### (5) 哈希层
#### (6) 使用嵌入编码分类特征
#### (7) 文本预处理
#### (8) 使用预训练语言模型组件
#### (9) 图像预处理层
```

### 2.4.4. TensorFlow数据集项目

### 2.4.5. 练习题

## 2.5. 第14章 使用卷积神经网络进行深度计算机视觉

### 2.5.1. 视觉皮层的结构

### 2.5.2. 卷积层

```
#### (1) 滤波器
#### (2) 堆叠多个特征图
#### (3) 使用Keras实现卷积层
#### (4) 内存需求
```

### 2.5.3. 池化层

### 2.5.4. 使用Keras实现池化层

### 2.5.5. CNN架构

```
#### (1) LeNet-5
#### (2) AlexNet
#### (3) GoogLeNet
#### (4) VGGNet
#### (5) ResNet
#### (6) Xception
#### (7) SENet
#### (8) 其他值得注意的架构
#### (9) 选择正确的CNN架构
```

### 2.5.6. 使用Keras实现ResNet-34 CNN

### 2.5.7. 使用Keras的预训练模型

### 2.5.8. 使用预训练模型进行迁移学习

### 2.5.9. 分类和定位

### 2.5.10. 物体检测

```
#### (1) 全卷积网络
#### (2) YOLO
```

### 2.5.11. 物体跟踪

### 2.5.12. 语义分割

### 2.5.13. 练习题

## 2.6. 第15章 使用RNN和CNN处理序列

### 2.6.1. 循环神经元和层

```
#### (1) 记忆单元
#### (2) 输入序列和输出序列
```

### 2.6.2. 训练RNN

### 2.6.3. 预测时间序列

```
#### (1) ARMA系列模型
#### (2) 为机器学习模型准备数据
#### (3) 使用线性模型进行预测
#### (4) 使用简单的RNN进行预测
#### (5) 使用深度RNN进行预测
#### (6) 多元时间序列预测
#### (7) 预测未来多个时间步
#### (8) 使用序列到序列模型进行预测
```

### 2.6.4. 处理长序列

```
#### (1) 解决不稳定的梯度问题
#### (2) 处理短期记忆问题
```

### 2.6.5. 练习题

## 2.7. 第16章 使用RNN和Transformer进行自然语言处理

### 2.7.1. 使用字符RNN生成莎士比亚文本

```
#### (1) 将文本拆分成字符序列
#### (2) 构建字符数据集
#### (3) 构建字符级RNN模型
#### (4) 训练字符级RNN模型
#### (5) 使用字符级RNN模型
#### (6) 使用有状态RNN生成文本
```

### 2.7.2. 情感分析

### 2.7.3. 情感分析的编码器-解码器网络

```
#### (1) 处理掩码
```

### 2.7.4. 注意力机制

```
#### (1) 注意力就是权重
#### (2) 视觉注意力
#### (3) 自注意力
```

### 2.7.5. Transformer

```
#### (1) 位置编码
#### (2) 编码器
#### (3) 解码器
```

### 2.7.6. 语言模型

```
#### (1) 比较语言模型
#### (2) 用于视觉任务的Transformer
#### (3) 大型多模态模型
```

### 2.7.7. 练习题

## 2.8. 第17章 使用自动编码器和GAN进行表示学习和生成学习

### 2.8.1. 高效的数据表示

### 2.8.2. 使用欠完备线性自动编码器执行PCA

### 2.8.3. 栈式自动编码器

```
#### (1) 使用Keras实现栈式自动编码器
#### (2) 可视化重建结果
#### (3) 可视化Fashion MNIST数据集
#### (4) 无监督预训练的栈式自动编码器
#### (5) 绑定权重
#### (6) 一次训练一个自动编码器
```

### 2.8.4. 卷积自动编码器

### 2.8.5. 循环自动编码器

### 2.8.6. 去噪自动编码器

### 2.8.7. 稀疏自动编码器

### 2.8.8. 变分自动编码器

```
#### (1) 生成Fashion MNIST图像
#### (2) 编码器和解码器网络
```

### 2.8.9. 生成对抗网络

```
#### (1) 训练难点
#### (2) 深度卷积GAN
#### (3) 渐进式增长GAN
#### (4) StyleGAN
```

### 2.8.10. 扩散模型

```
#### (1) 前向扩散过程
#### (2) 反向过程
#### (3) DDPM的训练
#### (4) 采样图像
#### (5) 速度更快的扩散模型
```

### 2.8.11. 练习题

## 2.9. 第18章 强化学习

### 2.9.1. 学习优化奖励

### 2.9.2. 策略搜索

### 2.9.3. OpenAI Gym介绍

```
#### (1) 共享内存
```

### 2.9.4. 神经网络策略

```
#### (1) 评估神经网络策略的动作
#### (2) 使用策略梯度训练神经网络策略
#### (3) 马尔可夫决策过程
#### (4) 折扣因子
```

### 2.9.5. 时序差分学习和Q学习

```
#### (1) 时序差分学习
#### (2) Q学习
```

### 2.9.6. 深度Q网络

```
#### (1) 使用TF-Agents库
#### (2) DQN架构
#### (3) 训练指标
#### (4) 创建训练流水线
#### (5) 运行训练流水线
```

### 2.9.7. 强化学习概览

### 2.9.8. 练习题

## 2.10. 第19章 大规模训练和部署模型

### 2.10.1. 在多个设备上训练模型

```
#### (1) 在单个机器上使用多个GPU
```

### 2.10.2. TensorFlow中的分布式训练

```
#### (1) MirroredStrategy
#### (2) 其他分布式策略
#### (3) 使用tf.distribute.Strategy的自定义训练循环
#### (4) 大型数据集的分布式训练
#### (5) Vertex AI上的分布式训练
#### (6) Keras Tuner的分布式训练
```

### 2.10.3. 在移动或嵌入式设备上运行模型

### 2.10.4. 在浏览器中运行模型

### 2.10.5. 将模型部署为Web服务

```
#### (1) 使用TF Serving
#### (2) 在Vertex AI上部署模型
#### (3) 在Vertex AI上运行批量预测作业
```

### 2.10.6. 练习题

# 3. 附录A 机器学习项目清单

# 4. 附录B 数学附注

# 5. 附录C TensorFlow简介

# 6. 附录D AutoGraph和跟踪

